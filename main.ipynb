{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e03a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f24c45f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = np.load('Assignment1-Dataset/train_data.npy')\n",
    "train_label = np.load('Assignment1-Dataset/train_label.npy')\n",
    "test_data = np.load('Assignment1-Dataset/test_data.npy')\n",
    "test_label = np.load('Assignment1-Dataset/test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20edb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b53d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def __tanh_derivative(self, a):\n",
    "        return 1.0 - a**2\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def __sigmoid_derivative(self, a):\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    def __relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def __relu_derivative(self, a):\n",
    "        return np.heaviside(a, 0)\n",
    "\n",
    "    def __leakyrelu(self, x, alpha=0.01):\n",
    "        return np.where(x >= 0, x, alpha * x)\n",
    "    \n",
    "    def __leakyrelu_derivative(self, x, alpha=0.01):\n",
    "        return np.heaviside(x, 1) * (1 - alpha) + alpha\n",
    "    \n",
    "    def __softmax(self, z):\n",
    "        z = np.atleast_2d(z)\n",
    "        max_z = np.max(z, axis=1, keepdims=True)\n",
    "        z = z - max_z\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    \n",
    "    def __softmax_derivative(self, z, z_hat):\n",
    "        return z_hat - z\n",
    "    \n",
    "    def __init__(self, activation_function = 'relu'):\n",
    "        if activation_function == \"tanh\":\n",
    "            self.f = self.__tanh\n",
    "            self.f_derivative = self.__tanh_derivative\n",
    "        elif activation_function == \"sigmoid\":\n",
    "            self.f = self.__sigmoid\n",
    "            self.f_derivative = self.__sigmoid_derivative   \n",
    "        elif activation_function == 'relu': \n",
    "            self.f = self.__relu\n",
    "            self.f_derivative = self.__relu_derivative\n",
    "        elif activation_function == 'leakyrelu':\n",
    "            self.f = self.__leakyrelu\n",
    "            self.f_derivative = self.__leakyrelu_derivative\n",
    "        elif activation_function == \"softmax\":\n",
    "            self.f = self.__softmax\n",
    "            self.f_derivative = self.__softmax_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36a040eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, \n",
    "                 n_in, \n",
    "                 n_out, \n",
    "                 activation_last_layer = 'relu',\n",
    "                 activation = 'relu',\n",
    "                 W = None,\n",
    "                 b = None,\n",
    "                 v_W = None,\n",
    "                 v_b = None,\n",
    "                 last_hidden_layer = False):\n",
    "    \n",
    "        '''\n",
    "        The class for a Hidden Layer in a MLP. \n",
    "\n",
    "        Attributes:\n",
    "        n_in (int): The dimensionality of the input to the Hidden Layer.\n",
    "        n_out (int): The dimensionality of the output, i.e. the number of hidden units.\n",
    "        activation_last_layer (str): The activation function of the previous Hidden Layer.\n",
    "        activation (str): The activation function of this current Hidden Layer\n",
    "        W (numpy array): The weight(s) applied to this current Hidden Layer. Set to None by default to allow initialisation later.\n",
    "        b (numpy array): The bias applied to this current Hidden Layer. Set to None by default to allow initialisation later.\n",
    "        v_W (numpy array): The 'velocity' or 'trajectory' term vt for the weight(s) in Momentum SGD. Set to None by default to allow initialisation later.\n",
    "        v_b (numpy array): The 'velocity' or 'trajectory' term vt for the bias in Momentum SGD. Set to None by default to allow initialisation later.\n",
    "        last_hidden_layer (bool): The boolean to determine if the current Hidden Layer object is the Last Hidden Layer in the MLP.\n",
    "        '''\n",
    "\n",
    "        self.last_hidden_layer = last_hidden_layer\n",
    "        self.input = None\n",
    "        self.activation = Activation(activation).f\n",
    "        self.activation_deriv = None\n",
    "\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv = Activation(activation_last_layer).f_derivative\n",
    "\n",
    "        #Xavier Initialisation - assign random small values (from uniform dist)\n",
    "        self.W = np.random.uniform(low = -np.sqrt(6. / (n_in + n_out)),\n",
    "                                   high = np.sqrt(6. / (n_in + n_out )),\n",
    "                                   size = (n_in, n_out))\n",
    "        \n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        if activation == 'logistic':\n",
    "           self.W *= 4\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "\n",
    "        self.v_W = np.zeros_like(self.grad_W)\n",
    "        self.v_b = np.zeros_like(self.grad_b)\n",
    "        \n",
    "        self.binomial_array=np.zeros(n_out)\n",
    "\n",
    "    @staticmethod\n",
    "    def dropout_forward(X, p_dropout):\n",
    "        '''\n",
    "        The method to perform dropout during the training of the forward pass.\n",
    "\n",
    "        Paremeters:\n",
    "        X (numpy array): The input data to be fed through the dropout forward pass.\n",
    "        p_dropout (float): The controlling factor of the proportion of neurons dropped in the network.\n",
    "\n",
    "        Returns:\n",
    "        out (numpy array): The resulting output array with values from inactive neurons as 0 and values from the active neuron equal to that of the input. \n",
    "        binomial_array (numpy array): An array with the same size of input, filled with 0s for neurons that are to be inactive and 1s for neurons that are to be active during training.\n",
    "        '''\n",
    "      \n",
    "        u = np.random.binomial(1, 1 - p_dropout, size=X.shape) \n",
    "        out = X * u\n",
    "        binomial_array=u\n",
    "        return out, binomial_array\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout_backward(delta, binomial_array, layer_num):\n",
    "        '''\n",
    "        The method to perform dropout during the backpropagation.\n",
    "\n",
    "        Parameters:\n",
    "        delta (numpy array): The delta generated for the backpropagation process.\n",
    "        binomial_array (numpy array): An array with the same size of input, filled with 0s for neurons that are to be inactive and 1s for neurons that are to be active during training.\n",
    "        layer_num (int): The current layer in the MLP which dropout is being performed on.\n",
    "\n",
    "        Returns:\n",
    "        delta (numpy array): The adjusted delta with dropout applied.\n",
    "        '''\n",
    "\n",
    "        delta*=nn.layers[layer_num - 1].binomial_array\n",
    "        return delta\n",
    "    \n",
    "    #forward progress for training epoch:\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        The feedforward pass of a single Hidden Layer.\n",
    "        Applies the weights and bias to the input, performs calculations via the selected activation function and returns this output.\n",
    "\n",
    "        Parameters:\n",
    "        input (numpy array): The input data, either from the output of the previous Hidden Layer or the initial input data. \n",
    "\n",
    "        Returns:\n",
    "        self.output (numpy array): The resulting output.\n",
    "        '''\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b #simple perceptron output\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None #linear if no activation specified\n",
    "            else self.activation(lin_output) #activation fn on w*I + b  (i.e. activation function on linear output)\n",
    "        ) \n",
    "\n",
    "        if not self.last_hidden_layer:\n",
    "            self.output, self.binomial_array = self.dropout_forward(self.output, DROPOUT_PROB)\n",
    "\n",
    "        self.input = input\n",
    "        return self.output\n",
    "\n",
    "    #backpropagation\n",
    "    def backward(self, delta, layer_num, output_layer = False):\n",
    "        '''\n",
    "        The backward pass of a single Hidden Layer.\n",
    "\n",
    "        Parameters:\n",
    "        delta (numpy array): The delta values to be applied to the activation derivative.\n",
    "        layer_num (int): The number of the current layer in the MLP, used to check if it is not the input layer.\n",
    "        output_layer (bool): A boolean to reflect if the current layer is not the output layer. \n",
    "\n",
    "        Returns delta (numpy array): The delta for the hidden layer to be used in parameter updates.\n",
    "        '''\n",
    "\n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.average(delta, axis=0)\n",
    "\n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "\n",
    "        if layer_num != 0:\n",
    "            delta=self.dropout_backward(delta, self.binomial_array, layer_num)\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa15ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    '''\n",
    "    Main class holding the structure of the Multi-Layer Perceptron. \n",
    "    \n",
    "    Attributes:\n",
    "    None\n",
    "    '''\n",
    "\n",
    "    def __init__(self, layers, activation = [None, 'relu', 'relu','relu', 'softmax'], weight_decay = 1.0):\n",
    "\n",
    "        '''\n",
    "        The initialisation of the MLP.\n",
    "\n",
    "        Attributes:\n",
    "        layers (list of int): A list containing the number of neurons in each respective layer.\n",
    "        activation (list of str): A list containing the activation functions to be used in each respective layer. Set to [None, 'relu', 'relu', 'relu', 'softmax'] as default.\n",
    "        weight_decay (float): The value set for the weight decay to be applied. Value of 1.0 indicates no weight decay to be applied.\n",
    "        '''\n",
    "        \n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "\n",
    "            last_hidden_layer = False\n",
    "\n",
    "            if i == len(layers) - 2: # -2 because -1 for output layer, and another -1 since it's index 0\n",
    "                last_hidden_layer = True\n",
    "\n",
    "            self.layers.append(HiddenLayer(layers[i], \n",
    "                                           layers[i+1], \n",
    "                                           activation[i], \n",
    "                                           activation[i+1],\n",
    "                                           last_hidden_layer=last_hidden_layer))\n",
    "            \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        The feedforward process conducted sequentially through each layer in the MLP.\n",
    "        Takes the input from the previous layer (or initial data if it is the input layer), applies weights & bias then activation function and feeds the resulting output as the input to the next layer via the HiddenLayer.forward() method.\n",
    "\n",
    "        Parameters:\n",
    "        input (numpy array): The input array to be fed through the feedforward process.\n",
    "\n",
    "        Returns:\n",
    "        output (numpy array): The resulting final output from the feedforward process across all layers.\n",
    "        '''\n",
    "\n",
    "        for layer in self.layers: \n",
    "            output = layer.forward(input) \n",
    "           \n",
    "            input = output \n",
    "        return output\n",
    "\n",
    "    def CE_loss(self, y, y_hat):\n",
    "        '''\n",
    "        The calculation of the Cross-Entropy loss function.\n",
    "        Computes the cross entropy loss, averages this and applies weight decay (if applicable) as well as calculating the respective delta to be used in the backpropagation process.\n",
    "\n",
    "        Parameters:\n",
    "        y (numpy array): The actual y values (or labels) from the data set.\n",
    "        y_hat (numpy array): The calculated y values (y hat) as output from the feedforward process.\n",
    "\n",
    "        Returns:\n",
    "        loss (float): The calculated Cross Entropy Loss value.\n",
    "        delta (numpy array): The calculated delta array to be used in the backpropagation process. \n",
    "        '''\n",
    "\n",
    "        loss = - np.nansum(y * np.log(y_hat))\n",
    "        loss = loss / y.shape[0] \n",
    "        loss = loss*self.weight_decay\n",
    "        delta = Activation(self.activation[-1]).f_derivative(y, y_hat)\n",
    "        return loss, delta\n",
    "\n",
    "    def backward(self, delta):\n",
    "        '''\n",
    "        The backpropagation process conducted backwards across each layer in the MLP.\n",
    "        Updates the delta via the Hidden Layer backward process and applies this updated delta as the delta input in the HiddenLayer.backward() method. \n",
    "        \n",
    "        Parameters:\n",
    "        delta (numpy array): The value for delta calculated in the Loss function.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        '''\n",
    "\n",
    "        delta = self.layers[-1].backward(delta, len(self.layers) -1, output_layer = True)\n",
    "        for layer_num, layer in reversed(list(enumerate(self.layers[:-1]))):\n",
    "            delta = layer.backward(delta, layer_num)\n",
    "                  \n",
    "    def update(self, lr, SGD_optim):\n",
    "      '''\n",
    "      The method to update the parameters under Stochastic Gradient Descent (SGD).\n",
    "      Updates the weights and bias parameters based on the learning rate and respective gradient. Includes functionality for applying SGD Momentum optimization.\n",
    "         \n",
    "      Parameters:\n",
    "      lr (float): The learning rate for the parameter updates.\n",
    "      SGD_optim (dict of str: str): The SGD Optimization values as a dictionary with keys 'Type': as the type of optimisation and 'Parameters': for the optimization parameter value.\n",
    "\n",
    "      Returns:\n",
    "      None\n",
    "      '''\n",
    "\n",
    "      if SGD_optim is None:\n",
    "          for layer in self.layers:\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "\n",
    "      elif SGD_optim['Type'] == 'Momentum':\n",
    "          for layer in self.layers:\n",
    "              layer.v_W = (SGD_optim['Parameter'] * layer.v_W) + (lr * layer.grad_W)\n",
    "              layer.v_b = (SGD_optim['Parameter'] * layer.v_b) + (lr * layer.grad_b)\n",
    "              layer.W = layer.W - layer.v_W\n",
    "              layer.b = layer.b - layer.v_b\n",
    "              \n",
    "    def fit(self, X, y, learning_rate = 0.1, epochs = 100, SGD_optim = None, batch_size = 1):\n",
    "        '''\n",
    "        The method to fit the MLP.\n",
    "        Iterates through epochs, runs the forward process to calculate respective loss (and delta) then runs the backpropagation process to update the parameters.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy array): The input X values.\n",
    "        y (numpy array): The corresponding y values (or labels).\n",
    "        learning_rate (float): The learning rate to be used in the parameter updates. Set to 0.1 by default.\n",
    "        epochs (int): The number of times the dataset is passed through the MLP. Set to 100 by default.\n",
    "        SGD_optim (dict of str: str): A dictionary containing the type of optimization and the respective optimization algorithm parameter to be used. Set as None by default.\n",
    "        batch_size (int): The size of the batches to be used in Mini-Batch learning.\n",
    "\n",
    "        Returns:\n",
    "        output_dct (dict of float): A dictionary containing the training cross-entropy loss, training accuracy and testing accuracy.\n",
    "        '''\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        training_loss = []\n",
    "        training_accuracy = []\n",
    "        testing_accuracy = []\n",
    "\n",
    "        num_batches = int(np.ceil(X.shape[0] / batch_size))\n",
    "            \n",
    "        for k in range(epochs):\n",
    "\n",
    "            loss = np.zeros(num_batches) \n",
    "\n",
    "            current_idx = 0 \n",
    "\n",
    "            #Shuffle the data, to ensure that each epoch will have different sequence of observations\n",
    "            X, y = Utils.shuffle(X, y)\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                \n",
    "                #forward pass \n",
    "                y_hat = self.forward(X[current_idx : current_idx + batch_size, :])\n",
    "\n",
    "                #backward pass\n",
    "                loss[batch_idx], delta = self.CE_loss(y[current_idx : current_idx + batch_size], y_hat)\n",
    "\n",
    "                self.backward(delta)\n",
    "\n",
    "                #update\n",
    "                self.update(learning_rate, SGD_optim)\n",
    "\n",
    "                #Update the index based on the batch window for the next round of Mini-Batch learning.\n",
    "                if (current_idx + batch_size) > X.shape[0]:\n",
    "                    batch_size = X.shape[0] - current_idx\n",
    "                current_idx += batch_size\n",
    "\n",
    "            #Predict and compute metrics for each run\n",
    "            test_predict = self.predict(test_df.X)\n",
    "            train_predict = self.predict(train_df.X)\n",
    "            test_predict = test_df.decode(test_predict)\n",
    "            train_predict = train_df.decode(train_predict)\n",
    "            test_accuracy = np.sum(test_predict == test_label[:, 0]) / test_predict.shape[0]\n",
    "            train_accuracy = np.sum(train_predict == train_label[:, 0]) / train_predict.shape[0]\n",
    "\n",
    "            training_loss.append(np.mean(loss))\n",
    "            training_accuracy.append(train_accuracy)\n",
    "            testing_accuracy.append(test_accuracy)\n",
    "\n",
    "            output_dict = {'Training Loss': training_loss, 'Training Accuracy': training_accuracy, 'Testing Accuracy': testing_accuracy}\n",
    "\n",
    "            print(f'Epoch {k+1}/{epochs} has been trained with Train Loss: {str(round(training_loss[-1], 4))}, Training Accuracy: {str(round(training_accuracy[-1] * 100, 4))}% and Testing Accuracy: {str(round(testing_accuracy[-1] * 100, 4))}%.')\n",
    "         \n",
    "        return output_dict\n",
    "            \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        The method to predict values based on input x by running forward process through the fitted MLP.\n",
    "\n",
    "        Parameters:\n",
    "        x (numpy array): The input x values on which to compute predictions.\n",
    "\n",
    "        Returns:\n",
    "        output (numpy array): The resulting predictions.\n",
    "        '''\n",
    "\n",
    "        x = np.array(x)\n",
    "        output = [i for i in range(x.shape[0])]\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = self.forward(x[i, :])\n",
    "        output = np.array(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2301164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    '''\n",
    "    The Class to apply preprocessing methods.\n",
    "\n",
    "    Attributes:\n",
    "    X (numpy array): The input array of X values.\n",
    "    y (numpy array): The input array of y values (or labels).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.predictions = None\n",
    "\n",
    "    def normalize(self):     \n",
    "        '''\n",
    "        Normalizes and transforms the X values based on min-max normalization.\n",
    "\n",
    "        Parameters:\n",
    "        None\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        '''\n",
    "\n",
    "        norm_data = (self.X - np.min(self.X))/(np.max(self.X) - np.min(self.X))\n",
    "        self.X = norm_data\n",
    "\n",
    "    def standardize(self):\n",
    "        '''\n",
    "        Standardizes and transforms the X values based on the mean & standard deviation.\n",
    "\n",
    "        Parameters:\n",
    "        None\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        '''\n",
    "\n",
    "        self.X = (self.X - np.mean(self.X)) / np.std(self.X)\n",
    "\n",
    "    @staticmethod\n",
    "    def label_encode(label_vector):\n",
    "        '''\n",
    "        Encodes the label (y) values based on one-hot encoding.\n",
    "        Creates an empty list for each observation, fills it with zeros then set the index of the class label to 1.\n",
    "\n",
    "        Parameters:\n",
    "        label_vector (numpy array): The label array to be one-hot encoded.\n",
    "\n",
    "        Returns:\n",
    "        encoded_label_vector (numpy array): The resulting one-hot encoded array for the labels.    \n",
    "        '''\n",
    "\n",
    "        num_classes = np.unique(label_vector).size\n",
    "        \n",
    "        encoded_label_vector = []\n",
    "        \n",
    "        for label in label_vector:\n",
    "            encoded_label = np.zeros(num_classes)\n",
    "            encoded_label[int(label)] = 1\n",
    "            encoded_label_vector.append(encoded_label)\n",
    "        \n",
    "        encoded_label_vector = np.array(encoded_label_vector) \n",
    "\n",
    "        return encoded_label_vector\n",
    "    \n",
    "    @staticmethod\n",
    "    def decode(prediction_matrix):\n",
    "        '''\n",
    "        Transforms a one-hot encoded matrix back to a class label.\n",
    "        Creates a zero array and fills it with the index of maximum value (i.e. 1) in the one-hot encoded array.\n",
    "\n",
    "        Parameters:\n",
    "        prediction_matrix (numpy array): The one-hot encoded label matrix.\n",
    "\n",
    "        Returns:\n",
    "        decoded_predictions (numpy array): A numpy array filled with the labels.\n",
    "        '''\n",
    "\n",
    "        decoded_predictions = np.zeros(prediction_matrix.shape[0])\n",
    "        for prediction_idx, prediction_vector in enumerate(prediction_matrix):\n",
    "            decoded_predictions[prediction_idx] = int(np.argmax(prediction_vector)) # we add the two index zeros because it's a nparray within a tuple\n",
    "        \n",
    "        return decoded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5450437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    '''\n",
    "    Class used to contain miscellaneous methods.\n",
    "\n",
    "    Attributes:\n",
    "    None\n",
    "    '''\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle(X, y):\n",
    "        '''\n",
    "        Randomly shuffles the data.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy array): The X values to be shuffled.\n",
    "        y (numpy array): The y values to be shuffled.\n",
    "\n",
    "        Returns:\n",
    "        X (numpy array), y (numpy array): The pair of the shuffled X & y numpy arrays.\n",
    "        '''\n",
    "        shuffled_idx = np.arange(X.shape[0])\n",
    "        np.random.shuffle(shuffled_idx)\n",
    "        X = X[shuffled_idx]\n",
    "        y = y[shuffled_idx]\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_confusion_mat(df):\n",
    "      '''\n",
    "      Creates a confusion matrix based on a Preprocessing object that has X, y and predicted y values.\n",
    "      Calculates the values to be placed in respective row/columns by summing the occurences in pairwise indices for the original and predicted y values.\n",
    "\n",
    "      Parameters:\n",
    "      df (Preprocessing): A Preprocessing object with original X, original y and predicted y values.\n",
    "\n",
    "      Returns:\n",
    "      confusion_mat (pandas DataFrame): A confusion matrix represented as a pandas DataFrame, where the rows (indexes) reflect predicted values and the columns reflect actual values.\n",
    "      '''\n",
    "\n",
    "      confusion_mat = pd.DataFrame(0, index = np.unique(df.y) , columns = np.unique(df.y))\n",
    "      for i in range(0, len(df.y)):\n",
    "        confusion_mat[int(df.y[i])].iloc[int(df.predictions[i])] += 1\n",
    "      return confusion_mat\n",
    "    \n",
    "    @staticmethod\n",
    "    def confusion_mat_measures(confusion_matrix):\n",
    "      '''\n",
    "      Produces a pandas DataFrame with Precision, Recall and F1 measures per class.\n",
    "      First calculates True Positive (TP), False Negative (FN), False Positive (FP) and True Negative (TN) values then calculates Precision, Recall and F1 values and stores them in a DataFrame.\n",
    "\n",
    "      Parameters:\n",
    "      confusion_matrix (pandas DataFrame): A confusion matrix as a Pandas DataFrame Object.\n",
    "\n",
    "      Returns:\n",
    "      scores_df (pandas Dataframe): A DataFrame with labels as rows (indexes) and Precision, Recall and F1 scores as columns.\n",
    "      '''\n",
    "\n",
    "      scores_df = pd.DataFrame(0, index = confusion_matrix.index, columns = ['Precision', 'Recall', 'F1'])\n",
    "      for  i in confusion_matrix.index:\n",
    "        TP = confusion_matrix[i][i]\n",
    "        FN = np.array(confusion_matrix[i].iloc[0:i].values.tolist() + confusion_matrix[i].iloc[i+1:].values.tolist()).sum()\n",
    "        FP = np.array(confusion_matrix.iloc[i][0:i].values.tolist() + confusion_matrix.iloc[i][i + 1:].values.tolist()).sum()\n",
    "        TN = confusion_matrix.sum().sum() - TP - FN - FP\n",
    "\n",
    "        Precision = TP / (TP + FP)\n",
    "        Recall = TP / (TP + FN)\n",
    "        F1 = (2 * Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "        scores_df.loc[i, 'Precision'] = Precision\n",
    "        scores_df.loc[i, 'Recall'] = Recall\n",
    "        scores_df.loc[i, 'F1'] = F1\n",
    "\n",
    "      scores_df.index.name = 'Label'\n",
    "      \n",
    "      return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296f4dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/4s9pjgcj79dbp8y5rdx60v_h0000gn/T/ipykernel_77018/1875021445.py:61: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  encoded_label[int(label)] = 1\n"
     ]
    }
   ],
   "source": [
    "train_df = Preprocessing(train_data, train_label)\n",
    "test_df = Preprocessing(test_data, test_label)\n",
    "\n",
    "# Standardize X matrix (features)\n",
    "#train_df.normalize()\n",
    "#test_df.normalize()\n",
    "train_df.standardize()\n",
    "test_df.standardize()\n",
    "\n",
    "# Perform one-hot encoding for our label vector (ONLY ON TRAIN)\n",
    "train_df.y = train_df.label_encode(train_df.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a517a402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 has been trained with Train Loss: 1.8923, Training Accuracy: 34.31% and Testing Accuracy: 33.77%.\n",
      "Epoch 2/200 has been trained with Train Loss: 1.7637, Training Accuracy: 36.504% and Testing Accuracy: 36.24%.\n",
      "Epoch 3/200 has been trained with Train Loss: 1.7234, Training Accuracy: 37.814% and Testing Accuracy: 36.4%.\n",
      "Epoch 4/200 has been trained with Train Loss: 1.7012, Training Accuracy: 38.576% and Testing Accuracy: 38.33%.\n",
      "Epoch 5/200 has been trained with Train Loss: 1.6819, Training Accuracy: 38.704% and Testing Accuracy: 38.62%.\n",
      "Epoch 6/200 has been trained with Train Loss: 1.6705, Training Accuracy: 39.772% and Testing Accuracy: 39.48%.\n",
      "Epoch 7/200 has been trained with Train Loss: 1.6645, Training Accuracy: 39.974% and Testing Accuracy: 40.36%.\n",
      "Epoch 8/200 has been trained with Train Loss: 1.6553, Training Accuracy: 40.342% and Testing Accuracy: 39.85%.\n",
      "Epoch 9/200 has been trained with Train Loss: 1.6485, Training Accuracy: 40.692% and Testing Accuracy: 39.74%.\n",
      "Epoch 10/200 has been trained with Train Loss: 1.6431, Training Accuracy: 40.72% and Testing Accuracy: 39.33%.\n",
      "Epoch 11/200 has been trained with Train Loss: 1.6362, Training Accuracy: 40.85% and Testing Accuracy: 40.1%.\n",
      "Epoch 12/200 has been trained with Train Loss: 1.6324, Training Accuracy: 40.756% and Testing Accuracy: 39.54%.\n",
      "Epoch 13/200 has been trained with Train Loss: 1.6301, Training Accuracy: 41.244% and Testing Accuracy: 39.98%.\n",
      "Epoch 14/200 has been trained with Train Loss: 1.626, Training Accuracy: 41.416% and Testing Accuracy: 40.52%.\n",
      "Epoch 15/200 has been trained with Train Loss: 1.6218, Training Accuracy: 41.342% and Testing Accuracy: 40.62%.\n",
      "Epoch 16/200 has been trained with Train Loss: 1.6185, Training Accuracy: 41.182% and Testing Accuracy: 39.83%.\n",
      "Epoch 17/200 has been trained with Train Loss: 1.619, Training Accuracy: 41.568% and Testing Accuracy: 40.28%.\n",
      "Epoch 18/200 has been trained with Train Loss: 1.6107, Training Accuracy: 41.106% and Testing Accuracy: 39.88%.\n",
      "Epoch 19/200 has been trained with Train Loss: 1.615, Training Accuracy: 41.484% and Testing Accuracy: 39.76%.\n",
      "Epoch 20/200 has been trained with Train Loss: 1.6118, Training Accuracy: 41.766% and Testing Accuracy: 41.07%.\n",
      "Epoch 21/200 has been trained with Train Loss: 1.6075, Training Accuracy: 42.056% and Testing Accuracy: 40.96%.\n",
      "Epoch 22/200 has been trained with Train Loss: 1.6051, Training Accuracy: 42.192% and Testing Accuracy: 41.21%.\n",
      "Epoch 23/200 has been trained with Train Loss: 1.6031, Training Accuracy: 42.158% and Testing Accuracy: 41.58%.\n",
      "Epoch 24/200 has been trained with Train Loss: 1.6005, Training Accuracy: 42.144% and Testing Accuracy: 40.79%.\n",
      "Epoch 25/200 has been trained with Train Loss: 1.5997, Training Accuracy: 42.416% and Testing Accuracy: 40.52%.\n",
      "Epoch 26/200 has been trained with Train Loss: 1.5981, Training Accuracy: 42.058% and Testing Accuracy: 40.42%.\n",
      "Epoch 27/200 has been trained with Train Loss: 1.5949, Training Accuracy: 42.09% and Testing Accuracy: 40.42%.\n",
      "Epoch 28/200 has been trained with Train Loss: 1.5912, Training Accuracy: 40.942% and Testing Accuracy: 40.08%.\n",
      "Epoch 29/200 has been trained with Train Loss: 1.5959, Training Accuracy: 42.652% and Testing Accuracy: 41.55%.\n",
      "Epoch 30/200 has been trained with Train Loss: 1.5919, Training Accuracy: 42.78% and Testing Accuracy: 41.21%.\n",
      "Epoch 31/200 has been trained with Train Loss: 1.591, Training Accuracy: 41.89% and Testing Accuracy: 41.04%.\n",
      "Epoch 32/200 has been trained with Train Loss: 1.5898, Training Accuracy: 42.566% and Testing Accuracy: 41.28%.\n",
      "Epoch 33/200 has been trained with Train Loss: 1.5948, Training Accuracy: 42.536% and Testing Accuracy: 41.15%.\n",
      "Epoch 34/200 has been trained with Train Loss: 1.5893, Training Accuracy: 42.336% and Testing Accuracy: 41.28%.\n",
      "Epoch 35/200 has been trained with Train Loss: 1.5863, Training Accuracy: 42.178% and Testing Accuracy: 41.21%.\n",
      "Epoch 36/200 has been trained with Train Loss: 1.587, Training Accuracy: 42.26% and Testing Accuracy: 40.71%.\n",
      "Epoch 37/200 has been trained with Train Loss: 1.5878, Training Accuracy: 42.626% and Testing Accuracy: 41.74%.\n",
      "Epoch 38/200 has been trained with Train Loss: 1.5873, Training Accuracy: 42.388% and Testing Accuracy: 40.75%.\n",
      "Epoch 39/200 has been trained with Train Loss: 1.5902, Training Accuracy: 42.576% and Testing Accuracy: 42.07%.\n",
      "Epoch 40/200 has been trained with Train Loss: 1.5803, Training Accuracy: 43.134% and Testing Accuracy: 40.91%.\n",
      "Epoch 41/200 has been trained with Train Loss: 1.5863, Training Accuracy: 42.514% and Testing Accuracy: 41.25%.\n",
      "Epoch 42/200 has been trained with Train Loss: 1.5811, Training Accuracy: 42.98% and Testing Accuracy: 41.63%.\n",
      "Epoch 43/200 has been trained with Train Loss: 1.5808, Training Accuracy: 42.88% and Testing Accuracy: 41.81%.\n",
      "Epoch 44/200 has been trained with Train Loss: 1.5827, Training Accuracy: 42.956% and Testing Accuracy: 41.41%.\n",
      "Epoch 45/200 has been trained with Train Loss: 1.5794, Training Accuracy: 42.616% and Testing Accuracy: 41.49%.\n",
      "Epoch 46/200 has been trained with Train Loss: 1.5771, Training Accuracy: 43.072% and Testing Accuracy: 41.82%.\n",
      "Epoch 47/200 has been trained with Train Loss: 1.5804, Training Accuracy: 42.542% and Testing Accuracy: 40.5%.\n",
      "Epoch 48/200 has been trained with Train Loss: 1.5768, Training Accuracy: 42.41% and Testing Accuracy: 41.09%.\n",
      "Epoch 49/200 has been trained with Train Loss: 1.5761, Training Accuracy: 43.388% and Testing Accuracy: 41.77%.\n",
      "Epoch 50/200 has been trained with Train Loss: 1.5792, Training Accuracy: 42.864% and Testing Accuracy: 41.39%.\n",
      "Epoch 51/200 has been trained with Train Loss: 1.5808, Training Accuracy: 42.96% and Testing Accuracy: 41.06%.\n",
      "Epoch 52/200 has been trained with Train Loss: 1.5781, Training Accuracy: 42.584% and Testing Accuracy: 40.92%.\n",
      "Epoch 53/200 has been trained with Train Loss: 1.5778, Training Accuracy: 42.994% and Testing Accuracy: 41.05%.\n",
      "Epoch 54/200 has been trained with Train Loss: 1.5818, Training Accuracy: 43.084% and Testing Accuracy: 41.76%.\n",
      "Epoch 55/200 has been trained with Train Loss: 1.5718, Training Accuracy: 43.082% and Testing Accuracy: 41.54%.\n",
      "Epoch 56/200 has been trained with Train Loss: 1.5724, Training Accuracy: 43.154% and Testing Accuracy: 41.38%.\n",
      "Epoch 57/200 has been trained with Train Loss: 1.5807, Training Accuracy: 43.0% and Testing Accuracy: 41.38%.\n",
      "Epoch 58/200 has been trained with Train Loss: 1.5792, Training Accuracy: 43.088% and Testing Accuracy: 41.22%.\n",
      "Epoch 59/200 has been trained with Train Loss: 1.5724, Training Accuracy: 42.716% and Testing Accuracy: 41.18%.\n",
      "Epoch 60/200 has been trained with Train Loss: 1.5733, Training Accuracy: 43.334% and Testing Accuracy: 42.2%.\n",
      "Epoch 61/200 has been trained with Train Loss: 1.5751, Training Accuracy: 42.984% and Testing Accuracy: 41.85%.\n",
      "Epoch 62/200 has been trained with Train Loss: 1.5766, Training Accuracy: 42.462% and Testing Accuracy: 40.35%.\n",
      "Epoch 63/200 has been trained with Train Loss: 1.5755, Training Accuracy: 42.994% and Testing Accuracy: 42.01%.\n",
      "Epoch 64/200 has been trained with Train Loss: 1.5768, Training Accuracy: 43.12% and Testing Accuracy: 40.53%.\n",
      "Epoch 65/200 has been trained with Train Loss: 1.5743, Training Accuracy: 43.484% and Testing Accuracy: 41.63%.\n",
      "Epoch 66/200 has been trained with Train Loss: 1.5743, Training Accuracy: 42.818% and Testing Accuracy: 41.11%.\n",
      "Epoch 67/200 has been trained with Train Loss: 1.5686, Training Accuracy: 43.484% and Testing Accuracy: 41.91%.\n",
      "Epoch 68/200 has been trained with Train Loss: 1.572, Training Accuracy: 43.19% and Testing Accuracy: 42.21%.\n",
      "Epoch 69/200 has been trained with Train Loss: 1.5701, Training Accuracy: 42.646% and Testing Accuracy: 40.93%.\n",
      "Epoch 70/200 has been trained with Train Loss: 1.5692, Training Accuracy: 43.184% and Testing Accuracy: 41.59%.\n",
      "Epoch 71/200 has been trained with Train Loss: 1.5633, Training Accuracy: 42.85% and Testing Accuracy: 42.06%.\n",
      "Epoch 72/200 has been trained with Train Loss: 1.5626, Training Accuracy: 43.422% and Testing Accuracy: 41.41%.\n",
      "Epoch 73/200 has been trained with Train Loss: 1.5666, Training Accuracy: 43.368% and Testing Accuracy: 41.88%.\n",
      "Epoch 74/200 has been trained with Train Loss: 1.5666, Training Accuracy: 43.186% and Testing Accuracy: 41.66%.\n",
      "Epoch 75/200 has been trained with Train Loss: 1.5639, Training Accuracy: 43.3% and Testing Accuracy: 41.9%.\n",
      "Epoch 76/200 has been trained with Train Loss: 1.5665, Training Accuracy: 43.382% and Testing Accuracy: 42.43%.\n",
      "Epoch 77/200 has been trained with Train Loss: 1.567, Training Accuracy: 43.052% and Testing Accuracy: 41.58%.\n",
      "Epoch 78/200 has been trained with Train Loss: 1.5674, Training Accuracy: 43.638% and Testing Accuracy: 41.56%.\n",
      "Epoch 79/200 has been trained with Train Loss: 1.5667, Training Accuracy: 43.334% and Testing Accuracy: 41.41%.\n",
      "Epoch 80/200 has been trained with Train Loss: 1.5647, Training Accuracy: 43.27% and Testing Accuracy: 41.43%.\n",
      "Epoch 81/200 has been trained with Train Loss: 1.5646, Training Accuracy: 42.99% and Testing Accuracy: 41.81%.\n",
      "Epoch 82/200 has been trained with Train Loss: 1.5674, Training Accuracy: 42.944% and Testing Accuracy: 41.21%.\n",
      "Epoch 83/200 has been trained with Train Loss: 1.5612, Training Accuracy: 43.4% and Testing Accuracy: 41.91%.\n",
      "Epoch 84/200 has been trained with Train Loss: 1.5596, Training Accuracy: 43.58% and Testing Accuracy: 41.55%.\n",
      "Epoch 85/200 has been trained with Train Loss: 1.5583, Training Accuracy: 43.042% and Testing Accuracy: 41.17%.\n",
      "Epoch 86/200 has been trained with Train Loss: 1.563, Training Accuracy: 43.668% and Testing Accuracy: 41.68%.\n",
      "Epoch 87/200 has been trained with Train Loss: 1.5591, Training Accuracy: 43.194% and Testing Accuracy: 41.09%.\n",
      "Epoch 88/200 has been trained with Train Loss: 1.5681, Training Accuracy: 43.296% and Testing Accuracy: 42.0%.\n",
      "Epoch 89/200 has been trained with Train Loss: 1.5663, Training Accuracy: 43.382% and Testing Accuracy: 41.1%.\n",
      "Epoch 90/200 has been trained with Train Loss: 1.5632, Training Accuracy: 43.05% and Testing Accuracy: 41.98%.\n",
      "Epoch 91/200 has been trained with Train Loss: 1.56, Training Accuracy: 43.008% and Testing Accuracy: 41.61%.\n",
      "Epoch 92/200 has been trained with Train Loss: 1.5631, Training Accuracy: 42.946% and Testing Accuracy: 41.54%.\n",
      "Epoch 93/200 has been trained with Train Loss: 1.5595, Training Accuracy: 43.706% and Testing Accuracy: 41.18%.\n",
      "Epoch 94/200 has been trained with Train Loss: 1.5604, Training Accuracy: 43.448% and Testing Accuracy: 41.89%.\n",
      "Epoch 95/200 has been trained with Train Loss: 1.5565, Training Accuracy: 43.394% and Testing Accuracy: 41.03%.\n",
      "Epoch 96/200 has been trained with Train Loss: 1.5613, Training Accuracy: 42.898% and Testing Accuracy: 41.34%.\n",
      "Epoch 97/200 has been trained with Train Loss: 1.5605, Training Accuracy: 43.402% and Testing Accuracy: 41.49%.\n",
      "Epoch 98/200 has been trained with Train Loss: 1.5559, Training Accuracy: 43.116% and Testing Accuracy: 41.97%.\n",
      "Epoch 99/200 has been trained with Train Loss: 1.5543, Training Accuracy: 43.494% and Testing Accuracy: 41.34%.\n",
      "Epoch 100/200 has been trained with Train Loss: 1.5595, Training Accuracy: 43.3% and Testing Accuracy: 41.21%.\n",
      "Epoch 101/200 has been trained with Train Loss: 1.5593, Training Accuracy: 43.242% and Testing Accuracy: 41.92%.\n",
      "Epoch 102/200 has been trained with Train Loss: 1.5585, Training Accuracy: 43.092% and Testing Accuracy: 41.21%.\n",
      "Epoch 103/200 has been trained with Train Loss: 1.5525, Training Accuracy: 43.254% and Testing Accuracy: 41.47%.\n",
      "Epoch 104/200 has been trained with Train Loss: 1.5574, Training Accuracy: 43.954% and Testing Accuracy: 41.67%.\n",
      "Epoch 105/200 has been trained with Train Loss: 1.5547, Training Accuracy: 43.234% and Testing Accuracy: 41.96%.\n",
      "Epoch 106/200 has been trained with Train Loss: 1.5554, Training Accuracy: 43.278% and Testing Accuracy: 42.08%.\n",
      "Epoch 107/200 has been trained with Train Loss: 1.5593, Training Accuracy: 44.204% and Testing Accuracy: 42.43%.\n",
      "Epoch 108/200 has been trained with Train Loss: 1.5578, Training Accuracy: 43.514% and Testing Accuracy: 42.23%.\n",
      "Epoch 109/200 has been trained with Train Loss: 1.5549, Training Accuracy: 43.072% and Testing Accuracy: 40.87%.\n",
      "Epoch 110/200 has been trained with Train Loss: 1.5604, Training Accuracy: 43.014% and Testing Accuracy: 41.69%.\n",
      "Epoch 111/200 has been trained with Train Loss: 1.5564, Training Accuracy: 43.398% and Testing Accuracy: 40.79%.\n",
      "Epoch 112/200 has been trained with Train Loss: 1.5569, Training Accuracy: 43.768% and Testing Accuracy: 41.68%.\n",
      "Epoch 113/200 has been trained with Train Loss: 1.5637, Training Accuracy: 43.854% and Testing Accuracy: 41.29%.\n",
      "Epoch 114/200 has been trained with Train Loss: 1.5608, Training Accuracy: 43.816% and Testing Accuracy: 41.43%.\n",
      "Epoch 115/200 has been trained with Train Loss: 1.5562, Training Accuracy: 43.234% and Testing Accuracy: 41.51%.\n",
      "Epoch 116/200 has been trained with Train Loss: 1.5593, Training Accuracy: 43.984% and Testing Accuracy: 41.96%.\n",
      "Epoch 117/200 has been trained with Train Loss: 1.5493, Training Accuracy: 43.396% and Testing Accuracy: 41.22%.\n",
      "Epoch 118/200 has been trained with Train Loss: 1.5549, Training Accuracy: 43.986% and Testing Accuracy: 42.36%.\n",
      "Epoch 119/200 has been trained with Train Loss: 1.5553, Training Accuracy: 43.976% and Testing Accuracy: 41.41%.\n",
      "Epoch 120/200 has been trained with Train Loss: 1.5558, Training Accuracy: 43.822% and Testing Accuracy: 42.4%.\n",
      "Epoch 121/200 has been trained with Train Loss: 1.5567, Training Accuracy: 42.82% and Testing Accuracy: 40.67%.\n",
      "Epoch 122/200 has been trained with Train Loss: 1.5554, Training Accuracy: 43.662% and Testing Accuracy: 42.0%.\n",
      "Epoch 123/200 has been trained with Train Loss: 1.5558, Training Accuracy: 43.742% and Testing Accuracy: 41.16%.\n",
      "Epoch 124/200 has been trained with Train Loss: 1.5515, Training Accuracy: 43.746% and Testing Accuracy: 41.97%.\n",
      "Epoch 125/200 has been trained with Train Loss: 1.5527, Training Accuracy: 43.678% and Testing Accuracy: 42.49%.\n",
      "Epoch 126/200 has been trained with Train Loss: 1.5535, Training Accuracy: 43.524% and Testing Accuracy: 42.64%.\n",
      "Epoch 127/200 has been trained with Train Loss: 1.5595, Training Accuracy: 43.482% and Testing Accuracy: 41.49%.\n",
      "Epoch 128/200 has been trained with Train Loss: 1.5465, Training Accuracy: 43.424% and Testing Accuracy: 41.69%.\n",
      "Epoch 129/200 has been trained with Train Loss: 1.5527, Training Accuracy: 43.73% and Testing Accuracy: 42.07%.\n",
      "Epoch 130/200 has been trained with Train Loss: 1.5521, Training Accuracy: 43.428% and Testing Accuracy: 41.91%.\n",
      "Epoch 131/200 has been trained with Train Loss: 1.5554, Training Accuracy: 43.77% and Testing Accuracy: 42.42%.\n",
      "Epoch 132/200 has been trained with Train Loss: 1.551, Training Accuracy: 43.562% and Testing Accuracy: 41.72%.\n",
      "Epoch 133/200 has been trained with Train Loss: 1.554, Training Accuracy: 43.954% and Testing Accuracy: 41.64%.\n",
      "Epoch 134/200 has been trained with Train Loss: 1.5526, Training Accuracy: 44.098% and Testing Accuracy: 42.07%.\n",
      "Epoch 135/200 has been trained with Train Loss: 1.5504, Training Accuracy: 43.5% and Testing Accuracy: 42.18%.\n",
      "Epoch 136/200 has been trained with Train Loss: 1.5511, Training Accuracy: 43.492% and Testing Accuracy: 41.08%.\n",
      "Epoch 137/200 has been trained with Train Loss: 1.5463, Training Accuracy: 43.708% and Testing Accuracy: 41.64%.\n",
      "Epoch 138/200 has been trained with Train Loss: 1.546, Training Accuracy: 43.844% and Testing Accuracy: 42.14%.\n",
      "Epoch 139/200 has been trained with Train Loss: 1.546, Training Accuracy: 44.448% and Testing Accuracy: 41.5%.\n",
      "Epoch 140/200 has been trained with Train Loss: 1.5545, Training Accuracy: 43.316% and Testing Accuracy: 41.73%.\n",
      "Epoch 141/200 has been trained with Train Loss: 1.5531, Training Accuracy: 44.024% and Testing Accuracy: 41.9%.\n",
      "Epoch 142/200 has been trained with Train Loss: 1.5484, Training Accuracy: 43.77% and Testing Accuracy: 42.35%.\n",
      "Epoch 143/200 has been trained with Train Loss: 1.5487, Training Accuracy: 43.712% and Testing Accuracy: 41.85%.\n",
      "Epoch 144/200 has been trained with Train Loss: 1.5486, Training Accuracy: 43.508% and Testing Accuracy: 42.23%.\n",
      "Epoch 145/200 has been trained with Train Loss: 1.5548, Training Accuracy: 43.978% and Testing Accuracy: 42.18%.\n",
      "Epoch 146/200 has been trained with Train Loss: 1.5576, Training Accuracy: 44.168% and Testing Accuracy: 42.04%.\n",
      "Epoch 147/200 has been trained with Train Loss: 1.546, Training Accuracy: 44.2% and Testing Accuracy: 41.97%.\n",
      "Epoch 148/200 has been trained with Train Loss: 1.5514, Training Accuracy: 43.912% and Testing Accuracy: 42.31%.\n",
      "Epoch 149/200 has been trained with Train Loss: 1.551, Training Accuracy: 43.712% and Testing Accuracy: 41.78%.\n",
      "Epoch 150/200 has been trained with Train Loss: 1.5493, Training Accuracy: 43.496% and Testing Accuracy: 41.44%.\n",
      "Epoch 151/200 has been trained with Train Loss: 1.5483, Training Accuracy: 44.15% and Testing Accuracy: 42.29%.\n",
      "Epoch 152/200 has been trained with Train Loss: 1.5434, Training Accuracy: 44.036% and Testing Accuracy: 42.75%.\n",
      "Epoch 153/200 has been trained with Train Loss: 1.5491, Training Accuracy: 43.348% and Testing Accuracy: 41.74%.\n",
      "Epoch 154/200 has been trained with Train Loss: 1.5525, Training Accuracy: 43.388% and Testing Accuracy: 41.6%.\n",
      "Epoch 155/200 has been trained with Train Loss: 1.5453, Training Accuracy: 43.638% and Testing Accuracy: 40.96%.\n",
      "Epoch 156/200 has been trained with Train Loss: 1.5473, Training Accuracy: 43.692% and Testing Accuracy: 41.74%.\n",
      "Epoch 157/200 has been trained with Train Loss: 1.5431, Training Accuracy: 43.872% and Testing Accuracy: 41.84%.\n",
      "Epoch 158/200 has been trained with Train Loss: 1.5507, Training Accuracy: 43.612% and Testing Accuracy: 41.05%.\n",
      "Epoch 159/200 has been trained with Train Loss: 1.5508, Training Accuracy: 43.648% and Testing Accuracy: 42.51%.\n",
      "Epoch 160/200 has been trained with Train Loss: 1.5488, Training Accuracy: 43.848% and Testing Accuracy: 41.32%.\n",
      "Epoch 161/200 has been trained with Train Loss: 1.5506, Training Accuracy: 43.618% and Testing Accuracy: 42.27%.\n",
      "Epoch 162/200 has been trained with Train Loss: 1.5452, Training Accuracy: 44.13% and Testing Accuracy: 42.21%.\n",
      "Epoch 163/200 has been trained with Train Loss: 1.546, Training Accuracy: 44.628% and Testing Accuracy: 42.63%.\n",
      "Epoch 164/200 has been trained with Train Loss: 1.5441, Training Accuracy: 44.152% and Testing Accuracy: 41.58%.\n",
      "Epoch 165/200 has been trained with Train Loss: 1.5417, Training Accuracy: 43.478% and Testing Accuracy: 41.32%.\n",
      "Epoch 166/200 has been trained with Train Loss: 1.545, Training Accuracy: 43.598% and Testing Accuracy: 41.43%.\n",
      "Epoch 167/200 has been trained with Train Loss: 1.5418, Training Accuracy: 43.988% and Testing Accuracy: 42.18%.\n",
      "Epoch 168/200 has been trained with Train Loss: 1.5458, Training Accuracy: 44.036% and Testing Accuracy: 41.86%.\n",
      "Epoch 169/200 has been trained with Train Loss: 1.5463, Training Accuracy: 44.268% and Testing Accuracy: 41.77%.\n",
      "Epoch 170/200 has been trained with Train Loss: 1.5446, Training Accuracy: 43.42% and Testing Accuracy: 40.69%.\n",
      "Epoch 171/200 has been trained with Train Loss: 1.5434, Training Accuracy: 44.154% and Testing Accuracy: 42.03%.\n",
      "Epoch 172/200 has been trained with Train Loss: 1.5464, Training Accuracy: 43.958% and Testing Accuracy: 41.89%.\n",
      "Epoch 173/200 has been trained with Train Loss: 1.542, Training Accuracy: 43.828% and Testing Accuracy: 41.62%.\n",
      "Epoch 174/200 has been trained with Train Loss: 1.5489, Training Accuracy: 43.478% and Testing Accuracy: 41.02%.\n",
      "Epoch 175/200 has been trained with Train Loss: 1.5401, Training Accuracy: 43.34% and Testing Accuracy: 42.12%.\n",
      "Epoch 176/200 has been trained with Train Loss: 1.5452, Training Accuracy: 44.036% and Testing Accuracy: 41.6%.\n",
      "Epoch 177/200 has been trained with Train Loss: 1.5425, Training Accuracy: 43.96% and Testing Accuracy: 42.33%.\n",
      "Epoch 178/200 has been trained with Train Loss: 1.5417, Training Accuracy: 44.218% and Testing Accuracy: 41.75%.\n",
      "Epoch 179/200 has been trained with Train Loss: 1.5462, Training Accuracy: 43.978% and Testing Accuracy: 42.1%.\n",
      "Epoch 180/200 has been trained with Train Loss: 1.5433, Training Accuracy: 43.78% and Testing Accuracy: 41.84%.\n",
      "Epoch 181/200 has been trained with Train Loss: 1.5446, Training Accuracy: 44.164% and Testing Accuracy: 42.07%.\n",
      "Epoch 182/200 has been trained with Train Loss: 1.5445, Training Accuracy: 44.328% and Testing Accuracy: 42.3%.\n",
      "Epoch 183/200 has been trained with Train Loss: 1.5452, Training Accuracy: 44.232% and Testing Accuracy: 41.77%.\n",
      "Epoch 184/200 has been trained with Train Loss: 1.5456, Training Accuracy: 43.484% and Testing Accuracy: 41.62%.\n",
      "Epoch 185/200 has been trained with Train Loss: 1.5394, Training Accuracy: 43.92% and Testing Accuracy: 41.98%.\n",
      "Epoch 186/200 has been trained with Train Loss: 1.5423, Training Accuracy: 43.978% and Testing Accuracy: 41.14%.\n",
      "Epoch 187/200 has been trained with Train Loss: 1.5474, Training Accuracy: 43.286% and Testing Accuracy: 41.58%.\n",
      "Epoch 188/200 has been trained with Train Loss: 1.5399, Training Accuracy: 44.21% and Testing Accuracy: 41.07%.\n",
      "Epoch 189/200 has been trained with Train Loss: 1.5414, Training Accuracy: 43.934% and Testing Accuracy: 41.92%.\n",
      "Epoch 190/200 has been trained with Train Loss: 1.541, Training Accuracy: 43.578% and Testing Accuracy: 41.38%.\n",
      "Epoch 191/200 has been trained with Train Loss: 1.5459, Training Accuracy: 43.916% and Testing Accuracy: 41.74%.\n",
      "Epoch 192/200 has been trained with Train Loss: 1.5425, Training Accuracy: 44.264% and Testing Accuracy: 41.93%.\n",
      "Epoch 193/200 has been trained with Train Loss: 1.54, Training Accuracy: 43.922% and Testing Accuracy: 42.65%.\n",
      "Epoch 194/200 has been trained with Train Loss: 1.5435, Training Accuracy: 43.914% and Testing Accuracy: 42.12%.\n",
      "Epoch 195/200 has been trained with Train Loss: 1.5383, Training Accuracy: 44.946% and Testing Accuracy: 42.71%.\n",
      "Epoch 196/200 has been trained with Train Loss: 1.5416, Training Accuracy: 44.796% and Testing Accuracy: 42.19%.\n",
      "Epoch 197/200 has been trained with Train Loss: 1.5444, Training Accuracy: 43.64% and Testing Accuracy: 42.16%.\n",
      "Epoch 198/200 has been trained with Train Loss: 1.5405, Training Accuracy: 44.144% and Testing Accuracy: 41.22%.\n",
      "Epoch 199/200 has been trained with Train Loss: 1.5425, Training Accuracy: 44.124% and Testing Accuracy: 41.49%.\n",
      "Epoch 200/200 has been trained with Train Loss: 1.5427, Training Accuracy: 43.842% and Testing Accuracy: 41.45%.\n",
      "============= Model Build Done =============\n",
      "Time taken to build model: 367.9498 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "LAYER_NEURONS = [128, 120, 120, 10]\n",
    "LAYER_ACTIVATION_FUNCS = [None, 'relu', 'relu', 'softmax']\n",
    "LEARNING_RATE = 0.005\n",
    "EPOCHS = 200\n",
    "DROPOUT_PROB = 0.5 \n",
    "SGD_OPTIM = None\n",
    "BATCH_SIZE = 100\n",
    "WEIGHT_DECAY = 0.98\n",
    "# Instantiate the multi-layer neural network\n",
    "nn = MLP(LAYER_NEURONS, LAYER_ACTIVATION_FUNCS, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "# Perform fitting using the training dataset\n",
    "t0 = time.time()\n",
    "trial1 = nn.fit(train_df.X, train_df.y, learning_rate = LEARNING_RATE, epochs = EPOCHS, SGD_optim = SGD_OPTIM, batch_size=BATCH_SIZE )\n",
    "t1 = time.time()\n",
    "print(f\"============= Model Build Done =============\")\n",
    "print(f\"Time taken to build model: {round(t1 - t0, 4)} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NedsEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
