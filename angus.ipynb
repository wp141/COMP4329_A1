{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP4329 Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "# from ipywidgets import interact, widgets\n",
    "# from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data\n",
    "train_data = np.load(\"Assignment1-Dataset/test_data.npy\")\n",
    "train_labels = np.load(\"Assignment1-Dataset/train_label.npy\")\n",
    "\n",
    "#load testing data \n",
    "test_data = np.load(\"Assignment1-Dataset/test_data.npy\")\n",
    "test_labels = np.load(\"Assignment1-Dataset/test_label.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Visualise data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "unique_arr = np.unique(train_labels)\n",
    "print(unique_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_derivative(self, a):\n",
    "        return 1.0 - a**2\n",
    "\n",
    "    def __sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def __sigmoid_derivative(self, a):\n",
    "        return a * (1 - a)\n",
    "\n",
    "    def __relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def __relu_derivative(self, a):\n",
    "        # Note: Heaviside at 0 is often 0.5 or 1. Using 0 matches some conventions for ReLU derivative.\n",
    "        return np.heaviside(a, 0)\n",
    "\n",
    "    def __leakyrelu(self, x, alpha=0.01):\n",
    "        return np.where(x >= 0, x, alpha * x)\n",
    "\n",
    "    def __leakyrelu_derivative(self, x, alpha=0.01):\n",
    "         # Derivative is 1 for x>=0, alpha otherwise. Using heaviside approximates this.\n",
    "        return np.heaviside(x, 1) * (1 - alpha) + alpha\n",
    "\n",
    "    def __softmax(self, z):\n",
    "        # Ensure input is 2D for consistent axis operations\n",
    "        z = np.atleast_2d(z)\n",
    "        # Improve numerical stability by subtracting the max\n",
    "        max_z = np.max(z, axis=1, keepdims=True)\n",
    "        stable_z = z - max_z\n",
    "        exp_z = np.exp(stable_z)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def __softmax_derivative(self, z, z_hat):\n",
    "        # Derivative of Cross-Entropy Loss w.r.t Softmax input (z) is simply (y_hat - y)\n",
    "        # This assumes it's used with Cross-Entropy loss during backpropagation.\n",
    "        # z = true labels (one-hot), z_hat = predictions (softmax output)\n",
    "        return z_hat - z\n",
    "\n",
    "    def __init__(self, activation_function = 'relu'):\n",
    "        self.f = None\n",
    "        self.f_derivative = None\n",
    "\n",
    "        if activation_function == \"tanh\":\n",
    "            self.f = self.__tanh\n",
    "            self.f_derivative = self.__tanh_derivative\n",
    "        elif activation_function == \"sigmoid\":\n",
    "            self.f = self.__sigmoid\n",
    "            self.f_derivative = self.__sigmoid_derivative\n",
    "        elif activation_function == 'relu':\n",
    "            self.f = self.__relu\n",
    "            self.f_derivative = self.__relu_derivative\n",
    "        elif activation_function == 'leakyrelu':\n",
    "            self.f = self.__leakyrelu\n",
    "            self.f_derivative = self.__leakyrelu_derivative\n",
    "        elif activation_function == \"softmax\":\n",
    "            self.f = self.__softmax\n",
    "            # Note: Softmax derivative is often coupled with CE loss in backprop\n",
    "            self.f_derivative = self.__softmax_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining Hidden Layers\n",
    "Just going through one hidden to wrap head around it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer():\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_in,\n",
    "                 n_out,\n",
    "                 activation_function_previous_layer = 'relu',\n",
    "                 activation_function = 'relu',\n",
    "                 W = None, # Not used for initialization in current code\n",
    "                 b = None,  # Not used for initialization in current code\n",
    "                 v_W = None, # Not used in current code\n",
    "                 v_b = None, # Not used in current code\n",
    "                 last_hidden_layer = False): # Not used in current code logic\n",
    "\n",
    "        self.input = None\n",
    "        self.activation_function = Activation(activation_function).f\n",
    "\n",
    "        # Set activation derivative for the *previous* layer's activation\n",
    "        # This is used in the backward pass calculation: delta_prev = delta_curr * W * f'(input_prev)\n",
    "        self.activation_function_derivative = None\n",
    "        if activation_function_previous_layer:\n",
    "            # Get the derivative function object from the Activation class\n",
    "            self.activation_function_derivative = Activation(activation_function_previous_layer).f_derivative\n",
    "\n",
    "        # Xavier Initialisation for weights\n",
    "        self.W = np.random.uniform(low = -np.sqrt(6. / (n_in + n_out)),\n",
    "                                   high = np.sqrt(6. / (n_in + n_out )),\n",
    "                                   size = (n_in, n_out))\n",
    "\n",
    "        # Initialise bias as zeros\n",
    "        self.b = np.zeros(n_out)\n",
    "\n",
    "        # Weight scaling for sigmoid activation\n",
    "        if activation_function == 'sigmoid':\n",
    "           self.W *= 4 # Why 4? Standard practice is often just Xavier/He init.\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "\n",
    "        # Velocity terms (for momentum) are initialized but not used in the update function\n",
    "        # self.v_W = np.zeros_like(self.grad_W)\n",
    "        # self.v_b = np.zeros_like(self.grad_b)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Ensure input is at least 2D for dot product\n",
    "        input = np.atleast_2d(input)\n",
    "        self.input = input # Store the actual input to this layer\n",
    "\n",
    "        # Compute linear output: z = xW + b\n",
    "        linear_output = np.dot(self.input, self.W) + self.b\n",
    "\n",
    "        # Apply activation function: a = f(z)\n",
    "        self.output = (\n",
    "            linear_output if self.activation_function is None\n",
    "            else self.activation_function(linear_output)\n",
    "        )\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, delta, layer_number, output_layer = False):\n",
    "        # Ensure delta is 2D\n",
    "        delta = np.atleast_2d(delta)\n",
    "\n",
    "        # Calculate gradients for weights and biases\n",
    "        # dL/dW = dL/da * da/dz * dz/dW = delta * f'(z) * x  <- But f'(z) handled in delta propagation\n",
    "        # Simplified: dL/dW = x^T * delta (where delta is dL/dz for the current layer)\n",
    "        # Input needs to be from the forward pass (self.input)\n",
    "        self.grad_W = np.dot(self.input.T, delta)\n",
    "\n",
    "        # dL/db = dL/da * da/dz * dz/db = delta * f'(z) * 1 <- But f'(z) handled in delta propagation\n",
    "        # Simplified: dL/db = sum(delta) over batch\n",
    "        self.grad_b = np.average(delta, axis=0) # Average gradient over the batch\n",
    "\n",
    "        # Propagate the error (delta) to the previous layer\n",
    "        # delta_prev = dL/da_prev = dL/dz * dz/da_prev = delta * W^T\n",
    "        # We also need to multiply by the derivative of the *previous* layer's activation function:\n",
    "        # delta_prev = (delta * W^T) * f'(z_prev)\n",
    "        # Note: self.activation_function_derivative is f' of the *previous* layer's activation\n",
    "        if self.activation_function_derivative:\n",
    "            # Calculate dL/da_prev\n",
    "            delta_prev_activation = np.dot(delta, self.W.T)\n",
    "            # Calculate dL/dz_prev = dL/da_prev * f'(z_prev)\n",
    "            # Here, self.activation_function_derivative is f' and self.input is the output of the previous layer (a_prev),\n",
    "            # but we need f'(z_prev). Assuming a_prev is close enough or using the derivative w.r.t 'a' instead of 'z'.\n",
    "            # A potentially more correct way is delta_prev = np.dot(delta, self.W.T) * self.activation_function_derivative(z_prev)\n",
    "            # where z_prev would need to be stored. The current code uses f'(a_prev).\n",
    "            delta = delta_prev_activation * self.activation_function_derivative(self.input) # self.input is a_prev\n",
    "\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self,\n",
    "                 layers, # List of neuron counts, e.g., [input_dim, hidden1_dim, ..., output_dim]\n",
    "                 activation_function = [None, 'relu', 'relu','relu', 'softmax'],\n",
    "                 weight_decay = 1.0): # Note: Weight decay not implemented in update\n",
    "\n",
    "        self.layers = []\n",
    "        self.params = [] # Not used\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "        self.weight_decay = weight_decay # Stored but not used in loss/update\n",
    "\n",
    "        # Create layers\n",
    "        for i in range(len(layers)-1):\n",
    "            # Determine previous layer's activation for derivative calculation in backward pass\n",
    "            activation_prev = activation_function[i]\n",
    "            # Determine current layer's activation\n",
    "            activation_curr = activation_function[i+1]\n",
    "\n",
    "            # The 'last_hidden_layer' flag seems unused in the HiddenLayer class logic\n",
    "            last_hidden_layer = (i == len(layers) - 2)\n",
    "\n",
    "            self.layers.append(HiddenLayer(layers[i],\n",
    "                                           layers[i+1],\n",
    "                                           activation_function_previous_layer = activation_prev,\n",
    "                                           activation_function = activation_curr,\n",
    "                                           last_hidden_layer=last_hidden_layer)) # Flag seems unused\n",
    "\n",
    "    def forward(self, input):\n",
    "        current_output = input\n",
    "        for layer in self.layers:\n",
    "            current_output = layer.forward(current_output)\n",
    "        # Final output is the output of the last layer\n",
    "        return current_output\n",
    "\n",
    "    def CE_loss(self, z, z_hat):\n",
    "        # z = true labels (one-hot), z_hat = predictions (softmax output)\n",
    "        # Ensure stability: clip predictions to avoid log(0)\n",
    "        epsilon = 1e-12\n",
    "        z_hat = np.clip(z_hat, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Calculate Cross-Entropy loss\n",
    "        loss = - np.sum(z * np.log(z_hat), axis=1) # Sum over classes first\n",
    "        loss = np.mean(loss) # Average over batch\n",
    "\n",
    "        # Apply weight decay (L2 regularization) - NOTE: Not done correctly here.\n",
    "        # L2 should be added to the loss: loss += (lambda/2m) * sum(W^2)\n",
    "        # The multiplication by self.weight_decay here seems incorrect.\n",
    "        # Also, weight decay usually applies during the weight update step.\n",
    "        # loss = loss * self.weight_decay # This is likely incorrect\n",
    "\n",
    "        # Calculate the initial delta for backpropagation (dL/dz for the output layer)\n",
    "        # For Softmax + CrossEntropy, this is simply (y_hat - y)\n",
    "        delta = Activation(self.activation_function[-1]).f_derivative(z, z_hat) # y_hat - y\n",
    "\n",
    "        return loss, delta\n",
    "\n",
    "    def backward(self, delta):\n",
    "        # Backpropagate delta through layers in reverse order\n",
    "        current_delta = delta\n",
    "        # Start from the last layer and go backwards\n",
    "        for layer_number, layer in reversed(list(enumerate(self.layers))):\n",
    "             output_layer_flag = (layer_number == len(self.layers) - 1)\n",
    "             # Pass the current delta and get the delta for the previous layer\n",
    "             current_delta = layer.backward(current_delta, layer_number, output_layer=output_layer_flag)\n",
    "\n",
    "\n",
    "    def update(self, lr, SGD_optim = None): # SGD_optim not used\n",
    "        # Standard Gradient Descent update\n",
    "        # Add weight decay term here if desired: W = W - lr * (grad_W + lambda * W)\n",
    "        for layer in self.layers:\n",
    "            # Standard SGD update\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "            # Example with L2 weight decay (lambda = self.weight_decay):\n",
    "            # layer.W -= lr * (layer.grad_W + self.weight_decay * layer.W)\n",
    "            # layer.b -= lr * layer.grad_b # Bias usually not regularized\n",
    "\n",
    "\n",
    "    def fit(self, X, y, learning_rate = 0.1, epochs = 100, SGD_optim = None, batch_size = 1):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y) # Assumes y is already one-hot encoded here\n",
    "        training_loss_history = []\n",
    "        training_accuracy_history = []\n",
    "        testing_accuracy_history = []\n",
    "\n",
    "        num_samples = X.shape[0]\n",
    "\n",
    "        for k in range(epochs):\n",
    "            start_time_epoch = time.time()\n",
    "            epoch_loss = []\n",
    "\n",
    "            # Shuffle data at the beginning of each epoch\n",
    "            X_shuffled, y_shuffled = Utils.shuffle(X, y)\n",
    "\n",
    "            num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                # Get mini-batch\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min(start_idx + batch_size, num_samples)\n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "                # Forward pass\n",
    "                y_hat = self.forward(X_batch)\n",
    "\n",
    "                # Calculate loss and initial delta\n",
    "                loss, delta = self.CE_loss(y_batch, y_hat)\n",
    "                epoch_loss.append(loss)\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(delta)\n",
    "\n",
    "                # Update weights\n",
    "                self.update(learning_rate, SGD_optim)\n",
    "\n",
    "            # Calculate average loss for the epoch\n",
    "            avg_epoch_loss = np.mean(epoch_loss)\n",
    "            training_loss_history.append(avg_epoch_loss)\n",
    "\n",
    "            # --- Evaluation ---\n",
    "            # Get predictions on training and test data\n",
    "            # Note: predict method expects single samples, modify for batch prediction if needed\n",
    "            # Or iterate - inefficient for large datasets\n",
    "            train_predictions_one_hot = self.predict(train_df.X) # Using global train_df\n",
    "            test_predictions_one_hot = self.predict(test_df.X)   # Using global test_df\n",
    "\n",
    "            # Decode predictions and true labels\n",
    "            train_predictions_decoded = Preprocessing.decode(train_predictions_one_hot)\n",
    "            test_predictions_decoded = Preprocessing.decode(test_predictions_one_hot)\n",
    "\n",
    "            # Assuming test_labels is loaded globally and is [N, 1] shape\n",
    "            test_labels_flat = test_labels.flatten()\n",
    "            # Decode one-hot encoded training labels used in fit()\n",
    "            train_labels_decoded = Preprocessing.decode(y) # y was passed to fit\n",
    "\n",
    "            # Calculate accuracy\n",
    "            train_accuracy = np.mean(train_predictions_decoded == train_labels_decoded)\n",
    "            test_accuracy = np.mean(test_predictions_decoded == test_labels_flat)\n",
    "\n",
    "            training_accuracy_history.append(train_accuracy)\n",
    "            testing_accuracy_history.append(test_accuracy)\n",
    "\n",
    "            end_time_epoch = time.time()\n",
    "            epoch_duration = end_time_epoch - start_time_epoch\n",
    "\n",
    "            print(f'Epoch {k+1}/{epochs} | Train Loss: {avg_epoch_loss:.4f} | Train Acc: {train_accuracy*100:.2f}% | Test Acc: {test_accuracy*100:.2f}% | Time: {epoch_duration:.2f}s')\n",
    "\n",
    "        # Prepare output dictionary\n",
    "        output_dict = {'Training Loss': training_loss_history,\n",
    "                       'Training Accuracy': training_accuracy_history,\n",
    "                       'Testing Accuracy': testing_accuracy_history}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Assumes x is the full dataset (N, features)\n",
    "        # Iterates through each sample - potentially slow\n",
    "        x = np.array(x)\n",
    "        num_samples = x.shape[0]\n",
    "        # Initialize output array - determine output shape based on last layer\n",
    "        output_dim = self.layers[-1].W.shape[1]\n",
    "        output = np.zeros((num_samples, output_dim))\n",
    "\n",
    "        for i in range(num_samples):\n",
    "             # Pass single sample, ensure it's 2D (1, features)\n",
    "             sample = np.atleast_2d(x[i, :])\n",
    "             output[i, :] = self.forward(sample)\n",
    "\n",
    "        return output # Returns (N, num_classes) probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y # Expects raw labels (not one-hot initially)\n",
    "        # self.predictions = None # Seems unused\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"Min-Max normalization\"\"\"\n",
    "        min_val = np.min(self.X) # Global min/max, consider feature-wise\n",
    "        max_val = np.max(self.X)\n",
    "        # Avoid division by zero if max == min\n",
    "        if max_val - min_val != 0:\n",
    "            self.X = (self.X - min_val) / (max_val - min_val)\n",
    "        # Else: data is constant, normalization doesn't change it but avoids NaN\n",
    "\n",
    "    def standardize(self):\n",
    "        \"\"\"Standardization (Z-score normalization)\"\"\"\n",
    "        mean_val = np.mean(self.X) # Global mean/std, consider feature-wise\n",
    "        std_val = np.std(self.X)\n",
    "        # Avoid division by zero if std is 0\n",
    "        if std_val != 0:\n",
    "            self.X = (self.X - mean_val) / std_val\n",
    "        # Else: data is constant, standardization results in zeros but avoids NaN\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def label_encode(label_vector):\n",
    "        \"\"\"Converts a vector of integer labels into a one-hot matrix.\"\"\"\n",
    "        labels = label_vector.flatten().astype(int) # Ensure flat integer array\n",
    "        num_classes = np.max(labels) + 1 # Determine num_classes dynamically\n",
    "        # Alternative: num_classes = len(np.unique(labels)) if labels might not contain all classes 0..N-1\n",
    "\n",
    "        one_hot = np.zeros((labels.size, num_classes))\n",
    "        one_hot[np.arange(labels.size), labels] = 1\n",
    "        return one_hot\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(prediction_matrix):\n",
    "        \"\"\"Converts a matrix of probabilities/scores into class labels.\"\"\"\n",
    "        # Finds the index (class) with the highest value for each sample\n",
    "        decoded_predictions = np.argmax(prediction_matrix, axis=1)\n",
    "        return decoded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle(X, y):\n",
    "        \"\"\"Shuffles X and y arrays consistently.\"\"\"\n",
    "        assert X.shape[0] == y.shape[0], \"X and y must have the same number of samples.\"\n",
    "        shuffled_idx = np.arange(X.shape[0])\n",
    "        np.random.shuffle(shuffled_idx)\n",
    "        return X[shuffled_idx], y[shuffled_idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
