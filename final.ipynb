{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18cd80db",
      "metadata": {
        "id": "18cd80db"
      },
      "source": [
        "## COMP4329 Assignment 1\n",
        "\n",
        "SIDs:\n",
        "- 510428929\n",
        "- 510429339\n",
        "- 510429203"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ea38b7",
      "metadata": {
        "id": "d8ea38b7"
      },
      "source": [
        "### Import Libraries\n",
        "\n",
        "Note: Pandas were not used for module implementation, only for results dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e03a3c",
      "metadata": {
        "id": "b8e03a3c"
      },
      "outputs": [],
      "source": [
        "# Import relevant libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9508c1aa",
      "metadata": {
        "id": "9508c1aa"
      },
      "source": [
        "### Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f24c45f5",
      "metadata": {
        "id": "f24c45f5"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "train_data = np.load('Assignment1-Dataset/train_data.npy')\n",
        "train_label = np.load('Assignment1-Dataset/train_label.npy')\n",
        "test_data = np.load('Assignment1-Dataset/test_data.npy')\n",
        "test_label = np.load('Assignment1-Dataset/test_label.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20edb054",
      "metadata": {
        "id": "20edb054",
        "outputId": "89c4143b-9ff5-4d5d-8ccb-8ff80b4f95b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 128)\n",
            "(50000, 1)\n",
            "(10000, 128)\n",
            "(10000, 1)\n"
          ]
        }
      ],
      "source": [
        "# Check data shape\n",
        "print(train_data.shape)\n",
        "print(train_label.shape)\n",
        "print(test_data.shape)\n",
        "print(test_label.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b26734",
      "metadata": {
        "id": "b5b26734"
      },
      "source": [
        "### Activation Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9b53d40",
      "metadata": {
        "id": "d9b53d40"
      },
      "outputs": [],
      "source": [
        "class Activation:\n",
        "\n",
        "    def __logistic(self, x):\n",
        "        return 1.0 /(1.0 + np.exp(-x))\n",
        "\n",
        "    def __logistic_derivative(self, a):\n",
        "        #where a = logistic(x)\n",
        "        return a * (1-a)\n",
        "\n",
        "    def __relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def __relu_derivative(self, a):\n",
        "        return np.heaviside(a, 0)\n",
        "\n",
        "    def __leakyrelu(self, x, alpha=0.01):\n",
        "        return np.where(x >= 0, x, alpha * x)\n",
        "\n",
        "    def __leakyrelu_derivative(self, x, alpha=0.01):\n",
        "        return np.heaviside(x, 1) * (1 - alpha) + alpha\n",
        "\n",
        "    def _gelu(self, x):\n",
        "        return 0.5 * x * (1.0 + np.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
        "\n",
        "    def _gelu_derivative(self, x):\n",
        "        # Compute the inner term for tanh\n",
        "        k = np.sqrt(2.0 / np.pi) * (x + 0.044715 * np.power(x, 3))\n",
        "        tanh_k = np.tanh(k)\n",
        "\n",
        "        # First term of the derivative\n",
        "        term1 = 0.5 * (1.0 + tanh_k)\n",
        "\n",
        "        # Second term with sech^2(k) = 1 - tanh^2(k)\n",
        "        term2 = 0.5 * x * (1 - np.power(tanh_k, 2)) * np.sqrt(2.0 / np.pi) * (1 + 3 * 0.044715 * np.power(x, 2))\n",
        "\n",
        "        return term1 + term2\n",
        "\n",
        "    def __softmax(self, z):\n",
        "        z = np.atleast_2d(z)\n",
        "        max_z = np.max(z, axis=1, keepdims=True)\n",
        "        z = z - max_z\n",
        "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
        "\n",
        "    def __softmax_derivative(self, z, z_hat):\n",
        "        return z_hat - z\n",
        "\n",
        "    def __init__(self, activation_function = 'relu'):\n",
        "        if activation_function == \"logistic\":\n",
        "            self.f = self.__logistic\n",
        "            self.f_deriv = self.__logistic_derivative\n",
        "        elif activation_function == 'relu':\n",
        "            self.f = self.__relu\n",
        "            self.f_derivative = self.__relu_derivative\n",
        "        elif activation_function == 'leakyrelu':\n",
        "            self.f = self.__leakyrelu\n",
        "            self.f_derivative = self.__leakyrelu_derivative\n",
        "        elif activation_function == \"softmax\":\n",
        "            self.f = self.__softmax\n",
        "            self.f_derivative = self.__softmax_derivative\n",
        "        elif activation_function == \"gelu\":\n",
        "            self.f = self._gelu\n",
        "            self.f_derivative = self._gelu_derivative"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eea74829",
      "metadata": {
        "id": "eea74829"
      },
      "source": [
        "### Hidden Layer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36a040eb",
      "metadata": {
        "id": "36a040eb"
      },
      "outputs": [],
      "source": [
        "class HiddenLayer(object):\n",
        "    def __init__(self,\n",
        "                 n_in,\n",
        "                 n_out,\n",
        "                 activation_last_layer = 'relu',\n",
        "                 activation = 'relu',\n",
        "                 initialisation = 'xavier',\n",
        "                 W = None,\n",
        "                 b = None,\n",
        "                 v_W = None,\n",
        "                 v_b = None,\n",
        "                 last_hidden_layer = False,\n",
        "                 use_batchnorm=False):\n",
        "\n",
        "        '''\n",
        "        The class for a Hidden Layer in a MLP.\n",
        "\n",
        "        Attributes:\n",
        "        n_in (int): The dimensionality of the input to the Hidden Layer.\n",
        "        n_out (int): The dimensionality of the output, i.e. the number of hidden units.\n",
        "\n",
        "        activation_last_layer (str): The activation function of the previous Hidden Layer.\n",
        "        activation (str): The activation function of this current Hidden Layer\n",
        "\n",
        "        W (numpy array): The weight(s) applied to this current Hidden Layer. Set to None by default to allow initialisation later.\n",
        "        b (numpy array): The bias applied to this current Hidden Layer. Set to None by default to allow initialisation later.\n",
        "\n",
        "        v_W (numpy array): The 'velocity' or 'trajectory' term vt for the weight(s) in Momentum SGD. Set to None by default to allow initialisation later.\n",
        "        v_b (numpy array): The 'velocity' or 'trajectory' term vt for the bias in Momentum SGD. Set to None by default to allow initialisation later.\n",
        "\n",
        "        last_hidden_layer (bool): The boolean to determine if the current Hidden Layer object is the Last Hidden Layer in the MLP.\n",
        "        '''\n",
        "\n",
        "\n",
        "        self.last_hidden_layer = last_hidden_layer\n",
        "        self.input = None\n",
        "        self.initialisation = initialisation\n",
        "\n",
        "        #Create a Activatino object Grab the .f method from relu\n",
        "        self.activation = Activation(activation).f\n",
        "\n",
        "        #Set activation deriv of last layer, none if no last layer\n",
        "        self.activation_deriv = None\n",
        "        if activation_last_layer:\n",
        "            self.activation_deriv = Activation(activation_last_layer).f_derivative\n",
        "\n",
        "        if self.initialisation == 'xavier':\n",
        "            #Xavier Initialisation - assign random small values (from uniform dist)\n",
        "            self.W = np.random.uniform(low = -np.sqrt(6. / (n_in + n_out)),\n",
        "                                    high = np.sqrt(6. / (n_in + n_out )),\n",
        "                                    size = (n_in, n_out))\n",
        "        elif self.initialisation == 'zeros':\n",
        "            self.W = np.zeros((n_in, n_out))\n",
        "        elif self.initialisation == 'random_small':\n",
        "            self.W = np.random.randn(n_in, n_out) * 0.01\n",
        "\n",
        "        #set size of the bias as the size of the output dimension (all zero)\n",
        "        self.b = np.zeros(n_out,)\n",
        "\n",
        "        # we set he size of weight gradients as the size of weight\n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "\n",
        "        #Create array of zeros with the same shape as the gradient weights\n",
        "        self.v_W = np.zeros_like(self.grad_W)\n",
        "        self.v_b = np.zeros_like(self.grad_b)\n",
        "        self.binomial_array=np.zeros(n_out)\n",
        "\n",
        "        #setting up batch normalisation\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        self.eps = 1e-8\n",
        "        self.gamma = np.ones(n_out)  # Scale\n",
        "        self.beta = np.zeros(n_out)  # Shift\n",
        "        self.running_mean = np.zeros(n_out)\n",
        "        self.running_var = np.ones(n_out)\n",
        "\n",
        "        self.grad_gamma = np.zeros(n_out)\n",
        "        self.grad_beta = np.zeros(n_out)\n",
        "\n",
        "        self.v_gamma = np.zeros_like(self.gamma)\n",
        "        self.v_beta = np.zeros_like(self.beta)\n",
        "\n",
        "        self.num_batches_tracked = 0\n",
        "        self.sum_batch_means = np.zeros(n_out)\n",
        "        self.sum_batch_vars  = np.zeros(n_out)\n",
        "        self.fixed_batch_size = None  # set after first forward pass\n",
        "\n",
        "    @staticmethod\n",
        "    def dropout_forward(X, p_dropout):\n",
        "        '''\n",
        "        The method to perform dropout during the training of the forward pass.\n",
        "\n",
        "        Paremeters:\n",
        "        X (numpy array): The input data to be fed through the dropout forward pass.\n",
        "        p_dropout (float): The controlling factor of the proportion of neurons dropped in the network.\n",
        "\n",
        "        Returns:\n",
        "        out (numpy array): The resulting output array with values from inactive neurons as 0 and values from the active neuron equal to that of the input.\n",
        "        binomial_array (numpy array): An array with the same size of input, filled with 0s for neurons that are to be inactive and 1s for neurons that are to be active during training.\n",
        "        '''\n",
        "\n",
        "        u = np.random.binomial(1, 1 - p_dropout, size=X.shape)\n",
        "        out = X * u\n",
        "        binomial_array=u\n",
        "        return out, binomial_array\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def dropout_backward(delta, binomial_array, layer_num):\n",
        "        '''\n",
        "        The method to perform dropout during the backpropagation.\n",
        "\n",
        "        Parameters:\n",
        "        delta (numpy array): The delta generated for the backpropagation process.\n",
        "        binomial_array (numpy array): An array with the same size of input, filled with 0s for neurons that are to be inactive and 1s for neurons that are to be active during training.\n",
        "        layer_num (int): The current layer in the MLP which dropout is being performed on.\n",
        "\n",
        "        Returns:\n",
        "        delta (numpy array): The adjusted delta with dropout applied.\n",
        "        '''\n",
        "\n",
        "        delta *= nn.layers[layer_num - 1].binomial_array\n",
        "        return delta\n",
        "\n",
        "    def finalise_batchnorm_stats(self):\n",
        "        if self.num_batches_tracked == 0:\n",
        "            return  # No training batches processed\n",
        "\n",
        "        # Compute running mean as the average of all batch means\n",
        "        self.running_mean = self.sum_batch_means / self.num_batches_tracked\n",
        "\n",
        "        # get an unbiased population variance\n",
        "        m = self.fixed_batch_size\n",
        "        self.running_var = (m / (m - 1)) * (self.sum_batch_vars / self.num_batches_tracked)\n",
        "\n",
        "    #forward progress for training epoch:\n",
        "    def forward(self, input, training=True):\n",
        "        '''\n",
        "        The feedforward pass of a single Hidden Layer.\n",
        "        Applies the weights and bias to the input, performs calculations via the selected activation function and returns this output.\n",
        "\n",
        "        Parameters:\n",
        "        input (numpy array): The input data, either from the output of the previous Hidden Layer or the initial input data.\n",
        "\n",
        "        Returns:\n",
        "        self.output (numpy array): The resulting output.\n",
        "        '''\n",
        "        #Set current input for this layer\n",
        "        self.input = input\n",
        "\n",
        "        #this is the whole layer\n",
        "        lin_output = np.dot(input, self.W) + self.b #simple perceptron output\n",
        "\n",
        "        #Setting unnormalised linear output for this layer\n",
        "        self.unnorm_lin_out = lin_output\n",
        "\n",
        "        #batch normalisation (before activation function)\n",
        "        if self.use_batchnorm and not self.last_hidden_layer:\n",
        "            if training:\n",
        "                #Compute batch statistics\n",
        "                batch_mean = np.mean(lin_output, axis=0)\n",
        "                batch_var = np.var(lin_output, axis=0)\n",
        "\n",
        "                #Store statistics\n",
        "                self.batch_mean = batch_mean\n",
        "                self.batch_var = batch_var\n",
        "\n",
        "                # Normalize\n",
        "                lin_output_norm = (lin_output - batch_mean) / np.sqrt(batch_var + self.eps)\n",
        "\n",
        "                #Scale and shift\n",
        "                lin_output = self.gamma * lin_output_norm + self.beta\n",
        "\n",
        "                #Store\n",
        "                self.bn_output = lin_output\n",
        "\n",
        "                # accumulate for inference stats\n",
        "                self.num_batches_tracked += 1\n",
        "                self.sum_batch_means += batch_mean\n",
        "                self.sum_batch_vars  += batch_var\n",
        "\n",
        "                if self.fixed_batch_size is None:\n",
        "                    self.fixed_batch_size = lin_output.shape[0]\n",
        "\n",
        "            else:\n",
        "                # Inference mode: use running statistics\n",
        "                # use precomputed inference stats\n",
        "                mean = self.running_mean\n",
        "                var  = self.running_var\n",
        "                lin_output_norm = (lin_output - mean) / np.sqrt(var + self.eps)\n",
        "                lin_output = self.gamma * lin_output_norm + self.beta\n",
        "                self.bn_output = lin_output\n",
        "\n",
        "        #feed linear output into activation function\n",
        "        self.output = (\n",
        "            lin_output if self.activation is None #linear if no activation specified\n",
        "            else self.activation(lin_output) #activation fn on w*I + b  (i.e. activation function on linear output)\n",
        "        )\n",
        "\n",
        "        #\n",
        "        if not self.last_hidden_layer:\n",
        "            self.output, self.binomial_array = self.dropout_forward(self.output, DROPOUT_PROB)\n",
        "\n",
        "        #return the output\n",
        "        return self.output\n",
        "\n",
        "    #backpropagation\n",
        "    def backward(self, delta, layer_num, output_layer = False):\n",
        "        '''\n",
        "        The backward pass of a single Hidden Layer.\n",
        "\n",
        "        Parameters:\n",
        "        delta (numpy array): The delta values to be applied to the activation derivative.\n",
        "        layer_num (int): The number of the current layer in the MLP, used to check if it is not the input layer.\n",
        "        output_layer (bool): A boolean to reflect if the current layer is not the output layer.\n",
        "\n",
        "        Returns delta (numpy array): The delta for the hidden layer to be used in parameters.\n",
        "        '''\n",
        "\n",
        "        #If using Batch norm and this is not the output layer\n",
        "        if self.use_batchnorm and not self.last_hidden_layer:\n",
        "            #Gradient w.r.t gamma and beta\n",
        "            x_norm = (self.unnorm_lin_out - self.batch_mean) / np.sqrt(self.batch_var + self.eps)\n",
        "            self.grad_gamma = np.sum(delta * x_norm, axis=0)\n",
        "            self.grad_beta = np.sum(delta, axis=0)\n",
        "\n",
        "            #Backprop through BN normalisation\n",
        "            N, D = delta.shape\n",
        "\n",
        "            #Backprop through scale and shift\n",
        "            d_bn = delta * self.gamma\n",
        "\n",
        "            #Gradients for normalised input (from BN)\n",
        "            x_mu = self.unnorm_lin_out - self.batch_mean\n",
        "            std_inv = 1. / np.sqrt(self.batch_var + self.eps)\n",
        "\n",
        "            d_var = np.sum(d_bn * x_mu * -0.5 * std_inv**3, axis=0)\n",
        "            d_mean = np.sum(d_bn * -std_inv, axis=0) + d_var * np.mean(-2. * x_mu, axis=0)\n",
        "\n",
        "            delta = d_bn * std_inv + d_var * 2 * x_mu / N + d_mean / N\n",
        "\n",
        "        #Completely different formulas as this is a vectorised implementation\n",
        "\n",
        "        #calcualtes gradients as input(^t) * delta\n",
        "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
        "\n",
        "        # The gradient of the bias vector b becomes the average of the delta values across the batch\n",
        "        self.grad_b = np.average(delta, axis=0)\n",
        "\n",
        "        #\n",
        "        if self.activation_deriv:\n",
        "            #Propogates the error backward through the weights * applies the derivative of the activation function to get the true local gradient\n",
        "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "\n",
        "        #If not in the onput layer\n",
        "        if layer_num != 0:\n",
        "            #Restores dropped-out connections if dropout was applied in the forward pass\n",
        "            delta=self.dropout_backward(delta, self.binomial_array, layer_num)\n",
        "\n",
        "        #pass delta to the next (previous) layer in the backward chain to repeat\n",
        "        return delta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f28975cf",
      "metadata": {
        "id": "f28975cf"
      },
      "source": [
        "### Network Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa15ff3c",
      "metadata": {
        "id": "fa15ff3c"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    '''\n",
        "    Main class holding the structure of the Multi-Layer Perceptron.\n",
        "\n",
        "    Attributes:\n",
        "    None\n",
        "    '''\n",
        "\n",
        "    def __init__(self, layers, activation = [None, 'relu', 'relu','relu', 'softmax'], weight_decay = 1.0, initialisation='xaiver', batch_normal=False):\n",
        "\n",
        "        '''\n",
        "        The initialisation of the MLP.\n",
        "\n",
        "        Attributes:\n",
        "        layers (list of int): A list containing the number of neurons in each respective layer.\n",
        "        activation (list of str): A list containing the activation functions to be used in each respective layer. Set to [None, 'relu', 'relu', 'relu', 'softmax'] as default.\n",
        "        weight_decay (float): The value set for the weight decay to be applied. Value of 1.0 indicates no weight decay to be applied.\n",
        "        '''\n",
        "\n",
        "        self.batch_normal = batch_normal\n",
        "        #Will contain all the hidden layer objects\n",
        "        self.layers = []\n",
        "        self.params = []\n",
        "\n",
        "        #Activation functions for each layer\n",
        "        self.activation = activation\n",
        "\n",
        "        #Weight decay coefficient\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        #Loop through the list of layers. initialise a new hidden layer object for each layer\n",
        "        for i in range(len(layers)-1):\n",
        "\n",
        "            last_hidden_layer = False\n",
        "\n",
        "            if i == len(layers) - 2: # -2 because -1 for output layer, and another -1 since it's index 0\n",
        "                last_hidden_layer = True\n",
        "\n",
        "            self.layers.append(HiddenLayer(layers[i],\n",
        "                                           layers[i+1],\n",
        "                                           activation[i],\n",
        "                                           activation[i+1],\n",
        "                                           initialisation=initialisation,\n",
        "                                           last_hidden_layer=last_hidden_layer,\n",
        "                                           use_batchnorm=self.batch_normal))\n",
        "\n",
        "    def forward(self, input, training=True):\n",
        "        '''\n",
        "        The feedforward process conducted sequentially through each layer in the MLP.\n",
        "        Takes the input from the previous layer (or initial data if it is the input layer), applies weights & bias then activation function and feeds the resulting output as the input to the next layer via the HiddenLayer.forward() method.\n",
        "\n",
        "        Parameters:\n",
        "        input (numpy array): The input array to be fed through the feedforward process.\n",
        "\n",
        "        Returns:\n",
        "        output (numpy array): The resulting final output from the feedforward process across all layers.\n",
        "        '''\n",
        "\n",
        "        #Perform forward propogation on each layer object\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(input, training=training)\n",
        "\n",
        "            input = output\n",
        "        return output\n",
        "\n",
        "\n",
        "    def CE_loss(self, y, y_hat):\n",
        "        '''\n",
        "        The calculation of the Cross-Entropy loss function.\n",
        "        Computes the cross entropy loss, averages this and applies weight decay (if applicable) as well as calculating the respective delta to be used in the backpropagation process.\n",
        "\n",
        "        Parameters:\n",
        "        y (numpy array): The actual y values (or labels) from the data set.\n",
        "        y_hat (numpy array): The calculated y values (y hat) as output from the feedforward process.\n",
        "\n",
        "        Returns:\n",
        "        loss (float): The calculated Cross Entropy Loss value.\n",
        "        delta (numpy array): The calculated delta array to be used in the backpropagation process.\n",
        "        '''\n",
        "        epsilon = 1e-12\n",
        "        y_hat = np.clip(y_hat, epsilon, 1. - epsilon)  # avoid log(0)\n",
        "        loss = - np.nansum(y * np.log(y_hat)) / y.shape[0]\n",
        "        loss *= self.weight_decay\n",
        "        delta = Activation(self.activation[-1]).f_derivative(y, y_hat)\n",
        "        return loss, delta\n",
        "\n",
        "    def backward(self, delta):\n",
        "        '''\n",
        "        The backpropagation process conducted backwards across each layer in the MLP.\n",
        "        Updates the delta via the Hidden Layer backward process and applies this updated delta as the delta input in the HiddenLayer.backward() method.\n",
        "\n",
        "        Parameters:\n",
        "        delta (numpy array): The value for delta calculated in the Loss function.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        '''\n",
        "\n",
        "        delta = self.layers[-1].backward(delta, len(self.layers) -1, output_layer = True)\n",
        "        for layer_num, layer in reversed(list(enumerate(self.layers[:-1]))):\n",
        "            delta = layer.backward(delta, layer_num)\n",
        "\n",
        "    def update(self, lr, SGD_optim):\n",
        "      '''\n",
        "      The method to update the parameters under Stochastic Gradient Descent (SGD).\n",
        "      Updates the weights and bias parameters based on the learning rate and respective gradient. Includes functionality for applying SGD Momentum optimization.\n",
        "\n",
        "      Parameters:\n",
        "      lr (float): The learning rate for the parameter updates.\n",
        "      SGD_optim (dict of str: str): The SGD Optimization values as a dictionary with keys 'Type': as the type of optimisation and 'Parameters': for the optimization parameter value.\n",
        "\n",
        "      Returns:\n",
        "      None\n",
        "      '''\n",
        "\n",
        "      #Update without momentum\n",
        "      if SGD_optim is None:\n",
        "          for layer in self.layers:\n",
        "            #Update weight and bias parameters\n",
        "            layer.W -= lr * layer.grad_W\n",
        "            layer.b -= lr * layer.grad_b\n",
        "\n",
        "            if layer.use_batchnorm:\n",
        "                #Update gamma and beta params for batch normalisation\n",
        "                layer.gamma -= lr * layer.grad_gamma\n",
        "                layer.beta  -= lr * layer.grad_beta\n",
        "\n",
        "      #Update with momentum\n",
        "      elif SGD_optim['Type'] == 'Momentum':\n",
        "          for layer in self.layers:\n",
        "              layer.v_W = (SGD_optim['Parameter'] * layer.v_W) + (lr * layer.grad_W)\n",
        "              layer.v_b = (SGD_optim['Parameter'] * layer.v_b) + (lr * layer.grad_b)\n",
        "              layer.W = layer.W - layer.v_W\n",
        "              layer.b = layer.b - layer.v_b\n",
        "\n",
        "              if layer.use_batchnorm:\n",
        "                #For BN parameters\n",
        "                layer.v_gamma= (SGD_optim['Parameter'] * layer.v_gamma) + (lr * layer.grad_gamma)\n",
        "                layer.v_beta = (SGD_optim['Parameter'] * layer.v_beta) + (lr * layer.grad_beta)\n",
        "\n",
        "                layer.gamma -= layer.v_gamma\n",
        "                layer.beta -= layer.v_beta\n",
        "\n",
        "    def fit(self, X, y, learning_rate = 0.1, epochs = 100, SGD_optim = None, batch_size = 1):\n",
        "        '''\n",
        "        The method to fit the MLP.\n",
        "        Iterates through epochs, runs the forward process to calculate respective loss (and delta) then runs the backpropagation process to update the parameters.\n",
        "\n",
        "        Parameters:\n",
        "        X (numpy array): The input X values.\n",
        "        y (numpy array): The corresponding y values (or labels).\n",
        "        learning_rate (float): The learning rate to be used in the parameter updates. Set to 0.1 by default.\n",
        "        epochs (int): The number of times the dataset is passed through the MLP. Set to 100 by default.\n",
        "        SGD_optim (dict of str: str): A dictionary containing the type of optimization and the respective optimization algorithm parameter to be used. Set as None by default.\n",
        "        batch_size (int): The size of the batches to be used in Mini-Batch learning.\n",
        "\n",
        "        Returns:\n",
        "        output_dct (dict of float): A dictionary containing the training cross-entropy loss, training accuracy and testing accuracy.\n",
        "        '''\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        training_loss = []\n",
        "        training_accuracy = []\n",
        "        testing_accuracy = []\n",
        "\n",
        "        #Split the data into batches\n",
        "        num_batches = int(np.ceil(X.shape[0] / batch_size))\n",
        "\n",
        "        #Perform epochs on batches\n",
        "        for k in range(epochs):\n",
        "\n",
        "            loss = np.zeros(num_batches)\n",
        "\n",
        "            current_idx = 0\n",
        "\n",
        "            #Shuffle the data, to ensure that each epoch will have different sequence of observations\n",
        "            X, y = Utils.shuffle(X, y)\n",
        "\n",
        "            for batch_idx in range(num_batches):\n",
        "\n",
        "                #forward pass\n",
        "                y_hat = self.forward(X[current_idx : current_idx + batch_size, :], training=True)\n",
        "\n",
        "                #backward pass\n",
        "                loss[batch_idx], delta = self.CE_loss(y[current_idx : current_idx + batch_size], y_hat)\n",
        "\n",
        "                self.backward(delta)\n",
        "\n",
        "                #update\n",
        "                self.update(learning_rate, SGD_optim)\n",
        "\n",
        "                #Update the index based on the batch window for the next round of Mini-Batch learning.\n",
        "                if (current_idx + batch_size) > X.shape[0]:\n",
        "                    batch_size = X.shape[0] - current_idx\n",
        "                current_idx += batch_size\n",
        "\n",
        "            # Finalize batchnorm running statistics for inference\n",
        "            for layer in self.layers:\n",
        "                if hasattr(layer, 'finalize_batchnorm_stats'):\n",
        "                    layer.finalize_batchnorm_stats()\n",
        "\n",
        "            #Predict and compute metrics for each run\n",
        "            test_predict = self.predict(test_df.X)\n",
        "            train_predict = self.predict(train_df.X)\n",
        "            test_predict = test_df.decode(test_predict)\n",
        "            train_predict = train_df.decode(train_predict)\n",
        "            test_accuracy = np.sum(test_predict == test_label[:, 0]) / test_predict.shape[0]\n",
        "            train_accuracy = np.sum(train_predict == train_label[:, 0]) / train_predict.shape[0]\n",
        "\n",
        "            training_loss.append(np.mean(loss))\n",
        "            training_accuracy.append(train_accuracy)\n",
        "            testing_accuracy.append(test_accuracy)\n",
        "\n",
        "            output_dict = {'Training Loss': training_loss, 'Training Accuracy': training_accuracy, 'Testing Accuracy': testing_accuracy}\n",
        "\n",
        "            print(f'Epoch {k+1}/{epochs} has been trained with Train Loss: {str(round(training_loss[-1], 4))}, Training Accuracy: {str(round(training_accuracy[-1] * 100, 4))}% and Testing Accuracy: {str(round(testing_accuracy[-1] * 100, 4))}%.')\n",
        "\n",
        "        return output_dict\n",
        "\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        The method to predict values based on input x by running forward process through the fitted MLP.\n",
        "\n",
        "        Parameters:\n",
        "        x (numpy array): The input x values on which to compute predictions.\n",
        "\n",
        "        Returns:\n",
        "        output (numpy array): The resulting predictions.\n",
        "        '''\n",
        "\n",
        "        x = np.array(x)\n",
        "        output = [i for i in range(x.shape[0])]\n",
        "        for i in np.arange(x.shape[0]):\n",
        "            output[i] = self.forward(x[i, :], training=False)\n",
        "        output = np.array(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "042d7f8b",
      "metadata": {
        "id": "042d7f8b"
      },
      "source": [
        "### Data Preprocessing methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2301164",
      "metadata": {
        "id": "f2301164"
      },
      "outputs": [],
      "source": [
        "class Preprocessing:\n",
        "    '''\n",
        "    The Class to apply preprocessing methods.\n",
        "\n",
        "    Attributes:\n",
        "    X (numpy array): The input array of X values.\n",
        "    y (numpy array): The input array of y values (or labels).\n",
        "    '''\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.predictions = None\n",
        "\n",
        "    def normalize(self):\n",
        "        '''\n",
        "        Normalizes and transforms the X values based on min-max normalization.\n",
        "\n",
        "        Parameters:\n",
        "        None\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        '''\n",
        "\n",
        "        norm_data = (self.X - np.min(self.X))/(np.max(self.X) - np.min(self.X))\n",
        "        self.X = norm_data\n",
        "\n",
        "    def standardize(self):\n",
        "        '''\n",
        "        Standardizes and transforms the X values based on the mean & standard deviation.\n",
        "\n",
        "        Parameters:\n",
        "        None\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        '''\n",
        "\n",
        "        self.X = (self.X - np.mean(self.X)) / np.std(self.X)\n",
        "\n",
        "    @staticmethod\n",
        "    def label_encode(label_vector):\n",
        "        '''\n",
        "        Encodes the label (y) values based on one-hot encoding.\n",
        "        Creates an empty list for each observation, fills it with zeros then set the index of the class label to 1.\n",
        "\n",
        "        Parameters:\n",
        "        label_vector (numpy array): The label array to be one-hot encoded.\n",
        "\n",
        "        Returns:\n",
        "        encoded_label_vector (numpy array): The resulting one-hot encoded array for the labels.\n",
        "        '''\n",
        "\n",
        "        num_classes = np.unique(label_vector).size\n",
        "\n",
        "        encoded_label_vector = []\n",
        "\n",
        "        for label in label_vector:\n",
        "            encoded_label = np.zeros(num_classes)\n",
        "            encoded_label[int(label)] = 1\n",
        "            encoded_label_vector.append(encoded_label)\n",
        "\n",
        "        encoded_label_vector = np.array(encoded_label_vector)\n",
        "\n",
        "        return encoded_label_vector\n",
        "\n",
        "    @staticmethod\n",
        "    def decode(prediction_matrix):\n",
        "        '''\n",
        "        Transforms a one-hot encoded matrix back to a class label.\n",
        "        Creates a zero array and fills it with the index of maximum value (i.e. 1) in the one-hot encoded array.\n",
        "\n",
        "        Parameters:\n",
        "        prediction_matrix (numpy array): The one-hot encoded label matrix.\n",
        "\n",
        "        Returns:\n",
        "        decoded_predictions (numpy array): A numpy array filled with the labels.\n",
        "        '''\n",
        "\n",
        "        decoded_predictions = np.zeros(prediction_matrix.shape[0])\n",
        "        for prediction_idx, prediction_vector in enumerate(prediction_matrix):\n",
        "            decoded_predictions[prediction_idx] = int(np.argmax(prediction_vector)) # we add the two index zeros because it's a nparray within a tuple\n",
        "\n",
        "        return decoded_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bc2e57d",
      "metadata": {
        "id": "0bc2e57d"
      },
      "source": [
        "### Miscellaneous methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5450437",
      "metadata": {
        "id": "c5450437"
      },
      "outputs": [],
      "source": [
        "class Utils:\n",
        "    '''\n",
        "    Class used to contain miscellaneous methods.\n",
        "\n",
        "    Attributes:\n",
        "    None\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle(X, y):\n",
        "        '''\n",
        "        Randomly shuffles the data.\n",
        "\n",
        "        Parameters:\n",
        "        X (numpy array): The X values to be shuffled.\n",
        "        y (numpy array): The y values to be shuffled.\n",
        "\n",
        "        Returns:\n",
        "        X (numpy array), y (numpy array): The pair of the shuffled X & y numpy arrays.\n",
        "        '''\n",
        "        shuffled_idx = np.arange(X.shape[0])\n",
        "        np.random.shuffle(shuffled_idx)\n",
        "        X = X[shuffled_idx]\n",
        "        y = y[shuffled_idx]\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    @staticmethod\n",
        "    def create_confusion_mat(df):\n",
        "      '''\n",
        "      Creates a confusion matrix based on a Preprocessing object that has X, y and predicted y values.\n",
        "      Calculates the values to be placed in respective row/columns by summing the occurences in pairwise indices for the original and predicted y values.\n",
        "\n",
        "      Parameters:\n",
        "      df (Preprocessing): A Preprocessing object with original X, original y and predicted y values.\n",
        "\n",
        "      Returns:\n",
        "      confusion_mat (pandas DataFrame): A confusion matrix represented as a pandas DataFrame, where the rows (indexes) reflect predicted values and the columns reflect actual values.\n",
        "      '''\n",
        "\n",
        "      confusion_mat = pd.DataFrame(0, index = np.unique(df.y) , columns = np.unique(df.y))\n",
        "      for i in range(0, len(df.y)):\n",
        "        confusion_mat[int(df.y[i])].iloc[int(df.predictions[i])] += 1\n",
        "      return confusion_mat\n",
        "\n",
        "    @staticmethod\n",
        "    def confusion_mat_measures(confusion_matrix):\n",
        "      '''\n",
        "      Produces a pandas DataFrame with Precision, Recall and F1 measures per class.\n",
        "      First calculates True Positive (TP), False Negative (FN), False Positive (FP) and True Negative (TN) values then calculates Precision, Recall and F1 values and stores them in a DataFrame.\n",
        "\n",
        "      Parameters:\n",
        "      confusion_matrix (pandas DataFrame): A confusion matrix as a Pandas DataFrame Object.\n",
        "\n",
        "      Returns:\n",
        "      scores_df (pandas Dataframe): A DataFrame with labels as rows (indexes) and Precision, Recall and F1 scores as columns.\n",
        "      '''\n",
        "\n",
        "      scores_df = pd.DataFrame(0, index = confusion_matrix.index, columns = ['Precision', 'Recall', 'F1'])\n",
        "      for  i in confusion_matrix.index:\n",
        "        TP = confusion_matrix[i][i]\n",
        "        FN = np.array(confusion_matrix[i].iloc[0:i].values.tolist() + confusion_matrix[i].iloc[i+1:].values.tolist()).sum()\n",
        "        FP = np.array(confusion_matrix.iloc[i][0:i].values.tolist() + confusion_matrix.iloc[i][i + 1:].values.tolist()).sum()\n",
        "        TN = confusion_matrix.sum().sum() - TP - FN - FP\n",
        "\n",
        "        Precision = TP / (TP + FP)\n",
        "        Recall = TP / (TP + FN)\n",
        "        F1 = (2 * Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "        scores_df.loc[i, 'Precision'] = Precision\n",
        "        scores_df.loc[i, 'Recall'] = Recall\n",
        "        scores_df.loc[i, 'F1'] = F1\n",
        "\n",
        "      scores_df.index.name = 'Label'\n",
        "\n",
        "      return scores_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b10b8e4c",
      "metadata": {
        "id": "b10b8e4c"
      },
      "source": [
        "### Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db653338",
      "metadata": {
        "id": "db653338",
        "outputId": "77cd83ed-f8b5-4251-fd2d-54cc71c2b6e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-f75cf5088000>:61: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  encoded_label[int(label)] = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300 has been trained with Train Loss: 1.7924, Training Accuracy: 39.218% and Testing Accuracy: 38.51%.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1444ed833d4c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Perform fitting using the training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrial1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGD_optim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD_OPTIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"============= Model Build Done =============\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-aa4091f223e2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, learning_rate, epochs, SGD_optim, batch_size)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m#Predict and compute metrics for each run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mtest_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mtrain_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mtest_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mtrain_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-aa4091f223e2>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-aa4091f223e2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, training)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m#Perform forward propogation on each layer object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-83bbe57b3ed9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, training)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m#this is the whole layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mlin_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;31m#simple perceptron output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m#Setting unnormalised linear output for this layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Instantiating our data and pre-processing it as required\n",
        "train_df = Preprocessing(train_data, train_label)\n",
        "test_df = Preprocessing(test_data, test_label)\n",
        "\n",
        "# Standardize X matrix (features)\n",
        "# train_df.normalize()\n",
        "# test_df.normalize()\n",
        "train_df.standardize()\n",
        "test_df.standardize()\n",
        "\n",
        "# Perform one-hot encoding for our label vector (ONLY ON TRAIN)\n",
        "train_df.y = train_df.label_encode(train_df.y)\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "LAYER_NEURONS = [128, 150, 10]\n",
        "LAYER_ACTIVATION_FUNCS = [None, 'leakyrelu', 'softmax']\n",
        "LEARNING_RATE = 0.005\n",
        "EPOCHS = 300\n",
        "DROPOUT_PROB = 0.5\n",
        "SGD_OPTIM = None\n",
        "BATCH_SIZE = 100\n",
        "WEIGHT_DECAY = 0.98\n",
        "BATCH_NORMAL = False\n",
        "INITIALISATION = 'xavier'\n",
        "\n",
        "# Instantiate the multi-layer neural network\n",
        "nn = MLP(LAYER_NEURONS, LAYER_ACTIVATION_FUNCS, weight_decay = WEIGHT_DECAY, initialisation=INITIALISATION, batch_normal=BATCH_NORMAL)\n",
        "\n",
        "# Perform fitting using the training dataset\n",
        "t0 = time.time()\n",
        "trial1 = nn.fit(train_df.X, train_df.y, learning_rate = LEARNING_RATE, epochs = EPOCHS, SGD_optim = SGD_OPTIM, batch_size=BATCH_SIZE )\n",
        "t1 = time.time()\n",
        "print(f\"============= Model Build Done =============\")\n",
        "print(f\"Time taken to build model: {round(t1 - t0, 4)} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1616b5f",
      "metadata": {
        "id": "f1616b5f"
      },
      "source": [
        "### Performance Figures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7141f53c",
      "metadata": {
        "id": "7141f53c"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize = (20, 10))\n",
        "ax[0].plot(trial1['Training Loss'])\n",
        "ax[0].title.set_text(\"Cross-Entropy Loss over Epoch\")\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[1].plot(trial1['Training Accuracy'], label = \"Training Accuracy\")\n",
        "ax[1].plot(trial1['Testing Accuracy'], label = \"Testing Accuracy\")\n",
        "ax[1].title.set_text(\"Training & Testing Accuracy over Epoch\")\n",
        "ax[1].set_xlabel(\"Epoch\")\n",
        "ax[1].set_ylabel(\"Accuracy\")\n",
        "ax[1].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d785dfa",
      "metadata": {
        "id": "9d785dfa",
        "outputId": "8fc1d43a-3979-4592-a187-3d59b9bc2d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Cross-Entropy Training Loss: 1.4412.\n",
            "Final Train accuracy: 48.556%.\n",
            "Final Test accuracy: 44.64%.\n",
            "Final Average F1 Score: 0.4407.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/tk/4s9pjgcj79dbp8y5rdx60v_h0000gn/T/ipykernel_78806/4225359946.py:43: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  confusion_mat[int(df.y[i])].iloc[int(df.predictions[i])] += 1\n",
            "/var/folders/tk/4s9pjgcj79dbp8y5rdx60v_h0000gn/T/ipykernel_78806/4225359946.py:43: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  confusion_mat[int(df.y[i])].iloc[int(df.predictions[i])] += 1\n",
            "/var/folders/tk/4s9pjgcj79dbp8y5rdx60v_h0000gn/T/ipykernel_78806/4225359946.py:70: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.5101123595505618' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  scores_df.loc[i, 'Precision'] = Precision\n",
            "/var/folders/tk/4s9pjgcj79dbp8y5rdx60v_h0000gn/T/ipykernel_78806/4225359946.py:71: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.454' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  scores_df.loc[i, 'Recall'] = Recall\n",
            "/var/folders/tk/4s9pjgcj79dbp8y5rdx60v_h0000gn/T/ipykernel_78806/4225359946.py:72: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.48042328042328036' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  scores_df.loc[i, 'F1'] = F1\n"
          ]
        }
      ],
      "source": [
        "test_df.predictions = nn.predict(test_df.X)\n",
        "train_df.predictions = nn.predict(train_df.X)\n",
        "test_df.predictions = test_df.decode(test_df.predictions)\n",
        "train_df.predictions = train_df.decode(train_df.predictions)\n",
        "\n",
        "CM = Utils.create_confusion_mat(test_df)\n",
        "\n",
        "measures = Utils.confusion_mat_measures(CM)\n",
        "\n",
        "\n",
        "# Accuracy & Performance Metrics\n",
        "test_accuracy = np.sum(test_df.predictions == test_df.y[:, 0]) / test_df.predictions.shape[0]\n",
        "train_accuracy = np.sum(train_df.predictions == train_label[:, 0]) / train_df.predictions.shape[0]\n",
        "F1_avg = measures['F1'].mean()\n",
        "CELoss = trial1['Training Loss'][-1]\n",
        "print(f'Final Cross-Entropy Training Loss: {round(CELoss, 4)}.')\n",
        "print(f'Final Train accuracy: {round(train_accuracy * 100, 4)}%.')\n",
        "print(f'Final Test accuracy: {round(test_accuracy * 100, 4)}%.')\n",
        "print(f'Final Average F1 Score: {round(F1_avg, 4)}.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62bec743",
      "metadata": {
        "id": "62bec743"
      },
      "source": [
        "### Ablation testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "888b8c0f",
      "metadata": {
        "id": "888b8c0f"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "train_df = Preprocessing(train_data, train_label)\n",
        "test_df = Preprocessing(test_data, test_label)\n",
        "\n",
        "train_df.standardize()\n",
        "test_df.standardize()\n",
        "\n",
        "train_df.y = train_df.label_encode(train_df.y)\n",
        "\n",
        "\n",
        "# --- Baseline/Default Hyperparameters ---\n",
        "BASELINE_NUM_HIDDEN_LAYERS = 3\n",
        "BASELINE_NEURONS_PER_LAYER = 100\n",
        "BASELINE_HIDDEN_ACTIVATION = 'relu'\n",
        "BASELINE_SGD_OPTIM = None\n",
        "BASELINE_WEIGHT_DECAY = 0.98\n",
        "BASELINE_INITIALISATION = 'xavier'\n",
        "BASELINE_BATCH_SIZE = 100\n",
        "BASELINE_DROPOUT_PROB = 0.5\n",
        "BASELINE_LEARNING_RATE = 0.005\n",
        "DROPOUT_PROB = BASELINE_DROPOUT_PROB\n",
        "EPOCHS = 300\n",
        "OUTPUT_ACTIVATION = 'softmax'\n",
        "INPUT_ACTIVATION = None\n",
        "\n",
        "\n",
        "input_layer_size = train_df.X.shape[1]\n",
        "try:\n",
        "    output_layer_size = len(np.unique(train_label))\n",
        "except NameError:\n",
        "     try: output_layer_size = len(np.unique(train_df.y))\n",
        "     except AttributeError: output_layer_size = train_df.y.shape[1]\n",
        "\n",
        "print(f\"Data Shapes: Input X={train_df.X.shape}, Train y={train_df.y.shape}\")\n",
        "print(f\"Deduced/Set Parameters: Input Size={input_layer_size}, Output Size={output_layer_size}\")\n",
        "\n",
        "BASELINE_HIDDEN_NEURONS = [BASELINE_NEURONS_PER_LAYER] * BASELINE_NUM_HIDDEN_LAYERS\n",
        "BASELINE_LAYER_NEURONS = [input_layer_size] + BASELINE_HIDDEN_NEURONS + [output_layer_size]\n",
        "BASELINE_ACTIVATION_FUNCS = [INPUT_ACTIVATION] + ([BASELINE_HIDDEN_ACTIVATION] * BASELINE_NUM_HIDDEN_LAYERS) + [OUTPUT_ACTIVATION]\n",
        "\n",
        "\n",
        "# --- Storage for Results ---\n",
        "neuron_study_results = {}\n",
        "activation_study_results = {}\n",
        "num_layers_study_results = {}\n",
        "sgd_optim_study_results = {}\n",
        "weight_decay_study_results = {}\n",
        "batch_size_study_results = {}\n",
        "dropout_prob_study_results = {}\n",
        "learning_rate_study_results = {}\n",
        "\n",
        "# --- Helper Function for Metric Calculation (to reduce repetition) ---\n",
        "def calculate_metrics(nn_model, trial_hist, test_dataframe, train_dataframe):\n",
        "    \"\"\"Calculates and returns key metrics after model training.\"\"\"\n",
        "    print(\"    Calculating performance metrics...\")\n",
        "    test_accuracy, F1_avg, CELoss = -1.0, -1.0, -1.0 # Defaults\n",
        "\n",
        "    try:\n",
        "        test_preds_raw = nn_model.predict(test_dataframe.X)\n",
        "        # Decode predictions (ensure decode methods exist and work)\n",
        "        test_preds_decoded = test_dataframe.decode(test_preds_raw)\n",
        "        try:\n",
        "            # Using the structure from your snippet:\n",
        "            test_accuracy = np.sum(test_preds_decoded == test_dataframe.y[:, 0]) / test_preds_decoded.shape[0]\n",
        "            original_predictions_attr = getattr(test_dataframe, 'predictions', None)\n",
        "            test_dataframe.predictions = test_preds_decoded\n",
        "            CM = Utils.create_confusion_mat(test_dataframe)\n",
        "            measures = Utils.confusion_mat_measures(CM)\n",
        "            F1_avg = measures['F1'].mean()\n",
        "            if original_predictions_attr is not None: # Restore original\n",
        "                 test_dataframe.predictions = original_predictions_attr\n",
        "            else: # Clean up\n",
        "                delattr(test_dataframe, 'predictions')\n",
        "\n",
        "\n",
        "        except Exception as metric_e:\n",
        "            print(f\"    ERROR calculating accuracy/F1: {metric_e}\")\n",
        "        try:\n",
        "            if isinstance(trial_hist, dict) and 'Training Loss' in trial_hist and len(trial_hist['Training Loss']) > 0:\n",
        "                 CELoss = trial_hist['Training Loss'][-1]\n",
        "            else:\n",
        "                 print(f\"    WARNING: Could not find 'Training Loss' in trial_hist or it's empty/not a dict. trial_hist type: {type(trial_hist)}\")\n",
        "        except Exception as loss_e:\n",
        "             print(f\"    ERROR accessing training loss: {loss_e}\")\n",
        "\n",
        "        print(f\"    Metrics: Test Accuracy={test_accuracy:.4f}, Avg F1={F1_avg:.4f}, Final Train Loss={CELoss:.4f}\")\n",
        "    except AttributeError as ae:\n",
        "         print(f\"    ERROR: Missing method/attribute like '.predict', '.decode', or '.y'? Error: {ae}\")\n",
        "    except Exception as e:\n",
        "        print(f\"    ERROR during metrics calculation: {e}\")\n",
        "\n",
        "    return test_accuracy, F1_avg, CELoss\n",
        "\n",
        "# ================================================================\n",
        "# PHASE 1: Neurons Per Hidden Layer\n",
        "# ================================================================\n",
        "neurons_per_layer_to_test = [50, 100, 150]\n",
        "fixed_activation_funcs_p1 = [INPUT_ACTIVATION] + ([BASELINE_HIDDEN_ACTIVATION] * BASELINE_NUM_HIDDEN_LAYERS) + [OUTPUT_ACTIVATION]\n",
        "for i, neurons_per_layer in enumerate(neurons_per_layer_to_test):\n",
        "    current_hidden_neurons = [neurons_per_layer] * BASELINE_NUM_HIDDEN_LAYERS\n",
        "    current_layer_neurons = [input_layer_size] + current_hidden_neurons + [output_layer_size]\n",
        "    config_key = f\"neurons_{'_'.join(map(str, current_hidden_neurons))}\"\n",
        "    print(f\"\\n--- Phase 1 / Trial {i+1}: Testing Neurons = {current_layer_neurons} ---\")\n",
        "    try:\n",
        "        nn = MLP(current_layer_neurons, fixed_activation_funcs_p1, weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
        "        t0 = time.time()\n",
        "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=BASELINE_BATCH_SIZE)\n",
        "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
        "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
        "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
        "        active_dropout_prob = DROPOUT_PROB\n",
        "        neuron_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': current_layer_neurons, 'activation_config': fixed_activation_funcs_p1, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': BASELINE_WEIGHT_DECAY, 'batch_size_tested': BASELINE_BATCH_SIZE, 'dropout_prob_tested': active_dropout_prob, 'learning_rate_tested': BASELINE_LEARNING_RATE, 'history': trial_result } # Store history too if needed\n",
        "    except Exception as e: print(f\"    ERROR: {e}\"); neuron_study_results[config_key] = {'error': str(e)}\n",
        "print(\"\\n\" + \"=\"*60 + \"\\nPhase 1 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================================================================\n",
        "# PHASE 2: Activation Functions\n",
        "# ================================================================\n",
        "hidden_activations_to_test = ['relu', 'leakyrelu', 'gelu']\n",
        "fixed_layer_neurons_p2 = BASELINE_LAYER_NEURONS\n",
        "for i, activation_func in enumerate(hidden_activations_to_test):\n",
        "    current_activation_funcs = [INPUT_ACTIVATION] + ([activation_func] * BASELINE_NUM_HIDDEN_LAYERS) + [OUTPUT_ACTIVATION]\n",
        "    config_key = f\"activation_{activation_func}\"\n",
        "    print(f\"\\n--- Phase 2 / Trial {i+1}: Testing Activation = '{activation_func}' ---\")\n",
        "    try:\n",
        "        nn = MLP(fixed_layer_neurons_p2, current_activation_funcs, weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
        "        t0 = time.time()\n",
        "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=BASELINE_BATCH_SIZE)\n",
        "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
        "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
        "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
        "        active_dropout_prob = DROPOUT_PROB\n",
        "        activation_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': fixed_layer_neurons_p2, 'activation_config': current_activation_funcs, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': BASELINE_WEIGHT_DECAY, 'batch_size_tested': BASELINE_BATCH_SIZE, 'dropout_prob_tested': active_dropout_prob, 'learning_rate_tested': BASELINE_LEARNING_RATE, 'history': trial_result }\n",
        "    except Exception as e: print(f\"    ERROR: {e}\"); activation_study_results[config_key] = {'error': str(e)}\n",
        "print(\"\\n\" + \"=\"*60 + \"\\nPhase 2 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================================================================\n",
        "# PHASE 3: Number of Hidden Layers\n",
        "# ================================================================\n",
        "num_hidden_layers_to_test = [1, 3, 5]\n",
        "for i, num_hidden in enumerate(num_hidden_layers_to_test):\n",
        "    current_hidden_neurons = [BASELINE_NEURONS_PER_LAYER] * num_hidden\n",
        "    current_layer_neurons = [input_layer_size] + current_hidden_neurons + [output_layer_size]\n",
        "    current_activation_funcs = [INPUT_ACTIVATION] + ([BASELINE_HIDDEN_ACTIVATION] * num_hidden) + [OUTPUT_ACTIVATION]\n",
        "    config_key = f\"num_hidden_{num_hidden}\"\n",
        "    print(f\"\\n--- Phase 3 / Trial {i+1}: Testing Num Hidden Layers = {num_hidden} ---\")\n",
        "    try:\n",
        "        nn = MLP(current_layer_neurons, current_activation_funcs, weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
        "        t0 = time.time()\n",
        "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=BASELINE_BATCH_SIZE)\n",
        "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
        "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
        "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
        "        active_dropout_prob = DROPOUT_PROB\n",
        "        num_layers_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': current_layer_neurons, 'activation_config': current_activation_funcs, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': BASELINE_WEIGHT_DECAY, 'batch_size_tested': BASELINE_BATCH_SIZE, 'dropout_prob_tested': active_dropout_prob, 'learning_rate_tested': BASELINE_LEARNING_RATE, 'history': trial_result }\n",
        "    except Exception as e: print(f\"    ERROR: {e}\"); num_layers_study_results[config_key] = {'error': str(e)}\n",
        "print(\"\\n\" + \"=\"*60 + \"\\nPhase 3 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================================================================\n",
        "# PHASE 4: SGD Optimization Parameter (Using Dictionary Input, with Metrics)\n",
        "# ================================================================\n",
        "\n",
        "sgd_optim_values_to_test = [None, {\"Type\": \"Momentum\", \"Parameter\": 0.5}, {\"Type\": \"Momentum\", \"Parameter\": 0.0}]\n",
        "for i, sgd_optim_value in enumerate(sgd_optim_values_to_test):\n",
        "    if sgd_optim_value is None:\n",
        "        config_key = \"sgd_optim_None\"\n",
        "    elif isinstance(sgd_optim_value, dict):\n",
        "        optim_type = sgd_optim_value.get(\"Type\", \"Unknown\")\n",
        "        optim_param = sgd_optim_value.get(\"Parameter\", \"Unknown\")\n",
        "        config_key = f\"sgd_optim_{optim_type}_{optim_param}\"\n",
        "    else:\n",
        "        config_key = f\"sgd_optim_{str(sgd_optim_value)}\"\n",
        "\n",
        "    print(f\"\\n--- Phase 4 / Trial {i+1}: Testing SGD_optim = {sgd_optim_value} ---\")\n",
        "    try:\n",
        "        nn = MLP(BASELINE_LAYER_NEURONS, BASELINE_ACTIVATION_FUNCS,\n",
        "                 weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
        "        t0 = time.time()\n",
        "        trial_result = nn.fit(train_df.X, train_df.y,\n",
        "                            learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS,\n",
        "                            SGD_optim=sgd_optim_value, # Passing None or Dict\n",
        "                            batch_size=BASELINE_BATCH_SIZE)\n",
        "        t1 = time.time()\n",
        "        time_taken = round(t1 - t0, 4)\n",
        "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
        "        active_dropout_prob = DROPOUT_PROB\n",
        "        sgd_optim_study_results[config_key] = {\n",
        "            'time_taken': time_taken,\n",
        "            'test_accuracy': test_accuracy,\n",
        "            'F1_avg': F1_avg,\n",
        "            'final_train_loss': CELoss,\n",
        "            'layer_config': BASELINE_LAYER_NEURONS,\n",
        "            'activation_config': BASELINE_ACTIVATION_FUNCS,\n",
        "            'sgd_optim_tested': sgd_optim_value,\n",
        "            'weight_decay_tested': BASELINE_WEIGHT_DECAY,\n",
        "            'batch_size_tested': BASELINE_BATCH_SIZE,\n",
        "            'dropout_prob_tested': active_dropout_prob,\n",
        "            'learning_rate_tested': BASELINE_LEARNING_RATE,\n",
        "            'history': trial_result\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ERROR: {e}\")\n",
        "        sgd_optim_study_results[config_key] = {'error': str(e)}\n",
        "print(\"\\n\" + \"=\"*60 + \"\\nPhase 4 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================================================================\n",
        "# PHASE 5: Weight Decay Parameter\n",
        "# ================================================================\n",
        "\n",
        "weight_decay_values_to_test = [0.98, 0.9, 1]\n",
        "for i, weight_decay_value in enumerate(weight_decay_values_to_test):\n",
        "    weight_decay_key_str = 'None' if weight_decay_value is None else str(weight_decay_value)\n",
        "    config_key = f\"weight_decay_{weight_decay_key_str}\"\n",
        "    print(f\"\\n--- Phase 5 / Trial {i+1}: Testing Weight Decay = {weight_decay_value} ---\")\n",
        "    try:\n",
        "        nn = MLP(BASELINE_LAYER_NEURONS, BASELINE_ACTIVATION_FUNCS, weight_decay=weight_decay_value, initialisation=BASELINE_INITIALISATION)\n",
        "        t0 = time.time()\n",
        "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=BASELINE_BATCH_SIZE)\n",
        "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
        "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
        "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
        "        active_dropout_prob = DROPOUT_PROB\n",
        "        weight_decay_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': BASELINE_LAYER_NEURONS, 'activation_config': BASELINE_ACTIVATION_FUNCS, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': weight_decay_value, 'batch_size_tested': BASELINE_BATCH_SIZE, 'dropout_prob_tested': active_dropout_prob, 'learning_rate_tested': BASELINE_LEARNING_RATE, 'history': trial_result }\n",
        "    except Exception as e: print(f\"    ERROR: {e}\"); weight_decay_study_results[config_key] = {'error': str(e)}\n",
        "print(\"\\n\" + \"=\"*60 + \"\\nPhase 5 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================================================================\n",
        "# PHASE 6: Batch Size\n",
        "# ================================================================\n",
        "batch_size_values_to_test = [100, 50, 1000]\n",
        "for i, batch_size_value in enumerate(batch_size_values_to_test):\n",
        "    config_key = f\"batch_size_{batch_size_value}\"\n",
        "    print(f\"\\n--- Phase 6 / Trial {i+1}: Testing Batch Size = {batch_size_value} ---\")\n",
        "    try:\n",
        "        nn = MLP(BASELINE_LAYER_NEURONS, BASELINE_ACTIVATION_FUNCS, weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
        "        t0 = time.time()\n",
        "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=batch_size_value)\n",
        "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
        "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
        "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
        "        active_dropout_prob = DROPOUT_PROB\n",
        "        batch_size_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': BASELINE_LAYER_NEURONS, 'activation_config': BASELINE_ACTIVATION_FUNCS, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': BASELINE_WEIGHT_DECAY, 'batch_size_tested': batch_size_value, 'dropout_prob_tested': active_dropout_prob, 'learning_rate_tested': BASELINE_LEARNING_RATE, 'history': trial_result }\n",
        "    except Exception as e: print(f\"    ERROR: {e}\"); batch_size_study_results[config_key] = {'error': str(e)}\n",
        "print(\"\\n\" + \"=\"*60 + \"\\nPhase 6 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================================================================\n",
        "# PHASE 7: Dropout Probability (using global variable)\n",
        "# ================================================================\n",
        "\n",
        "dropout_prob_values_to_test = [0.5, 0.1, 0.9]\n",
        "for i, dropout_prob_value in enumerate(dropout_prob_values_to_test):\n",
        "    config_key = f\"dropout_prob_{dropout_prob_value}\"\n",
        "    print(f\"\\n--- Phase 7 / Trial {i+1}: Setting GLOBAL DROPOUT_PROB = {dropout_prob_value} ---\")\n",
        "    DROPOUT_PROB = dropout_prob_value # Set global variable\n",
        "    try:\n",
        "        nn = MLP(BASELINE_LAYER_NEURONS, BASELINE_ACTIVATION_FUNCS, weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
        "        t0 = time.time()\n",
        "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=BASELINE_BATCH_SIZE)\n",
        "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
        "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
        "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
        "        dropout_prob_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': BASELINE_LAYER_NEURONS, 'activation_config': BASELINE_ACTIVATION_FUNCS, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': BASELINE_WEIGHT_DECAY, 'batch_size_tested': BASELINE_BATCH_SIZE, 'dropout_prob_tested': dropout_prob_value, 'learning_rate_tested': BASELINE_LEARNING_RATE, 'history': trial_result }\n",
        "    except Exception as e: print(f\"    ERROR: {e}\"); dropout_prob_study_results[config_key] = {'error': str(e)}\n",
        "\n",
        "DROPOUT_PROB = BASELINE_DROPOUT_PROB\n",
        "print(f\"\\nGlobal DROPOUT_PROB reset to baseline: {DROPOUT_PROB} after Phase 7.\")\n",
        "print(\"\\n\" + \"=\"*60 + \"\\nPhase 7 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================================================================\n",
        "# PHASE 8: Learning Rate\n",
        "# ================================================================\n",
        "learning_rate_values_to_test = [0.005, 0.001, 0.01]\n",
        "for i, learning_rate_value in enumerate(learning_rate_values_to_test):\n",
        "    config_key = f\"learning_rate_{learning_rate_value}\"\n",
        "    print(f\"\\n--- Phase 8 / Trial {i+1}: Testing Learning Rate = {learning_rate_value} ---\")\n",
        "    try:\n",
        "        nn = MLP(BASELINE_LAYER_NEURONS, BASELINE_ACTIVATION_FUNCS, weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
        "        t0 = time.time()\n",
        "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=learning_rate_value, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=BASELINE_BATCH_SIZE)\n",
        "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
        "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
        "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
        "        active_dropout_prob = DROPOUT_PROB # Capture baseline dropout used\n",
        "        learning_rate_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': BASELINE_LAYER_NEURONS, 'activation_config': BASELINE_ACTIVATION_FUNCS, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': BASELINE_WEIGHT_DECAY, 'batch_size_tested': BASELINE_BATCH_SIZE, 'dropout_prob_tested': active_dropout_prob, 'learning_rate_tested': learning_rate_value, 'history': trial_result }\n",
        "    except Exception as e: print(f\"    ERROR: {e}\"); learning_rate_study_results[config_key] = {'error': str(e)}\n",
        "print(\"\\n\" + \"=\"*60 + \"\\nPhase 8 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# ================================================================\n",
        "# WRITE ALL RESULTS TO FILE\n",
        "# ================================================================\n",
        "print(\"All Independent Ablation Studies Finished. Writing results to file...\")\n",
        "\n",
        "# Get current timestamp\n",
        "now = datetime.datetime.now()\n",
        "timestamp_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "results_filename = f\"results_{timestamp_str}.txt\"\n",
        "\n",
        "all_results = [\n",
        "    (\"Phase 1: Neurons Per Layer\", neuron_study_results),\n",
        "    (\"Phase 2: Activation Function\", activation_study_results),\n",
        "    (\"Phase 3: Number of Hidden Layers\", num_layers_study_results),\n",
        "    (\"Phase 4: SGD Optimization\", sgd_optim_study_results),\n",
        "    (\"Phase 5: Weight Decay\", weight_decay_study_results),\n",
        "    (\"Phase 6: Batch Size\", batch_size_study_results),\n",
        "    (\"Phase 7: Dropout Probability\", dropout_prob_study_results),\n",
        "    (\"Phase 8: Learning Rate\", learning_rate_study_results),\n",
        "]\n",
        "\n",
        "try:\n",
        "    with open(results_filename, 'w') as f:\n",
        "        f.write(f\"Ablation Study Results - Generated on: {now.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "        # Write Baseline Configuration\n",
        "        f.write(\"Baseline Configuration:\\n\")\n",
        "        f.write(f\"  Num Hidden Layers: {BASELINE_NUM_HIDDEN_LAYERS}\\n\")\n",
        "        f.write(f\"  Neurons Per Layer: {BASELINE_NEURONS_PER_LAYER}\\n\")\n",
        "        f.write(f\"  Hidden Activation: {BASELINE_HIDDEN_ACTIVATION}\\n\")\n",
        "        f.write(f\"  SGD Optim: {BASELINE_SGD_OPTIM}\\n\")\n",
        "        f.write(f\"  Weight Decay: {BASELINE_WEIGHT_DECAY}\\n\")\n",
        "        f.write(f\"  Initialisation: {BASELINE_INITIALISATION}\\n\")\n",
        "        f.write(f\"  Batch Size: {BASELINE_BATCH_SIZE}\\n\")\n",
        "        f.write(f\"  Dropout Prob: {BASELINE_DROPOUT_PROB}\\n\")\n",
        "        f.write(f\"  Learning Rate: {BASELINE_LEARNING_RATE}\\n\")\n",
        "        f.write(f\"  Epochs: {EPOCHS}\\n\")\n",
        "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "\n",
        "        for phase_name, results_dict in all_results:\n",
        "            f.write(f\"--- {phase_name} ---\\n\\n\")\n",
        "            if not results_dict:\n",
        "                f.write(\"  No results recorded for this phase.\\n\\n\")\n",
        "                continue\n",
        "\n",
        "            # Sort results by config key for consistent order (optional)\n",
        "            sorted_items = sorted(results_dict.items())\n",
        "\n",
        "            for config_key, data in sorted_items:\n",
        "                f.write(f\"Config Key: {config_key}\\n\")\n",
        "                if 'error' in data:\n",
        "                    f.write(f\"  Status: ERROR\\n\")\n",
        "                    f.write(f\"  Error Msg: {data['error']}\\n\")\n",
        "                else:\n",
        "                    # Safely get metrics with default -1 if missing\n",
        "                    time_val = data.get('time_taken', -1)\n",
        "                    acc = data.get('test_accuracy', -1)\n",
        "                    f1 = data.get('F1_avg', -1)\n",
        "                    loss = data.get('final_train_loss', -1)\n",
        "\n",
        "                    f.write(f\"  Status: Success\\n\")\n",
        "                    f.write(f\"  Time Taken (s): {time_val:.2f}\\n\")\n",
        "                    f.write(f\"  Test Accuracy: {acc:.4f}\\n\")\n",
        "                    f.write(f\"  Average F1 Score: {f1:.4f}\\n\")\n",
        "                    f.write(f\"  Final Train Loss: {loss:.4f}\\n\")\n",
        "\n",
        "                f.write(\"-\" * 40 + \"\\n\") # Separator between trials\n",
        "            f.write(\"\\n\") # Space after phase section\n",
        "\n",
        "    print(f\"Results successfully written to {results_filename}\")\n",
        "\n",
        "except IOError as e:\n",
        "    print(f\"ERROR: Could not write results to file {results_filename}. Error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during file writing: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "NedsEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}