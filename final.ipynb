{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18cd80db",
   "metadata": {},
   "source": [
    "## COMP4329 Assignment 1\n",
    "\n",
    "- Ned O'Rourke SID: 510428929\n",
    "- Will Polich SID: 510428929\n",
    "- Angus McBean SID: 510428929"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea38b7",
   "metadata": {},
   "source": [
    "### Import Libraries \n",
    "\n",
    "Note: Pandas were not used for module implementation, only for results dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e03a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9508c1aa",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c45f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = np.load('Assignment1-Dataset/train_data.npy')\n",
    "train_label = np.load('Assignment1-Dataset/train_label.npy')\n",
    "test_data = np.load('Assignment1-Dataset/test_data.npy')\n",
    "test_label = np.load('Assignment1-Dataset/test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20edb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b26734",
   "metadata": {},
   "source": [
    "### Activation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b53d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \n",
    "    def __logistic(self, x):\n",
    "        return 1.0 /(1.0 + np.exp(-x))\n",
    "    \n",
    "    def __logistic_derivative(self, a):\n",
    "        #where a = logistic(x)\n",
    "        return a * (1-a)\n",
    "    \n",
    "    def __relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def __relu_derivative(self, a):\n",
    "        return np.heaviside(a, 0)\n",
    "\n",
    "    def __leakyrelu(self, x, alpha=0.01):\n",
    "        return np.where(x >= 0, x, alpha * x)\n",
    "    \n",
    "    def __leakyrelu_derivative(self, x, alpha=0.01):\n",
    "        return np.heaviside(x, 1) * (1 - alpha) + alpha\n",
    "\n",
    "    def _gelu(self, x):\n",
    "        return 0.5 * x * (1.0 + np.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "\n",
    "    def _gelu_derivative(self, x):\n",
    "        # Compute the inner term for tanh\n",
    "        k = np.sqrt(2.0 / np.pi) * (x + 0.044715 * np.power(x, 3))\n",
    "        tanh_k = np.tanh(k)\n",
    "        \n",
    "        # First term of the derivative\n",
    "        term1 = 0.5 * (1.0 + tanh_k)\n",
    "        \n",
    "        # Second term with sech^2(k) = 1 - tanh^2(k)\n",
    "        term2 = 0.5 * x * (1 - np.power(tanh_k, 2)) * np.sqrt(2.0 / np.pi) * (1 + 3 * 0.044715 * np.power(x, 2))\n",
    "        \n",
    "        return term1 + term2\n",
    "    \n",
    "    def __softmax(self, z):\n",
    "        z = np.atleast_2d(z)\n",
    "        max_z = np.max(z, axis=1, keepdims=True)\n",
    "        z = z - max_z\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    \n",
    "    def __softmax_derivative(self, z, z_hat):\n",
    "        return z_hat - z\n",
    "    \n",
    "    def __init__(self, activation_function = 'relu'):\n",
    "        if activation_function == \"logistic\":\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_derivative\n",
    "        elif activation_function == 'relu': \n",
    "            self.f = self.__relu\n",
    "            self.f_derivative = self.__relu_derivative\n",
    "        elif activation_function == 'leakyrelu':\n",
    "            self.f = self.__leakyrelu\n",
    "            self.f_derivative = self.__leakyrelu_derivative\n",
    "        elif activation_function == \"softmax\":\n",
    "            self.f = self.__softmax\n",
    "            self.f_derivative = self.__softmax_derivative\n",
    "        elif activation_function == \"gelu\":\n",
    "            self.f = self._gelu\n",
    "            self.f_derivative = self._gelu_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea74829",
   "metadata": {},
   "source": [
    "### Hidden Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a040eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, \n",
    "                 n_in, \n",
    "                 n_out, \n",
    "                 activation_last_layer = 'relu',\n",
    "                 activation = 'relu',\n",
    "                 initialisation = 'xavier',\n",
    "                 W = None,\n",
    "                 b = None,\n",
    "                 v_W = None,\n",
    "                 v_b = None,\n",
    "                 last_hidden_layer = False,\n",
    "                 use_batchnorm=False):\n",
    "    \n",
    "        '''\n",
    "        The class for a Hidden Layer in a MLP. \n",
    "\n",
    "        Attributes:\n",
    "        n_in (int): The dimensionality of the input to the Hidden Layer.\n",
    "        n_out (int): The dimensionality of the output, i.e. the number of hidden units.\n",
    "\n",
    "        activation_last_layer (str): The activation function of the previous Hidden Layer.\n",
    "        activation (str): The activation function of this current Hidden Layer\n",
    "\n",
    "        W (numpy array): The weight(s) applied to this current Hidden Layer. Set to None by default to allow initialisation later.\n",
    "        b (numpy array): The bias applied to this current Hidden Layer. Set to None by default to allow initialisation later.\n",
    "\n",
    "        v_W (numpy array): The 'velocity' or 'trajectory' term vt for the weight(s) in Momentum SGD. Set to None by default to allow initialisation later.\n",
    "        v_b (numpy array): The 'velocity' or 'trajectory' term vt for the bias in Momentum SGD. Set to None by default to allow initialisation later.\n",
    "\n",
    "        last_hidden_layer (bool): The boolean to determine if the current Hidden Layer object is the Last Hidden Layer in the MLP.\n",
    "        '''\n",
    "\n",
    "        \n",
    "        self.last_hidden_layer = last_hidden_layer\n",
    "        self.input = None\n",
    "        self.initialisation = initialisation\n",
    "\n",
    "        #Create a Activatino object Grab the .f method from relu\n",
    "        self.activation = Activation(activation).f\n",
    "        \n",
    "        #Set activation deriv of last layer, none if no last layer\n",
    "        self.activation_deriv = None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv = Activation(activation_last_layer).f_derivative\n",
    "\n",
    "        if self.initialisation == 'xavier':\n",
    "            #Xavier Initialisation - assign random small values (from uniform dist)\n",
    "            self.W = np.random.uniform(low = -np.sqrt(6. / (n_in + n_out)),\n",
    "                                    high = np.sqrt(6. / (n_in + n_out )),\n",
    "                                    size = (n_in, n_out))\n",
    "        elif self.initialisation == 'zeros':\n",
    "            self.W = np.zeros((n_in, n_out))\n",
    "        elif self.initialisation == 'random_small':\n",
    "            self.W = np.random.randn(n_in, n_out) * 0.01\n",
    "        \n",
    "        #set size of the bias as the size of the output dimension (all zero)\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        # we set he size of weight gradients as the size of weight\n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "\n",
    "        #Create array of zeros with the same shape as the gradient weights\n",
    "        self.v_W = np.zeros_like(self.grad_W)\n",
    "        self.v_b = np.zeros_like(self.grad_b)\n",
    "        self.binomial_array=np.zeros(n_out)\n",
    "\n",
    "        #setting up batch normalisation\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.eps = 1e-8\n",
    "        self.gamma = np.ones(n_out)  # Scale\n",
    "        self.beta = np.zeros(n_out)  # Shift\n",
    "        self.running_mean = np.zeros(n_out)\n",
    "        self.running_var = np.ones(n_out)\n",
    "\n",
    "        self.grad_gamma = np.zeros(n_out)\n",
    "        self.grad_beta = np.zeros(n_out)\n",
    "\n",
    "        self.v_gamma = np.zeros_like(self.gamma)\n",
    "        self.v_beta = np.zeros_like(self.beta)\n",
    "\n",
    "        self.num_batches_tracked = 0\n",
    "        self.sum_batch_means = np.zeros(n_out)\n",
    "        self.sum_batch_vars  = np.zeros(n_out)\n",
    "        self.fixed_batch_size = None  # set after first forward pass\n",
    "\n",
    "    @staticmethod\n",
    "    def dropout_forward(X, p_dropout):\n",
    "        '''\n",
    "        The method to perform dropout during the training of the forward pass.\n",
    "\n",
    "        Paremeters:\n",
    "        X (numpy array): The input data to be fed through the dropout forward pass.\n",
    "        p_dropout (float): The controlling factor of the proportion of neurons dropped in the network.\n",
    "\n",
    "        Returns:\n",
    "        out (numpy array): The resulting output array with values from inactive neurons as 0 and values from the active neuron equal to that of the input. \n",
    "        binomial_array (numpy array): An array with the same size of input, filled with 0s for neurons that are to be inactive and 1s for neurons that are to be active during training.\n",
    "        '''\n",
    "      \n",
    "        u = np.random.binomial(1, 1 - p_dropout, size=X.shape) \n",
    "        out = X * u\n",
    "        binomial_array=u\n",
    "        return out, binomial_array\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout_backward(delta, binomial_array, layer_num):\n",
    "        '''\n",
    "        The method to perform dropout during the backpropagation.\n",
    "\n",
    "        Parameters:\n",
    "        delta (numpy array): The delta generated for the backpropagation process.\n",
    "        binomial_array (numpy array): An array with the same size of input, filled with 0s for neurons that are to be inactive and 1s for neurons that are to be active during training.\n",
    "        layer_num (int): The current layer in the MLP which dropout is being performed on.\n",
    "\n",
    "        Returns:\n",
    "        delta (numpy array): The adjusted delta with dropout applied.\n",
    "        '''\n",
    "\n",
    "        delta *= nn.layers[layer_num - 1].binomial_array\n",
    "        return delta\n",
    "    \n",
    "    def finalise_batchnorm_stats(self):\n",
    "        if self.num_batches_tracked == 0:\n",
    "            return  # No training batches processed\n",
    "\n",
    "        # Compute running mean as the average of all batch means\n",
    "        self.running_mean = self.sum_batch_means / self.num_batches_tracked\n",
    "\n",
    "        # get an unbiased population variance\n",
    "        m = self.fixed_batch_size\n",
    "        self.running_var = (m / (m - 1)) * (self.sum_batch_vars / self.num_batches_tracked)\n",
    "    \n",
    "    #forward progress for training epoch:\n",
    "    def forward(self, input, training=True):\n",
    "        '''\n",
    "        The feedforward pass of a single Hidden Layer.\n",
    "        Applies the weights and bias to the input, performs calculations via the selected activation function and returns this output.\n",
    "\n",
    "        Parameters:\n",
    "        input (numpy array): The input data, either from the output of the previous Hidden Layer or the initial input data. \n",
    "\n",
    "        Returns:\n",
    "        self.output (numpy array): The resulting output.\n",
    "        '''\n",
    "        #Set current input for this layer\n",
    "        self.input = input\n",
    "\n",
    "        #this is the whole layer\n",
    "        lin_output = np.dot(input, self.W) + self.b #simple perceptron output\n",
    "\n",
    "        #Setting unnormalised linear output for this layer\n",
    "        self.unnorm_lin_out = lin_output \n",
    "\n",
    "        #batch normalisation (before activation function)\n",
    "        if self.use_batchnorm and not self.last_hidden_layer:\n",
    "            if training:\n",
    "                #Compute batch statistics\n",
    "                batch_mean = np.mean(lin_output, axis=0)\n",
    "                batch_var = np.var(lin_output, axis=0)\n",
    "\n",
    "                #Store statistics\n",
    "                self.batch_mean = batch_mean\n",
    "                self.batch_var = batch_var\n",
    "\n",
    "                # Normalize\n",
    "                lin_output_norm = (lin_output - batch_mean) / np.sqrt(batch_var + self.eps)\n",
    "                \n",
    "                #Scale and shift\n",
    "                lin_output = self.gamma * lin_output_norm + self.beta\n",
    "\n",
    "                #Store\n",
    "                self.bn_output = lin_output\n",
    "\n",
    "                # accumulate for inference stats\n",
    "                self.num_batches_tracked += 1\n",
    "                self.sum_batch_means += batch_mean\n",
    "                self.sum_batch_vars  += batch_var\n",
    "\n",
    "                if self.fixed_batch_size is None:\n",
    "                    self.fixed_batch_size = lin_output.shape[0]\n",
    "                \n",
    "            else:\n",
    "                # Inference mode: use running statistics\n",
    "                # use precomputed inference stats\n",
    "                mean = self.running_mean\n",
    "                var  = self.running_var\n",
    "                lin_output_norm = (lin_output - mean) / np.sqrt(var + self.eps)\n",
    "                lin_output = self.gamma * lin_output_norm + self.beta\n",
    "                self.bn_output = lin_output\n",
    "\n",
    "        #feed linear output into activation function\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None #linear if no activation specified\n",
    "            else self.activation(lin_output) #activation fn on w*I + b  (i.e. activation function on linear output)\n",
    "        ) \n",
    "\n",
    "        #\n",
    "        if not self.last_hidden_layer:\n",
    "            self.output, self.binomial_array = self.dropout_forward(self.output, DROPOUT_PROB)\n",
    "\n",
    "        #return the output\n",
    "        return self.output\n",
    "\n",
    "    #backpropagation\n",
    "    def backward(self, delta, layer_num, output_layer = False):\n",
    "        '''\n",
    "        The backward pass of a single Hidden Layer.\n",
    "\n",
    "        Parameters:\n",
    "        delta (numpy array): The delta values to be applied to the activation derivative.\n",
    "        layer_num (int): The number of the current layer in the MLP, used to check if it is not the input layer.\n",
    "        output_layer (bool): A boolean to reflect if the current layer is not the output layer. \n",
    "\n",
    "        Returns delta (numpy array): The delta for the hidden layer to be used in parameters.\n",
    "        '''\n",
    "\n",
    "        #If using Batch norm and this is not the output layer\n",
    "        if self.use_batchnorm and not self.last_hidden_layer:\n",
    "            #Gradient w.r.t gamma and beta\n",
    "            x_norm = (self.unnorm_lin_out - self.batch_mean) / np.sqrt(self.batch_var + self.eps)\n",
    "            self.grad_gamma = np.sum(delta * x_norm, axis=0)\n",
    "            self.grad_beta = np.sum(delta, axis=0)\n",
    "\n",
    "            #Backprop through BN normalisation\n",
    "            N, D = delta.shape\n",
    "\n",
    "            #Backprop through scale and shift\n",
    "            d_bn = delta * self.gamma\n",
    "\n",
    "            #Gradients for normalised input (from BN)\n",
    "            x_mu = self.unnorm_lin_out - self.batch_mean\n",
    "            std_inv = 1. / np.sqrt(self.batch_var + self.eps)\n",
    "\n",
    "            d_var = np.sum(d_bn * x_mu * -0.5 * std_inv**3, axis=0)\n",
    "            d_mean = np.sum(d_bn * -std_inv, axis=0) + d_var * np.mean(-2. * x_mu, axis=0)\n",
    "\n",
    "            delta = d_bn * std_inv + d_var * 2 * x_mu / N + d_mean / N\n",
    "\n",
    "        #Completely different formulas as this is a vectorised implementation \n",
    "\n",
    "        #calcualtes gradients as input(^t) * delta\n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "\n",
    "        # The gradient of the bias vector b becomes the average of the delta values across the batch\n",
    "        self.grad_b = np.average(delta, axis=0)\n",
    "\n",
    "        # \n",
    "        if self.activation_deriv:\n",
    "            #Propogates the error backward through the weights * applies the derivative of the activation function to get the true local gradient\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "\n",
    "        #If not in the onput layer\n",
    "        if layer_num != 0:\n",
    "            #Restores dropped-out connections if dropout was applied in the forward pass\n",
    "            delta=self.dropout_backward(delta, self.binomial_array, layer_num)\n",
    "        \n",
    "        #pass delta to the next (previous) layer in the backward chain to repeat\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28975cf",
   "metadata": {},
   "source": [
    "### Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    '''\n",
    "    Main class holding the structure of the Multi-Layer Perceptron. \n",
    "    \n",
    "    Attributes:\n",
    "    None\n",
    "    '''\n",
    "\n",
    "    def __init__(self, layers, activation = [None, 'relu', 'relu','relu', 'softmax'], weight_decay = 1.0, initialisation='xaiver', batch_normal=False):\n",
    "\n",
    "        '''\n",
    "        The initialisation of the MLP.\n",
    "\n",
    "        Attributes:\n",
    "        layers (list of int): A list containing the number of neurons in each respective layer.\n",
    "        activation (list of str): A list containing the activation functions to be used in each respective layer. Set to [None, 'relu', 'relu', 'relu', 'softmax'] as default.\n",
    "        weight_decay (float): The value set for the weight decay to be applied. Value of 1.0 indicates no weight decay to be applied.\n",
    "        '''\n",
    "        \n",
    "        self.batch_normal = batch_normal\n",
    "        #Will contain all the hidden layer objects \n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        \n",
    "        #Activation functions for each layer\n",
    "        self.activation = activation\n",
    "\n",
    "        #Weight decay coefficient \n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        #Loop through the list of layers. initialise a new hidden layer object for each layer\n",
    "        for i in range(len(layers)-1):\n",
    "\n",
    "            last_hidden_layer = False\n",
    "\n",
    "            if i == len(layers) - 2: # -2 because -1 for output layer, and another -1 since it's index 0\n",
    "                last_hidden_layer = True\n",
    "\n",
    "            self.layers.append(HiddenLayer(layers[i], \n",
    "                                           layers[i+1], \n",
    "                                           activation[i], \n",
    "                                           activation[i+1],\n",
    "                                           initialisation=initialisation,\n",
    "                                           last_hidden_layer=last_hidden_layer,\n",
    "                                           use_batchnorm=self.batch_normal))\n",
    "            \n",
    "    def forward(self, input, training=True):\n",
    "        '''\n",
    "        The feedforward process conducted sequentially through each layer in the MLP.\n",
    "        Takes the input from the previous layer (or initial data if it is the input layer), applies weights & bias then activation function and feeds the resulting output as the input to the next layer via the HiddenLayer.forward() method.\n",
    "\n",
    "        Parameters:\n",
    "        input (numpy array): The input array to be fed through the feedforward process.\n",
    "\n",
    "        Returns:\n",
    "        output (numpy array): The resulting final output from the feedforward process across all layers.\n",
    "        '''\n",
    "\n",
    "        #Perform forward propogation on each layer object \n",
    "        for layer in self.layers: \n",
    "            output = layer.forward(input, training=training) \n",
    "           \n",
    "            input = output \n",
    "        return output\n",
    "\n",
    "    \n",
    "    def CE_loss(self, y, y_hat):\n",
    "        '''\n",
    "        The calculation of the Cross-Entropy loss function.\n",
    "        Computes the cross entropy loss, averages this and applies weight decay (if applicable) as well as calculating the respective delta to be used in the backpropagation process.\n",
    "\n",
    "        Parameters:\n",
    "        y (numpy array): The actual y values (or labels) from the data set.\n",
    "        y_hat (numpy array): The calculated y values (y hat) as output from the feedforward process.\n",
    "\n",
    "        Returns:\n",
    "        loss (float): The calculated Cross Entropy Loss value.\n",
    "        delta (numpy array): The calculated delta array to be used in the backpropagation process. \n",
    "        '''\n",
    "        epsilon = 1e-12\n",
    "        y_hat = np.clip(y_hat, epsilon, 1. - epsilon)  # avoid log(0)\n",
    "        loss = - np.nansum(y * np.log(y_hat)) / y.shape[0]\n",
    "        loss *= self.weight_decay\n",
    "        delta = Activation(self.activation[-1]).f_derivative(y, y_hat)\n",
    "        return loss, delta\n",
    "\n",
    "    def backward(self, delta):\n",
    "        '''\n",
    "        The backpropagation process conducted backwards across each layer in the MLP.\n",
    "        Updates the delta via the Hidden Layer backward process and applies this updated delta as the delta input in the HiddenLayer.backward() method. \n",
    "        \n",
    "        Parameters:\n",
    "        delta (numpy array): The value for delta calculated in the Loss function.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        '''\n",
    "\n",
    "        delta = self.layers[-1].backward(delta, len(self.layers) -1, output_layer = True)\n",
    "        for layer_num, layer in reversed(list(enumerate(self.layers[:-1]))):\n",
    "            delta = layer.backward(delta, layer_num)\n",
    "                  \n",
    "    def update(self, lr, SGD_optim):\n",
    "      '''\n",
    "      The method to update the parameters under Stochastic Gradient Descent (SGD).\n",
    "      Updates the weights and bias parameters based on the learning rate and respective gradient. Includes functionality for applying SGD Momentum optimization.\n",
    "         \n",
    "      Parameters:\n",
    "      lr (float): The learning rate for the parameter updates.\n",
    "      SGD_optim (dict of str: str): The SGD Optimization values as a dictionary with keys 'Type': as the type of optimisation and 'Parameters': for the optimization parameter value.\n",
    "\n",
    "      Returns:\n",
    "      None\n",
    "      '''\n",
    "\n",
    "      #Update without momentum\n",
    "      if SGD_optim is None:\n",
    "          for layer in self.layers:\n",
    "            #Update weight and bias parameters\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "\n",
    "            if layer.use_batchnorm:\n",
    "                #Update gamma and beta params for batch normalisation\n",
    "                layer.gamma -= lr * layer.grad_gamma\n",
    "                layer.beta  -= lr * layer.grad_beta\n",
    "\n",
    "      #Update with momentum\n",
    "      elif SGD_optim['Type'] == 'Momentum':\n",
    "          for layer in self.layers:\n",
    "              layer.v_W = (SGD_optim['Parameter'] * layer.v_W) + (lr * layer.grad_W)\n",
    "              layer.v_b = (SGD_optim['Parameter'] * layer.v_b) + (lr * layer.grad_b)\n",
    "              layer.W = layer.W - layer.v_W\n",
    "              layer.b = layer.b - layer.v_b\n",
    "              \n",
    "              if layer.use_batchnorm:\n",
    "                #For BN parameters\n",
    "                layer.v_gamma= (SGD_optim['Parameter'] * layer.v_gamma) + (lr * layer.grad_gamma)\n",
    "                layer.v_beta = (SGD_optim['Parameter'] * layer.v_beta) + (lr * layer.grad_beta)\n",
    "\n",
    "                layer.gamma -= layer.v_gamma\n",
    "                layer.beta -= layer.v_beta\n",
    "              \n",
    "    def fit(self, X, y, learning_rate = 0.1, epochs = 100, SGD_optim = None, batch_size = 1):\n",
    "        '''\n",
    "        The method to fit the MLP.\n",
    "        Iterates through epochs, runs the forward process to calculate respective loss (and delta) then runs the backpropagation process to update the parameters.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy array): The input X values.\n",
    "        y (numpy array): The corresponding y values (or labels).\n",
    "        learning_rate (float): The learning rate to be used in the parameter updates. Set to 0.1 by default.\n",
    "        epochs (int): The number of times the dataset is passed through the MLP. Set to 100 by default.\n",
    "        SGD_optim (dict of str: str): A dictionary containing the type of optimization and the respective optimization algorithm parameter to be used. Set as None by default.\n",
    "        batch_size (int): The size of the batches to be used in Mini-Batch learning.\n",
    "\n",
    "        Returns:\n",
    "        output_dct (dict of float): A dictionary containing the training cross-entropy loss, training accuracy and testing accuracy.\n",
    "        '''\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        training_loss = []\n",
    "        training_accuracy = []\n",
    "        testing_accuracy = []\n",
    "\n",
    "        #Split the data into batches \n",
    "        num_batches = int(np.ceil(X.shape[0] / batch_size))\n",
    "            \n",
    "        #Perform epochs on batches \n",
    "        for k in range(epochs):\n",
    "\n",
    "            loss = np.zeros(num_batches) \n",
    "\n",
    "            current_idx = 0 \n",
    "\n",
    "            #Shuffle the data, to ensure that each epoch will have different sequence of observations\n",
    "            X, y = Utils.shuffle(X, y)\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                \n",
    "                #forward pass \n",
    "                y_hat = self.forward(X[current_idx : current_idx + batch_size, :], training=True)\n",
    "\n",
    "                #backward pass\n",
    "                loss[batch_idx], delta = self.CE_loss(y[current_idx : current_idx + batch_size], y_hat)\n",
    "\n",
    "                self.backward(delta)\n",
    "\n",
    "                #update\n",
    "                self.update(learning_rate, SGD_optim)\n",
    "\n",
    "                #Update the index based on the batch window for the next round of Mini-Batch learning.\n",
    "                if (current_idx + batch_size) > X.shape[0]:\n",
    "                    batch_size = X.shape[0] - current_idx\n",
    "                current_idx += batch_size\n",
    "            \n",
    "            # Finalize batchnorm running statistics for inference\n",
    "            for layer in self.layers:\n",
    "                if hasattr(layer, 'finalize_batchnorm_stats'):\n",
    "                    layer.finalize_batchnorm_stats()\n",
    "\n",
    "            #Predict and compute metrics for each run\n",
    "            test_predict = self.predict(test_df.X)\n",
    "            train_predict = self.predict(train_df.X)\n",
    "            test_predict = test_df.decode(test_predict)\n",
    "            train_predict = train_df.decode(train_predict)\n",
    "            test_accuracy = np.sum(test_predict == test_label[:, 0]) / test_predict.shape[0]\n",
    "            train_accuracy = np.sum(train_predict == train_label[:, 0]) / train_predict.shape[0]\n",
    "\n",
    "            training_loss.append(np.mean(loss))\n",
    "            training_accuracy.append(train_accuracy)\n",
    "            testing_accuracy.append(test_accuracy)\n",
    "\n",
    "            output_dict = {'Training Loss': training_loss, 'Training Accuracy': training_accuracy, 'Testing Accuracy': testing_accuracy}\n",
    "\n",
    "            print(f'Epoch {k+1}/{epochs} has been trained with Train Loss: {str(round(training_loss[-1], 4))}, Training Accuracy: {str(round(training_accuracy[-1] * 100, 4))}% and Testing Accuracy: {str(round(testing_accuracy[-1] * 100, 4))}%.')\n",
    "         \n",
    "        return output_dict\n",
    "            \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        The method to predict values based on input x by running forward process through the fitted MLP.\n",
    "\n",
    "        Parameters:\n",
    "        x (numpy array): The input x values on which to compute predictions.\n",
    "\n",
    "        Returns:\n",
    "        output (numpy array): The resulting predictions.\n",
    "        '''\n",
    "\n",
    "        x = np.array(x)\n",
    "        output = [i for i in range(x.shape[0])]\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = self.forward(x[i, :], training=False)\n",
    "        output = np.array(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042d7f8b",
   "metadata": {},
   "source": [
    "### Data Preprocessing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2301164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    '''\n",
    "    The Class to apply preprocessing methods.\n",
    "\n",
    "    Attributes:\n",
    "    X (numpy array): The input array of X values.\n",
    "    y (numpy array): The input array of y values (or labels).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.predictions = None\n",
    "\n",
    "    def normalize(self):     \n",
    "        '''\n",
    "        Normalizes and transforms the X values based on min-max normalization.\n",
    "\n",
    "        Parameters:\n",
    "        None\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        '''\n",
    "\n",
    "        norm_data = (self.X - np.min(self.X))/(np.max(self.X) - np.min(self.X))\n",
    "        self.X = norm_data\n",
    "\n",
    "    def standardize(self):\n",
    "        '''\n",
    "        Standardizes and transforms the X values based on the mean & standard deviation.\n",
    "\n",
    "        Parameters:\n",
    "        None\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        '''\n",
    "\n",
    "        self.X = (self.X - np.mean(self.X)) / np.std(self.X)\n",
    "\n",
    "    @staticmethod\n",
    "    def label_encode(label_vector):\n",
    "        '''\n",
    "        Encodes the label (y) values based on one-hot encoding.\n",
    "        Creates an empty list for each observation, fills it with zeros then set the index of the class label to 1.\n",
    "\n",
    "        Parameters:\n",
    "        label_vector (numpy array): The label array to be one-hot encoded.\n",
    "\n",
    "        Returns:\n",
    "        encoded_label_vector (numpy array): The resulting one-hot encoded array for the labels.    \n",
    "        '''\n",
    "\n",
    "        num_classes = np.unique(label_vector).size\n",
    "        \n",
    "        encoded_label_vector = []\n",
    "        \n",
    "        for label in label_vector:\n",
    "            encoded_label = np.zeros(num_classes)\n",
    "            encoded_label[int(label)] = 1\n",
    "            encoded_label_vector.append(encoded_label)\n",
    "        \n",
    "        encoded_label_vector = np.array(encoded_label_vector) \n",
    "\n",
    "        return encoded_label_vector\n",
    "    \n",
    "    @staticmethod\n",
    "    def decode(prediction_matrix):\n",
    "        '''\n",
    "        Transforms a one-hot encoded matrix back to a class label.\n",
    "        Creates a zero array and fills it with the index of maximum value (i.e. 1) in the one-hot encoded array.\n",
    "\n",
    "        Parameters:\n",
    "        prediction_matrix (numpy array): The one-hot encoded label matrix.\n",
    "\n",
    "        Returns:\n",
    "        decoded_predictions (numpy array): A numpy array filled with the labels.\n",
    "        '''\n",
    "\n",
    "        decoded_predictions = np.zeros(prediction_matrix.shape[0])\n",
    "        for prediction_idx, prediction_vector in enumerate(prediction_matrix):\n",
    "            decoded_predictions[prediction_idx] = int(np.argmax(prediction_vector)) # we add the two index zeros because it's a nparray within a tuple\n",
    "        \n",
    "        return decoded_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc2e57d",
   "metadata": {},
   "source": [
    "### Miscellaneous methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5450437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    '''\n",
    "    Class used to contain miscellaneous methods.\n",
    "\n",
    "    Attributes:\n",
    "    None\n",
    "    '''\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle(X, y):\n",
    "        '''\n",
    "        Randomly shuffles the data.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy array): The X values to be shuffled.\n",
    "        y (numpy array): The y values to be shuffled.\n",
    "\n",
    "        Returns:\n",
    "        X (numpy array), y (numpy array): The pair of the shuffled X & y numpy arrays.\n",
    "        '''\n",
    "        shuffled_idx = np.arange(X.shape[0])\n",
    "        np.random.shuffle(shuffled_idx)\n",
    "        X = X[shuffled_idx]\n",
    "        y = y[shuffled_idx]\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_confusion_mat(df):\n",
    "      '''\n",
    "      Creates a confusion matrix based on a Preprocessing object that has X, y and predicted y values.\n",
    "      Calculates the values to be placed in respective row/columns by summing the occurences in pairwise indices for the original and predicted y values.\n",
    "\n",
    "      Parameters:\n",
    "      df (Preprocessing): A Preprocessing object with original X, original y and predicted y values.\n",
    "\n",
    "      Returns:\n",
    "      confusion_mat (pandas DataFrame): A confusion matrix represented as a pandas DataFrame, where the rows (indexes) reflect predicted values and the columns reflect actual values.\n",
    "      '''\n",
    "\n",
    "      confusion_mat = pd.DataFrame(0, index = np.unique(df.y) , columns = np.unique(df.y))\n",
    "      for i in range(0, len(df.y)):\n",
    "        confusion_mat[int(df.y[i])].iloc[int(df.predictions[i])] += 1\n",
    "      return confusion_mat\n",
    "    \n",
    "    @staticmethod\n",
    "    def confusion_mat_measures(confusion_matrix):\n",
    "      '''\n",
    "      Produces a pandas DataFrame with Precision, Recall and F1 measures per class.\n",
    "      First calculates True Positive (TP), False Negative (FN), False Positive (FP) and True Negative (TN) values then calculates Precision, Recall and F1 values and stores them in a DataFrame.\n",
    "\n",
    "      Parameters:\n",
    "      confusion_matrix (pandas DataFrame): A confusion matrix as a Pandas DataFrame Object.\n",
    "\n",
    "      Returns:\n",
    "      scores_df (pandas Dataframe): A DataFrame with labels as rows (indexes) and Precision, Recall and F1 scores as columns.\n",
    "      '''\n",
    "\n",
    "      scores_df = pd.DataFrame(0, index = confusion_matrix.index, columns = ['Precision', 'Recall', 'F1'])\n",
    "      for  i in confusion_matrix.index:\n",
    "        TP = confusion_matrix[i][i]\n",
    "        FN = np.array(confusion_matrix[i].iloc[0:i].values.tolist() + confusion_matrix[i].iloc[i+1:].values.tolist()).sum()\n",
    "        FP = np.array(confusion_matrix.iloc[i][0:i].values.tolist() + confusion_matrix.iloc[i][i + 1:].values.tolist()).sum()\n",
    "        TN = confusion_matrix.sum().sum() - TP - FN - FP\n",
    "\n",
    "        Precision = TP / (TP + FP)\n",
    "        Recall = TP / (TP + FN)\n",
    "        F1 = (2 * Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "        scores_df.loc[i, 'Precision'] = Precision\n",
    "        scores_df.loc[i, 'Recall'] = Recall\n",
    "        scores_df.loc[i, 'F1'] = F1\n",
    "\n",
    "      scores_df.index.name = 'Label'\n",
    "      \n",
    "      return scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b8e4c",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db653338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/4s9pjgcj79dbp8y5rdx60v_h0000gn/T/ipykernel_78806/1875021445.py:61: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  encoded_label[int(label)] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 has been trained with Train Loss: 1.7936, Training Accuracy: 37.774% and Testing Accuracy: 37.76%.\n",
      "Epoch 2/300 has been trained with Train Loss: 1.689, Training Accuracy: 40.268% and Testing Accuracy: 39.17%.\n",
      "Epoch 3/300 has been trained with Train Loss: 1.6546, Training Accuracy: 41.132% and Testing Accuracy: 40.29%.\n",
      "Epoch 4/300 has been trained with Train Loss: 1.6391, Training Accuracy: 41.21% and Testing Accuracy: 39.98%.\n",
      "Epoch 5/300 has been trained with Train Loss: 1.6278, Training Accuracy: 41.418% and Testing Accuracy: 39.89%.\n",
      "Epoch 6/300 has been trained with Train Loss: 1.6209, Training Accuracy: 42.332% and Testing Accuracy: 41.66%.\n",
      "Epoch 7/300 has been trained with Train Loss: 1.6091, Training Accuracy: 42.486% and Testing Accuracy: 41.0%.\n",
      "Epoch 8/300 has been trained with Train Loss: 1.606, Training Accuracy: 42.456% and Testing Accuracy: 41.0%.\n",
      "Epoch 9/300 has been trained with Train Loss: 1.5956, Training Accuracy: 42.336% and Testing Accuracy: 41.11%.\n",
      "Epoch 10/300 has been trained with Train Loss: 1.5949, Training Accuracy: 42.904% and Testing Accuracy: 41.47%.\n",
      "Epoch 11/300 has been trained with Train Loss: 1.5877, Training Accuracy: 42.81% and Testing Accuracy: 41.5%.\n",
      "Epoch 12/300 has been trained with Train Loss: 1.5816, Training Accuracy: 43.386% and Testing Accuracy: 41.25%.\n",
      "Epoch 13/300 has been trained with Train Loss: 1.5862, Training Accuracy: 44.02% and Testing Accuracy: 41.52%.\n",
      "Epoch 14/300 has been trained with Train Loss: 1.5787, Training Accuracy: 43.934% and Testing Accuracy: 43.07%.\n",
      "Epoch 15/300 has been trained with Train Loss: 1.5795, Training Accuracy: 43.916% and Testing Accuracy: 42.02%.\n",
      "Epoch 16/300 has been trained with Train Loss: 1.5725, Training Accuracy: 43.924% and Testing Accuracy: 42.75%.\n",
      "Epoch 17/300 has been trained with Train Loss: 1.5736, Training Accuracy: 43.416% and Testing Accuracy: 41.84%.\n",
      "Epoch 18/300 has been trained with Train Loss: 1.5709, Training Accuracy: 45.07% and Testing Accuracy: 43.03%.\n",
      "Epoch 19/300 has been trained with Train Loss: 1.5662, Training Accuracy: 44.244% and Testing Accuracy: 42.39%.\n",
      "Epoch 20/300 has been trained with Train Loss: 1.5676, Training Accuracy: 44.118% and Testing Accuracy: 42.04%.\n",
      "Epoch 21/300 has been trained with Train Loss: 1.5639, Training Accuracy: 44.052% and Testing Accuracy: 41.85%.\n",
      "Epoch 22/300 has been trained with Train Loss: 1.5643, Training Accuracy: 44.63% and Testing Accuracy: 42.98%.\n",
      "Epoch 23/300 has been trained with Train Loss: 1.5557, Training Accuracy: 44.426% and Testing Accuracy: 42.74%.\n",
      "Epoch 24/300 has been trained with Train Loss: 1.5549, Training Accuracy: 43.91% and Testing Accuracy: 41.81%.\n",
      "Epoch 25/300 has been trained with Train Loss: 1.5581, Training Accuracy: 44.692% and Testing Accuracy: 42.51%.\n",
      "Epoch 26/300 has been trained with Train Loss: 1.5528, Training Accuracy: 44.204% and Testing Accuracy: 42.43%.\n",
      "Epoch 27/300 has been trained with Train Loss: 1.5534, Training Accuracy: 43.898% and Testing Accuracy: 40.93%.\n",
      "Epoch 28/300 has been trained with Train Loss: 1.5515, Training Accuracy: 45.638% and Testing Accuracy: 43.47%.\n",
      "Epoch 29/300 has been trained with Train Loss: 1.5475, Training Accuracy: 44.496% and Testing Accuracy: 42.37%.\n",
      "Epoch 30/300 has been trained with Train Loss: 1.549, Training Accuracy: 44.334% and Testing Accuracy: 42.08%.\n",
      "Epoch 31/300 has been trained with Train Loss: 1.5432, Training Accuracy: 44.902% and Testing Accuracy: 42.34%.\n",
      "Epoch 32/300 has been trained with Train Loss: 1.5449, Training Accuracy: 45.078% and Testing Accuracy: 43.75%.\n",
      "Epoch 33/300 has been trained with Train Loss: 1.5391, Training Accuracy: 44.768% and Testing Accuracy: 41.86%.\n",
      "Epoch 34/300 has been trained with Train Loss: 1.541, Training Accuracy: 44.544% and Testing Accuracy: 42.76%.\n",
      "Epoch 35/300 has been trained with Train Loss: 1.5385, Training Accuracy: 44.626% and Testing Accuracy: 41.7%.\n",
      "Epoch 36/300 has been trained with Train Loss: 1.5351, Training Accuracy: 44.648% and Testing Accuracy: 42.39%.\n",
      "Epoch 37/300 has been trained with Train Loss: 1.5387, Training Accuracy: 46.02% and Testing Accuracy: 42.97%.\n",
      "Epoch 38/300 has been trained with Train Loss: 1.5388, Training Accuracy: 44.758% and Testing Accuracy: 41.92%.\n",
      "Epoch 39/300 has been trained with Train Loss: 1.5382, Training Accuracy: 46.222% and Testing Accuracy: 43.88%.\n",
      "Epoch 40/300 has been trained with Train Loss: 1.5348, Training Accuracy: 44.512% and Testing Accuracy: 42.23%.\n",
      "Epoch 41/300 has been trained with Train Loss: 1.5389, Training Accuracy: 45.894% and Testing Accuracy: 43.37%.\n",
      "Epoch 42/300 has been trained with Train Loss: 1.529, Training Accuracy: 44.642% and Testing Accuracy: 42.18%.\n",
      "Epoch 43/300 has been trained with Train Loss: 1.5318, Training Accuracy: 44.198% and Testing Accuracy: 42.25%.\n",
      "Epoch 44/300 has been trained with Train Loss: 1.5299, Training Accuracy: 45.338% and Testing Accuracy: 43.77%.\n",
      "Epoch 45/300 has been trained with Train Loss: 1.5356, Training Accuracy: 45.714% and Testing Accuracy: 43.57%.\n",
      "Epoch 46/300 has been trained with Train Loss: 1.5305, Training Accuracy: 45.376% and Testing Accuracy: 42.61%.\n",
      "Epoch 47/300 has been trained with Train Loss: 1.529, Training Accuracy: 45.6% and Testing Accuracy: 42.96%.\n",
      "Epoch 48/300 has been trained with Train Loss: 1.5232, Training Accuracy: 44.562% and Testing Accuracy: 42.69%.\n",
      "Epoch 49/300 has been trained with Train Loss: 1.523, Training Accuracy: 45.442% and Testing Accuracy: 42.63%.\n",
      "Epoch 50/300 has been trained with Train Loss: 1.5238, Training Accuracy: 45.684% and Testing Accuracy: 44.01%.\n",
      "Epoch 51/300 has been trained with Train Loss: 1.5222, Training Accuracy: 45.972% and Testing Accuracy: 43.29%.\n",
      "Epoch 52/300 has been trained with Train Loss: 1.5238, Training Accuracy: 45.492% and Testing Accuracy: 43.69%.\n",
      "Epoch 53/300 has been trained with Train Loss: 1.5224, Training Accuracy: 45.732% and Testing Accuracy: 44.04%.\n",
      "Epoch 54/300 has been trained with Train Loss: 1.5182, Training Accuracy: 45.872% and Testing Accuracy: 43.0%.\n",
      "Epoch 55/300 has been trained with Train Loss: 1.5228, Training Accuracy: 45.376% and Testing Accuracy: 42.43%.\n",
      "Epoch 56/300 has been trained with Train Loss: 1.5236, Training Accuracy: 46.664% and Testing Accuracy: 44.26%.\n",
      "Epoch 57/300 has been trained with Train Loss: 1.5163, Training Accuracy: 46.59% and Testing Accuracy: 43.97%.\n",
      "Epoch 58/300 has been trained with Train Loss: 1.5204, Training Accuracy: 45.558% and Testing Accuracy: 43.07%.\n",
      "Epoch 59/300 has been trained with Train Loss: 1.5157, Training Accuracy: 45.876% and Testing Accuracy: 43.34%.\n",
      "Epoch 60/300 has been trained with Train Loss: 1.513, Training Accuracy: 43.956% and Testing Accuracy: 41.71%.\n",
      "Epoch 61/300 has been trained with Train Loss: 1.5192, Training Accuracy: 45.902% and Testing Accuracy: 42.73%.\n",
      "Epoch 62/300 has been trained with Train Loss: 1.511, Training Accuracy: 46.108% and Testing Accuracy: 43.07%.\n",
      "Epoch 63/300 has been trained with Train Loss: 1.5148, Training Accuracy: 46.066% and Testing Accuracy: 43.92%.\n",
      "Epoch 64/300 has been trained with Train Loss: 1.51, Training Accuracy: 46.106% and Testing Accuracy: 43.15%.\n",
      "Epoch 65/300 has been trained with Train Loss: 1.5065, Training Accuracy: 45.436% and Testing Accuracy: 42.9%.\n",
      "Epoch 66/300 has been trained with Train Loss: 1.5171, Training Accuracy: 46.456% and Testing Accuracy: 43.91%.\n",
      "Epoch 67/300 has been trained with Train Loss: 1.5111, Training Accuracy: 45.688% and Testing Accuracy: 43.45%.\n",
      "Epoch 68/300 has been trained with Train Loss: 1.5068, Training Accuracy: 45.39% and Testing Accuracy: 42.41%.\n",
      "Epoch 69/300 has been trained with Train Loss: 1.5114, Training Accuracy: 45.762% and Testing Accuracy: 43.12%.\n",
      "Epoch 70/300 has been trained with Train Loss: 1.5052, Training Accuracy: 45.932% and Testing Accuracy: 43.09%.\n",
      "Epoch 71/300 has been trained with Train Loss: 1.5056, Training Accuracy: 45.754% and Testing Accuracy: 43.63%.\n",
      "Epoch 72/300 has been trained with Train Loss: 1.5094, Training Accuracy: 46.762% and Testing Accuracy: 44.38%.\n",
      "Epoch 73/300 has been trained with Train Loss: 1.5049, Training Accuracy: 45.986% and Testing Accuracy: 43.19%.\n",
      "Epoch 74/300 has been trained with Train Loss: 1.5023, Training Accuracy: 45.57% and Testing Accuracy: 43.16%.\n",
      "Epoch 75/300 has been trained with Train Loss: 1.5013, Training Accuracy: 46.016% and Testing Accuracy: 44.29%.\n",
      "Epoch 76/300 has been trained with Train Loss: 1.5021, Training Accuracy: 46.178% and Testing Accuracy: 44.51%.\n",
      "Epoch 77/300 has been trained with Train Loss: 1.4983, Training Accuracy: 46.44% and Testing Accuracy: 43.24%.\n",
      "Epoch 78/300 has been trained with Train Loss: 1.5042, Training Accuracy: 46.68% and Testing Accuracy: 43.67%.\n",
      "Epoch 79/300 has been trained with Train Loss: 1.5, Training Accuracy: 45.938% and Testing Accuracy: 43.2%.\n",
      "Epoch 80/300 has been trained with Train Loss: 1.4964, Training Accuracy: 46.55% and Testing Accuracy: 43.8%.\n",
      "Epoch 81/300 has been trained with Train Loss: 1.4984, Training Accuracy: 45.684% and Testing Accuracy: 42.7%.\n",
      "Epoch 82/300 has been trained with Train Loss: 1.4994, Training Accuracy: 45.394% and Testing Accuracy: 42.28%.\n",
      "Epoch 83/300 has been trained with Train Loss: 1.5048, Training Accuracy: 46.236% and Testing Accuracy: 42.97%.\n",
      "Epoch 84/300 has been trained with Train Loss: 1.4987, Training Accuracy: 47.374% and Testing Accuracy: 44.24%.\n",
      "Epoch 85/300 has been trained with Train Loss: 1.498, Training Accuracy: 47.438% and Testing Accuracy: 43.81%.\n",
      "Epoch 86/300 has been trained with Train Loss: 1.5007, Training Accuracy: 46.688% and Testing Accuracy: 44.22%.\n",
      "Epoch 87/300 has been trained with Train Loss: 1.4968, Training Accuracy: 46.578% and Testing Accuracy: 43.67%.\n",
      "Epoch 88/300 has been trained with Train Loss: 1.4959, Training Accuracy: 46.948% and Testing Accuracy: 43.96%.\n",
      "Epoch 89/300 has been trained with Train Loss: 1.495, Training Accuracy: 46.848% and Testing Accuracy: 44.44%.\n",
      "Epoch 90/300 has been trained with Train Loss: 1.4937, Training Accuracy: 46.932% and Testing Accuracy: 43.42%.\n",
      "Epoch 91/300 has been trained with Train Loss: 1.4933, Training Accuracy: 46.266% and Testing Accuracy: 43.06%.\n",
      "Epoch 92/300 has been trained with Train Loss: 1.4916, Training Accuracy: 46.55% and Testing Accuracy: 44.39%.\n",
      "Epoch 93/300 has been trained with Train Loss: 1.4892, Training Accuracy: 47.318% and Testing Accuracy: 44.02%.\n",
      "Epoch 94/300 has been trained with Train Loss: 1.4915, Training Accuracy: 46.678% and Testing Accuracy: 44.28%.\n",
      "Epoch 95/300 has been trained with Train Loss: 1.4923, Training Accuracy: 46.834% and Testing Accuracy: 42.87%.\n",
      "Epoch 96/300 has been trained with Train Loss: 1.4952, Training Accuracy: 46.844% and Testing Accuracy: 44.07%.\n",
      "Epoch 97/300 has been trained with Train Loss: 1.4939, Training Accuracy: 46.822% and Testing Accuracy: 44.27%.\n",
      "Epoch 98/300 has been trained with Train Loss: 1.4908, Training Accuracy: 47.092% and Testing Accuracy: 43.84%.\n",
      "Epoch 99/300 has been trained with Train Loss: 1.4894, Training Accuracy: 46.646% and Testing Accuracy: 43.79%.\n",
      "Epoch 100/300 has been trained with Train Loss: 1.4919, Training Accuracy: 47.19% and Testing Accuracy: 43.69%.\n",
      "Epoch 101/300 has been trained with Train Loss: 1.4898, Training Accuracy: 46.982% and Testing Accuracy: 43.94%.\n",
      "Epoch 102/300 has been trained with Train Loss: 1.4827, Training Accuracy: 46.474% and Testing Accuracy: 43.59%.\n",
      "Epoch 103/300 has been trained with Train Loss: 1.487, Training Accuracy: 47.022% and Testing Accuracy: 44.7%.\n",
      "Epoch 104/300 has been trained with Train Loss: 1.4889, Training Accuracy: 47.612% and Testing Accuracy: 44.04%.\n",
      "Epoch 105/300 has been trained with Train Loss: 1.4874, Training Accuracy: 47.622% and Testing Accuracy: 44.44%.\n",
      "Epoch 106/300 has been trained with Train Loss: 1.4892, Training Accuracy: 47.392% and Testing Accuracy: 44.59%.\n",
      "Epoch 107/300 has been trained with Train Loss: 1.4882, Training Accuracy: 47.324% and Testing Accuracy: 44.06%.\n",
      "Epoch 108/300 has been trained with Train Loss: 1.4836, Training Accuracy: 46.998% and Testing Accuracy: 44.3%.\n",
      "Epoch 109/300 has been trained with Train Loss: 1.4874, Training Accuracy: 47.314% and Testing Accuracy: 44.07%.\n",
      "Epoch 110/300 has been trained with Train Loss: 1.4866, Training Accuracy: 47.02% and Testing Accuracy: 44.32%.\n",
      "Epoch 111/300 has been trained with Train Loss: 1.4845, Training Accuracy: 47.14% and Testing Accuracy: 44.39%.\n",
      "Epoch 112/300 has been trained with Train Loss: 1.4868, Training Accuracy: 46.826% and Testing Accuracy: 43.24%.\n",
      "Epoch 113/300 has been trained with Train Loss: 1.4864, Training Accuracy: 46.178% and Testing Accuracy: 43.2%.\n",
      "Epoch 114/300 has been trained with Train Loss: 1.4797, Training Accuracy: 46.73% and Testing Accuracy: 43.54%.\n",
      "Epoch 115/300 has been trained with Train Loss: 1.4844, Training Accuracy: 46.588% and Testing Accuracy: 43.75%.\n",
      "Epoch 116/300 has been trained with Train Loss: 1.4808, Training Accuracy: 46.69% and Testing Accuracy: 43.94%.\n",
      "Epoch 117/300 has been trained with Train Loss: 1.4844, Training Accuracy: 47.122% and Testing Accuracy: 44.4%.\n",
      "Epoch 118/300 has been trained with Train Loss: 1.4798, Training Accuracy: 46.972% and Testing Accuracy: 44.3%.\n",
      "Epoch 119/300 has been trained with Train Loss: 1.4851, Training Accuracy: 47.208% and Testing Accuracy: 44.19%.\n",
      "Epoch 120/300 has been trained with Train Loss: 1.4792, Training Accuracy: 47.348% and Testing Accuracy: 44.89%.\n",
      "Epoch 121/300 has been trained with Train Loss: 1.4796, Training Accuracy: 46.588% and Testing Accuracy: 43.49%.\n",
      "Epoch 122/300 has been trained with Train Loss: 1.4796, Training Accuracy: 46.636% and Testing Accuracy: 44.04%.\n",
      "Epoch 123/300 has been trained with Train Loss: 1.4819, Training Accuracy: 47.118% and Testing Accuracy: 43.54%.\n",
      "Epoch 124/300 has been trained with Train Loss: 1.4811, Training Accuracy: 46.962% and Testing Accuracy: 43.89%.\n",
      "Epoch 125/300 has been trained with Train Loss: 1.4808, Training Accuracy: 47.53% and Testing Accuracy: 44.51%.\n",
      "Epoch 126/300 has been trained with Train Loss: 1.4788, Training Accuracy: 47.798% and Testing Accuracy: 44.94%.\n",
      "Epoch 127/300 has been trained with Train Loss: 1.4777, Training Accuracy: 47.018% and Testing Accuracy: 44.05%.\n",
      "Epoch 128/300 has been trained with Train Loss: 1.4772, Training Accuracy: 47.478% and Testing Accuracy: 43.58%.\n",
      "Epoch 129/300 has been trained with Train Loss: 1.4779, Training Accuracy: 47.51% and Testing Accuracy: 44.26%.\n",
      "Epoch 130/300 has been trained with Train Loss: 1.4759, Training Accuracy: 47.848% and Testing Accuracy: 44.32%.\n",
      "Epoch 131/300 has been trained with Train Loss: 1.4724, Training Accuracy: 47.422% and Testing Accuracy: 44.81%.\n",
      "Epoch 132/300 has been trained with Train Loss: 1.4769, Training Accuracy: 47.542% and Testing Accuracy: 44.17%.\n",
      "Epoch 133/300 has been trained with Train Loss: 1.4723, Training Accuracy: 47.1% and Testing Accuracy: 43.74%.\n",
      "Epoch 134/300 has been trained with Train Loss: 1.4787, Training Accuracy: 48.03% and Testing Accuracy: 44.86%.\n",
      "Epoch 135/300 has been trained with Train Loss: 1.4729, Training Accuracy: 47.35% and Testing Accuracy: 45.01%.\n",
      "Epoch 136/300 has been trained with Train Loss: 1.4732, Training Accuracy: 48.316% and Testing Accuracy: 45.5%.\n",
      "Epoch 137/300 has been trained with Train Loss: 1.4747, Training Accuracy: 47.764% and Testing Accuracy: 45.21%.\n",
      "Epoch 138/300 has been trained with Train Loss: 1.4682, Training Accuracy: 47.226% and Testing Accuracy: 43.59%.\n",
      "Epoch 139/300 has been trained with Train Loss: 1.4754, Training Accuracy: 47.722% and Testing Accuracy: 44.84%.\n",
      "Epoch 140/300 has been trained with Train Loss: 1.4744, Training Accuracy: 46.808% and Testing Accuracy: 44.52%.\n",
      "Epoch 141/300 has been trained with Train Loss: 1.4691, Training Accuracy: 47.534% and Testing Accuracy: 44.61%.\n",
      "Epoch 142/300 has been trained with Train Loss: 1.4793, Training Accuracy: 46.908% and Testing Accuracy: 44.29%.\n",
      "Epoch 143/300 has been trained with Train Loss: 1.4738, Training Accuracy: 47.092% and Testing Accuracy: 43.75%.\n",
      "Epoch 144/300 has been trained with Train Loss: 1.4749, Training Accuracy: 48.126% and Testing Accuracy: 45.17%.\n",
      "Epoch 145/300 has been trained with Train Loss: 1.4679, Training Accuracy: 46.3% and Testing Accuracy: 42.9%.\n",
      "Epoch 146/300 has been trained with Train Loss: 1.4712, Training Accuracy: 47.498% and Testing Accuracy: 44.01%.\n",
      "Epoch 147/300 has been trained with Train Loss: 1.4735, Training Accuracy: 47.736% and Testing Accuracy: 44.59%.\n",
      "Epoch 148/300 has been trained with Train Loss: 1.467, Training Accuracy: 46.56% and Testing Accuracy: 43.42%.\n",
      "Epoch 149/300 has been trained with Train Loss: 1.4787, Training Accuracy: 46.946% and Testing Accuracy: 43.98%.\n",
      "Epoch 150/300 has been trained with Train Loss: 1.4711, Training Accuracy: 47.598% and Testing Accuracy: 44.03%.\n",
      "Epoch 151/300 has been trained with Train Loss: 1.4707, Training Accuracy: 47.586% and Testing Accuracy: 44.74%.\n",
      "Epoch 152/300 has been trained with Train Loss: 1.4676, Training Accuracy: 47.656% and Testing Accuracy: 45.21%.\n",
      "Epoch 153/300 has been trained with Train Loss: 1.4695, Training Accuracy: 47.45% and Testing Accuracy: 43.83%.\n",
      "Epoch 154/300 has been trained with Train Loss: 1.4682, Training Accuracy: 47.344% and Testing Accuracy: 44.28%.\n",
      "Epoch 155/300 has been trained with Train Loss: 1.471, Training Accuracy: 46.986% and Testing Accuracy: 43.22%.\n",
      "Epoch 156/300 has been trained with Train Loss: 1.4691, Training Accuracy: 47.954% and Testing Accuracy: 43.63%.\n",
      "Epoch 157/300 has been trained with Train Loss: 1.4684, Training Accuracy: 47.55% and Testing Accuracy: 44.37%.\n",
      "Epoch 158/300 has been trained with Train Loss: 1.4689, Training Accuracy: 47.824% and Testing Accuracy: 44.16%.\n",
      "Epoch 159/300 has been trained with Train Loss: 1.4676, Training Accuracy: 47.522% and Testing Accuracy: 44.64%.\n",
      "Epoch 160/300 has been trained with Train Loss: 1.4698, Training Accuracy: 47.466% and Testing Accuracy: 43.84%.\n",
      "Epoch 161/300 has been trained with Train Loss: 1.4688, Training Accuracy: 47.32% and Testing Accuracy: 44.13%.\n",
      "Epoch 162/300 has been trained with Train Loss: 1.4702, Training Accuracy: 47.614% and Testing Accuracy: 44.45%.\n",
      "Epoch 163/300 has been trained with Train Loss: 1.466, Training Accuracy: 47.654% and Testing Accuracy: 44.07%.\n",
      "Epoch 164/300 has been trained with Train Loss: 1.4622, Training Accuracy: 47.752% and Testing Accuracy: 43.83%.\n",
      "Epoch 165/300 has been trained with Train Loss: 1.4643, Training Accuracy: 48.114% and Testing Accuracy: 44.44%.\n",
      "Epoch 166/300 has been trained with Train Loss: 1.4708, Training Accuracy: 47.538% and Testing Accuracy: 43.89%.\n",
      "Epoch 167/300 has been trained with Train Loss: 1.4624, Training Accuracy: 46.3% and Testing Accuracy: 43.54%.\n",
      "Epoch 168/300 has been trained with Train Loss: 1.466, Training Accuracy: 47.97% and Testing Accuracy: 44.43%.\n",
      "Epoch 169/300 has been trained with Train Loss: 1.4606, Training Accuracy: 47.034% and Testing Accuracy: 44.02%.\n",
      "Epoch 170/300 has been trained with Train Loss: 1.4641, Training Accuracy: 47.716% and Testing Accuracy: 44.31%.\n",
      "Epoch 171/300 has been trained with Train Loss: 1.4613, Training Accuracy: 46.924% and Testing Accuracy: 43.05%.\n",
      "Epoch 172/300 has been trained with Train Loss: 1.4624, Training Accuracy: 47.852% and Testing Accuracy: 44.96%.\n",
      "Epoch 173/300 has been trained with Train Loss: 1.4637, Training Accuracy: 47.414% and Testing Accuracy: 44.35%.\n",
      "Epoch 174/300 has been trained with Train Loss: 1.4628, Training Accuracy: 48.264% and Testing Accuracy: 43.95%.\n",
      "Epoch 175/300 has been trained with Train Loss: 1.4625, Training Accuracy: 47.968% and Testing Accuracy: 44.24%.\n",
      "Epoch 176/300 has been trained with Train Loss: 1.4644, Training Accuracy: 48.41% and Testing Accuracy: 44.39%.\n",
      "Epoch 177/300 has been trained with Train Loss: 1.4594, Training Accuracy: 47.83% and Testing Accuracy: 44.28%.\n",
      "Epoch 178/300 has been trained with Train Loss: 1.4596, Training Accuracy: 47.84% and Testing Accuracy: 44.37%.\n",
      "Epoch 179/300 has been trained with Train Loss: 1.4584, Training Accuracy: 47.416% and Testing Accuracy: 44.56%.\n",
      "Epoch 180/300 has been trained with Train Loss: 1.46, Training Accuracy: 48.21% and Testing Accuracy: 44.51%.\n",
      "Epoch 181/300 has been trained with Train Loss: 1.4621, Training Accuracy: 48.026% and Testing Accuracy: 44.28%.\n",
      "Epoch 182/300 has been trained with Train Loss: 1.462, Training Accuracy: 46.78% and Testing Accuracy: 43.35%.\n",
      "Epoch 183/300 has been trained with Train Loss: 1.4585, Training Accuracy: 48.104% and Testing Accuracy: 44.95%.\n",
      "Epoch 184/300 has been trained with Train Loss: 1.4596, Training Accuracy: 47.694% and Testing Accuracy: 44.18%.\n",
      "Epoch 185/300 has been trained with Train Loss: 1.4593, Training Accuracy: 48.61% and Testing Accuracy: 45.1%.\n",
      "Epoch 186/300 has been trained with Train Loss: 1.4543, Training Accuracy: 47.41% and Testing Accuracy: 44.39%.\n",
      "Epoch 187/300 has been trained with Train Loss: 1.4588, Training Accuracy: 47.61% and Testing Accuracy: 43.83%.\n",
      "Epoch 188/300 has been trained with Train Loss: 1.4562, Training Accuracy: 47.53% and Testing Accuracy: 44.31%.\n",
      "Epoch 189/300 has been trained with Train Loss: 1.4623, Training Accuracy: 47.122% and Testing Accuracy: 43.34%.\n",
      "Epoch 190/300 has been trained with Train Loss: 1.461, Training Accuracy: 48.324% and Testing Accuracy: 44.07%.\n",
      "Epoch 191/300 has been trained with Train Loss: 1.4619, Training Accuracy: 48.454% and Testing Accuracy: 45.68%.\n",
      "Epoch 192/300 has been trained with Train Loss: 1.4548, Training Accuracy: 47.958% and Testing Accuracy: 44.12%.\n",
      "Epoch 193/300 has been trained with Train Loss: 1.4597, Training Accuracy: 48.01% and Testing Accuracy: 45.03%.\n",
      "Epoch 194/300 has been trained with Train Loss: 1.4558, Training Accuracy: 47.746% and Testing Accuracy: 44.09%.\n",
      "Epoch 195/300 has been trained with Train Loss: 1.459, Training Accuracy: 47.87% and Testing Accuracy: 44.8%.\n",
      "Epoch 196/300 has been trained with Train Loss: 1.4557, Training Accuracy: 48.266% and Testing Accuracy: 44.68%.\n",
      "Epoch 197/300 has been trained with Train Loss: 1.46, Training Accuracy: 48.106% and Testing Accuracy: 45.01%.\n",
      "Epoch 198/300 has been trained with Train Loss: 1.4588, Training Accuracy: 47.356% and Testing Accuracy: 43.89%.\n",
      "Epoch 199/300 has been trained with Train Loss: 1.4537, Training Accuracy: 47.582% and Testing Accuracy: 43.81%.\n",
      "Epoch 200/300 has been trained with Train Loss: 1.4547, Training Accuracy: 48.066% and Testing Accuracy: 43.86%.\n",
      "Epoch 201/300 has been trained with Train Loss: 1.4609, Training Accuracy: 47.97% and Testing Accuracy: 44.8%.\n",
      "Epoch 202/300 has been trained with Train Loss: 1.4557, Training Accuracy: 47.602% and Testing Accuracy: 44.18%.\n",
      "Epoch 203/300 has been trained with Train Loss: 1.4598, Training Accuracy: 48.418% and Testing Accuracy: 44.49%.\n",
      "Epoch 204/300 has been trained with Train Loss: 1.4537, Training Accuracy: 48.202% and Testing Accuracy: 44.8%.\n",
      "Epoch 205/300 has been trained with Train Loss: 1.4575, Training Accuracy: 47.426% and Testing Accuracy: 43.6%.\n",
      "Epoch 206/300 has been trained with Train Loss: 1.4545, Training Accuracy: 48.376% and Testing Accuracy: 45.54%.\n",
      "Epoch 207/300 has been trained with Train Loss: 1.4615, Training Accuracy: 47.87% and Testing Accuracy: 44.42%.\n",
      "Epoch 208/300 has been trained with Train Loss: 1.4584, Training Accuracy: 48.15% and Testing Accuracy: 44.98%.\n",
      "Epoch 209/300 has been trained with Train Loss: 1.4544, Training Accuracy: 48.146% and Testing Accuracy: 44.11%.\n",
      "Epoch 210/300 has been trained with Train Loss: 1.4538, Training Accuracy: 47.62% and Testing Accuracy: 44.16%.\n",
      "Epoch 211/300 has been trained with Train Loss: 1.4549, Training Accuracy: 48.326% and Testing Accuracy: 44.61%.\n",
      "Epoch 212/300 has been trained with Train Loss: 1.4552, Training Accuracy: 48.66% and Testing Accuracy: 44.85%.\n",
      "Epoch 213/300 has been trained with Train Loss: 1.4571, Training Accuracy: 48.26% and Testing Accuracy: 45.59%.\n",
      "Epoch 214/300 has been trained with Train Loss: 1.4564, Training Accuracy: 48.38% and Testing Accuracy: 44.99%.\n",
      "Epoch 215/300 has been trained with Train Loss: 1.453, Training Accuracy: 47.81% and Testing Accuracy: 44.78%.\n",
      "Epoch 216/300 has been trained with Train Loss: 1.4545, Training Accuracy: 47.882% and Testing Accuracy: 43.95%.\n",
      "Epoch 217/300 has been trained with Train Loss: 1.4542, Training Accuracy: 47.798% and Testing Accuracy: 45.08%.\n",
      "Epoch 218/300 has been trained with Train Loss: 1.4537, Training Accuracy: 48.042% and Testing Accuracy: 43.94%.\n",
      "Epoch 219/300 has been trained with Train Loss: 1.4562, Training Accuracy: 48.35% and Testing Accuracy: 44.37%.\n",
      "Epoch 220/300 has been trained with Train Loss: 1.4538, Training Accuracy: 48.354% and Testing Accuracy: 44.98%.\n",
      "Epoch 221/300 has been trained with Train Loss: 1.4489, Training Accuracy: 48.262% and Testing Accuracy: 44.49%.\n",
      "Epoch 222/300 has been trained with Train Loss: 1.4519, Training Accuracy: 48.5% and Testing Accuracy: 44.3%.\n",
      "Epoch 223/300 has been trained with Train Loss: 1.4539, Training Accuracy: 48.49% and Testing Accuracy: 44.89%.\n",
      "Epoch 224/300 has been trained with Train Loss: 1.4512, Training Accuracy: 47.694% and Testing Accuracy: 45.01%.\n",
      "Epoch 225/300 has been trained with Train Loss: 1.4497, Training Accuracy: 47.702% and Testing Accuracy: 43.78%.\n",
      "Epoch 226/300 has been trained with Train Loss: 1.45, Training Accuracy: 47.814% and Testing Accuracy: 43.9%.\n",
      "Epoch 227/300 has been trained with Train Loss: 1.4565, Training Accuracy: 48.586% and Testing Accuracy: 44.75%.\n",
      "Epoch 228/300 has been trained with Train Loss: 1.4475, Training Accuracy: 47.88% and Testing Accuracy: 44.23%.\n",
      "Epoch 229/300 has been trained with Train Loss: 1.455, Training Accuracy: 48.438% and Testing Accuracy: 44.58%.\n",
      "Epoch 230/300 has been trained with Train Loss: 1.4575, Training Accuracy: 47.518% and Testing Accuracy: 44.03%.\n",
      "Epoch 231/300 has been trained with Train Loss: 1.4531, Training Accuracy: 48.948% and Testing Accuracy: 44.99%.\n",
      "Epoch 232/300 has been trained with Train Loss: 1.4542, Training Accuracy: 47.744% and Testing Accuracy: 44.26%.\n",
      "Epoch 233/300 has been trained with Train Loss: 1.4525, Training Accuracy: 48.022% and Testing Accuracy: 44.67%.\n",
      "Epoch 234/300 has been trained with Train Loss: 1.4482, Training Accuracy: 48.252% and Testing Accuracy: 45.47%.\n",
      "Epoch 235/300 has been trained with Train Loss: 1.4493, Training Accuracy: 47.932% and Testing Accuracy: 44.85%.\n",
      "Epoch 236/300 has been trained with Train Loss: 1.4515, Training Accuracy: 48.132% and Testing Accuracy: 44.36%.\n",
      "Epoch 237/300 has been trained with Train Loss: 1.4484, Training Accuracy: 47.772% and Testing Accuracy: 43.91%.\n",
      "Epoch 238/300 has been trained with Train Loss: 1.4476, Training Accuracy: 47.652% and Testing Accuracy: 43.85%.\n",
      "Epoch 239/300 has been trained with Train Loss: 1.4417, Training Accuracy: 48.656% and Testing Accuracy: 44.6%.\n",
      "Epoch 240/300 has been trained with Train Loss: 1.4532, Training Accuracy: 48.772% and Testing Accuracy: 44.79%.\n",
      "Epoch 241/300 has been trained with Train Loss: 1.4497, Training Accuracy: 48.74% and Testing Accuracy: 45.64%.\n",
      "Epoch 242/300 has been trained with Train Loss: 1.447, Training Accuracy: 48.448% and Testing Accuracy: 44.94%.\n",
      "Epoch 243/300 has been trained with Train Loss: 1.4499, Training Accuracy: 48.224% and Testing Accuracy: 44.73%.\n",
      "Epoch 244/300 has been trained with Train Loss: 1.4511, Training Accuracy: 48.914% and Testing Accuracy: 44.7%.\n",
      "Epoch 245/300 has been trained with Train Loss: 1.4497, Training Accuracy: 48.156% and Testing Accuracy: 44.31%.\n",
      "Epoch 246/300 has been trained with Train Loss: 1.448, Training Accuracy: 48.064% and Testing Accuracy: 43.84%.\n",
      "Epoch 247/300 has been trained with Train Loss: 1.4498, Training Accuracy: 48.478% and Testing Accuracy: 44.69%.\n",
      "Epoch 248/300 has been trained with Train Loss: 1.4452, Training Accuracy: 48.118% and Testing Accuracy: 44.16%.\n",
      "Epoch 249/300 has been trained with Train Loss: 1.4462, Training Accuracy: 47.896% and Testing Accuracy: 44.12%.\n",
      "Epoch 250/300 has been trained with Train Loss: 1.4439, Training Accuracy: 48.436% and Testing Accuracy: 44.39%.\n",
      "Epoch 251/300 has been trained with Train Loss: 1.4471, Training Accuracy: 47.832% and Testing Accuracy: 44.76%.\n",
      "Epoch 252/300 has been trained with Train Loss: 1.4502, Training Accuracy: 48.446% and Testing Accuracy: 44.46%.\n",
      "Epoch 253/300 has been trained with Train Loss: 1.4468, Training Accuracy: 48.362% and Testing Accuracy: 44.67%.\n",
      "Epoch 254/300 has been trained with Train Loss: 1.4505, Training Accuracy: 48.714% and Testing Accuracy: 44.74%.\n",
      "Epoch 255/300 has been trained with Train Loss: 1.4521, Training Accuracy: 47.922% and Testing Accuracy: 43.95%.\n",
      "Epoch 256/300 has been trained with Train Loss: 1.445, Training Accuracy: 48.428% and Testing Accuracy: 44.3%.\n",
      "Epoch 257/300 has been trained with Train Loss: 1.4452, Training Accuracy: 48.542% and Testing Accuracy: 44.83%.\n",
      "Epoch 258/300 has been trained with Train Loss: 1.4472, Training Accuracy: 48.486% and Testing Accuracy: 44.21%.\n",
      "Epoch 259/300 has been trained with Train Loss: 1.4486, Training Accuracy: 48.606% and Testing Accuracy: 44.49%.\n",
      "Epoch 260/300 has been trained with Train Loss: 1.4468, Training Accuracy: 48.08% and Testing Accuracy: 44.39%.\n",
      "Epoch 261/300 has been trained with Train Loss: 1.4424, Training Accuracy: 48.342% and Testing Accuracy: 44.15%.\n",
      "Epoch 262/300 has been trained with Train Loss: 1.4455, Training Accuracy: 47.45% and Testing Accuracy: 43.77%.\n",
      "Epoch 263/300 has been trained with Train Loss: 1.4454, Training Accuracy: 48.458% and Testing Accuracy: 44.6%.\n",
      "Epoch 264/300 has been trained with Train Loss: 1.4477, Training Accuracy: 47.998% and Testing Accuracy: 44.79%.\n",
      "Epoch 265/300 has been trained with Train Loss: 1.4468, Training Accuracy: 48.564% and Testing Accuracy: 44.54%.\n",
      "Epoch 266/300 has been trained with Train Loss: 1.4463, Training Accuracy: 48.616% and Testing Accuracy: 44.81%.\n",
      "Epoch 267/300 has been trained with Train Loss: 1.4428, Training Accuracy: 48.4% and Testing Accuracy: 43.93%.\n",
      "Epoch 268/300 has been trained with Train Loss: 1.4439, Training Accuracy: 47.928% and Testing Accuracy: 44.24%.\n",
      "Epoch 269/300 has been trained with Train Loss: 1.4424, Training Accuracy: 48.106% and Testing Accuracy: 44.84%.\n",
      "Epoch 270/300 has been trained with Train Loss: 1.4467, Training Accuracy: 48.962% and Testing Accuracy: 45.35%.\n",
      "Epoch 271/300 has been trained with Train Loss: 1.4477, Training Accuracy: 48.978% and Testing Accuracy: 44.39%.\n",
      "Epoch 272/300 has been trained with Train Loss: 1.4439, Training Accuracy: 48.972% and Testing Accuracy: 45.27%.\n",
      "Epoch 273/300 has been trained with Train Loss: 1.4445, Training Accuracy: 48.45% and Testing Accuracy: 44.97%.\n",
      "Epoch 274/300 has been trained with Train Loss: 1.4434, Training Accuracy: 47.708% and Testing Accuracy: 43.13%.\n",
      "Epoch 275/300 has been trained with Train Loss: 1.4393, Training Accuracy: 47.958% and Testing Accuracy: 44.51%.\n",
      "Epoch 276/300 has been trained with Train Loss: 1.4429, Training Accuracy: 48.28% and Testing Accuracy: 44.99%.\n",
      "Epoch 277/300 has been trained with Train Loss: 1.4435, Training Accuracy: 48.478% and Testing Accuracy: 44.6%.\n",
      "Epoch 278/300 has been trained with Train Loss: 1.4448, Training Accuracy: 48.42% and Testing Accuracy: 44.1%.\n",
      "Epoch 279/300 has been trained with Train Loss: 1.4474, Training Accuracy: 47.944% and Testing Accuracy: 43.8%.\n",
      "Epoch 280/300 has been trained with Train Loss: 1.4405, Training Accuracy: 48.116% and Testing Accuracy: 45.17%.\n",
      "Epoch 281/300 has been trained with Train Loss: 1.4442, Training Accuracy: 48.572% and Testing Accuracy: 44.46%.\n",
      "Epoch 282/300 has been trained with Train Loss: 1.4417, Training Accuracy: 48.272% and Testing Accuracy: 44.5%.\n",
      "Epoch 283/300 has been trained with Train Loss: 1.439, Training Accuracy: 48.83% and Testing Accuracy: 45.25%.\n",
      "Epoch 284/300 has been trained with Train Loss: 1.44, Training Accuracy: 49.118% and Testing Accuracy: 45.22%.\n",
      "Epoch 285/300 has been trained with Train Loss: 1.4405, Training Accuracy: 48.466% and Testing Accuracy: 44.34%.\n",
      "Epoch 286/300 has been trained with Train Loss: 1.4428, Training Accuracy: 49.184% and Testing Accuracy: 45.35%.\n",
      "Epoch 287/300 has been trained with Train Loss: 1.4418, Training Accuracy: 48.398% and Testing Accuracy: 44.82%.\n",
      "Epoch 288/300 has been trained with Train Loss: 1.446, Training Accuracy: 48.272% and Testing Accuracy: 44.35%.\n",
      "Epoch 289/300 has been trained with Train Loss: 1.4455, Training Accuracy: 48.44% and Testing Accuracy: 44.19%.\n",
      "Epoch 290/300 has been trained with Train Loss: 1.444, Training Accuracy: 47.442% and Testing Accuracy: 44.07%.\n",
      "Epoch 291/300 has been trained with Train Loss: 1.4444, Training Accuracy: 48.568% and Testing Accuracy: 44.42%.\n",
      "Epoch 292/300 has been trained with Train Loss: 1.4418, Training Accuracy: 48.356% and Testing Accuracy: 44.54%.\n",
      "Epoch 293/300 has been trained with Train Loss: 1.4415, Training Accuracy: 48.212% and Testing Accuracy: 44.24%.\n",
      "Epoch 294/300 has been trained with Train Loss: 1.4413, Training Accuracy: 47.974% and Testing Accuracy: 44.76%.\n",
      "Epoch 295/300 has been trained with Train Loss: 1.4377, Training Accuracy: 48.866% and Testing Accuracy: 44.74%.\n",
      "Epoch 296/300 has been trained with Train Loss: 1.4376, Training Accuracy: 48.464% and Testing Accuracy: 44.55%.\n",
      "Epoch 297/300 has been trained with Train Loss: 1.4364, Training Accuracy: 48.324% and Testing Accuracy: 44.58%.\n",
      "Epoch 298/300 has been trained with Train Loss: 1.4365, Training Accuracy: 48.986% and Testing Accuracy: 44.7%.\n",
      "Epoch 299/300 has been trained with Train Loss: 1.4419, Training Accuracy: 49.184% and Testing Accuracy: 45.0%.\n",
      "Epoch 300/300 has been trained with Train Loss: 1.4412, Training Accuracy: 48.35% and Testing Accuracy: 44.52%.\n",
      "============= Model Build Done =============\n",
      "Time taken to build model: 432.7209 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Instantiating our data and pre-processing it as required\n",
    "train_df = Preprocessing(train_data, train_label)\n",
    "test_df = Preprocessing(test_data, test_label)\n",
    "\n",
    "# Standardize X matrix (features)\n",
    "# train_df.normalize()\n",
    "# test_df.normalize()\n",
    "train_df.standardize()\n",
    "test_df.standardize()\n",
    "\n",
    "# Perform one-hot encoding for our label vector (ONLY ON TRAIN)\n",
    "train_df.y = train_df.label_encode(train_df.y)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "LAYER_NEURONS = [128, 150, 10]\n",
    "LAYER_ACTIVATION_FUNCS = [None, 'leakyrelu', 'softmax']\n",
    "LEARNING_RATE = 0.005\n",
    "EPOCHS = 300\n",
    "DROPOUT_PROB = 0.5\n",
    "SGD_OPTIM = None\n",
    "BATCH_SIZE = 100\n",
    "WEIGHT_DECAY = 0.98\n",
    "BATCH_NORMAL = False\n",
    "INITIALISATION = 'xavier'\n",
    "\n",
    "# Instantiate the multi-layer neural network\n",
    "nn = MLP(LAYER_NEURONS, LAYER_ACTIVATION_FUNCS, weight_decay = WEIGHT_DECAY, initialisation=INITIALISATION, batch_normal=BATCH_NORMAL)\n",
    "\n",
    "# Perform fitting using the training dataset\n",
    "t0 = time.time()\n",
    "trial1 = nn.fit(train_df.X, train_df.y, learning_rate = LEARNING_RATE, epochs = EPOCHS, SGD_optim = SGD_OPTIM, batch_size=BATCH_SIZE )\n",
    "t1 = time.time()\n",
    "print(f\"============= Model Build Done =============\")\n",
    "print(f\"Time taken to build model: {round(t1 - t0, 4)} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1616b5f",
   "metadata": {},
   "source": [
    "### Performance Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize = (20, 10))\n",
    "ax[0].plot(trial1['Training Loss'])\n",
    "ax[0].title.set_text(\"Cross-Entropy Loss over Epoch\")\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].plot(trial1['Training Accuracy'], label = \"Training Accuracy\")\n",
    "ax[1].plot(trial1['Testing Accuracy'], label = \"Testing Accuracy\")\n",
    "ax[1].title.set_text(\"Training & Testing Accuracy over Epoch\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d785dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Cross-Entropy Training Loss: 1.4412.\n",
      "Final Train accuracy: 48.556%.\n",
      "Final Test accuracy: 44.64%.\n",
      "Final Average F1 Score: 0.4407.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/4s9pjgcj79dbp8y5rdx60v_h0000gn/T/ipykernel_78806/4225359946.py:43: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  confusion_mat[int(df.y[i])].iloc[int(df.predictions[i])] += 1\n",
      "/var/folders/tk/4s9pjgcj79dbp8y5rdx60v_h0000gn/T/ipykernel_78806/4225359946.py:43: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  confusion_mat[int(df.y[i])].iloc[int(df.predictions[i])] += 1\n",
      "/var/folders/tk/4s9pjgcj79dbp8y5rdx60v_h0000gn/T/ipykernel_78806/4225359946.py:70: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.5101123595505618' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  scores_df.loc[i, 'Precision'] = Precision\n",
      "/var/folders/tk/4s9pjgcj79dbp8y5rdx60v_h0000gn/T/ipykernel_78806/4225359946.py:71: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.454' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  scores_df.loc[i, 'Recall'] = Recall\n",
      "/var/folders/tk/4s9pjgcj79dbp8y5rdx60v_h0000gn/T/ipykernel_78806/4225359946.py:72: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.48042328042328036' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  scores_df.loc[i, 'F1'] = F1\n"
     ]
    }
   ],
   "source": [
    "test_df.predictions = nn.predict(test_df.X)\n",
    "train_df.predictions = nn.predict(train_df.X)\n",
    "test_df.predictions = test_df.decode(test_df.predictions)\n",
    "train_df.predictions = train_df.decode(train_df.predictions)\n",
    "\n",
    "CM = Utils.create_confusion_mat(test_df)\n",
    "     \n",
    "measures = Utils.confusion_mat_measures(CM)\n",
    "\n",
    "\n",
    "# Accuracy & Performance Metrics\n",
    "test_accuracy = np.sum(test_df.predictions == test_df.y[:, 0]) / test_df.predictions.shape[0]\n",
    "train_accuracy = np.sum(train_df.predictions == train_label[:, 0]) / train_df.predictions.shape[0]\n",
    "F1_avg = measures['F1'].mean()\n",
    "CELoss = trial1['Training Loss'][-1]\n",
    "print(f'Final Cross-Entropy Training Loss: {round(CELoss, 4)}.')\n",
    "print(f'Final Train accuracy: {round(train_accuracy * 100, 4)}%.')\n",
    "print(f'Final Test accuracy: {round(test_accuracy * 100, 4)}%.')\n",
    "print(f'Final Average F1 Score: {round(F1_avg, 4)}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bec743",
   "metadata": {},
   "source": [
    "### Ablation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b8c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "train_df = Preprocessing(train_data, train_label)\n",
    "test_df = Preprocessing(test_data, test_label)\n",
    "\n",
    "train_df.standardize()\n",
    "test_df.standardize()\n",
    "\n",
    "train_df.y = train_df.label_encode(train_df.y)\n",
    "\n",
    "\n",
    "# --- Baseline/Default Hyperparameters ---\n",
    "BASELINE_NUM_HIDDEN_LAYERS = 3\n",
    "BASELINE_NEURONS_PER_LAYER = 100\n",
    "BASELINE_HIDDEN_ACTIVATION = 'relu'\n",
    "BASELINE_SGD_OPTIM = None\n",
    "BASELINE_WEIGHT_DECAY = 0.98\n",
    "BASELINE_INITIALISATION = 'xavier'\n",
    "BASELINE_BATCH_SIZE = 100\n",
    "BASELINE_DROPOUT_PROB = 0.5\n",
    "BASELINE_LEARNING_RATE = 0.005\n",
    "DROPOUT_PROB = BASELINE_DROPOUT_PROB\n",
    "EPOCHS = 300\n",
    "OUTPUT_ACTIVATION = 'softmax'\n",
    "INPUT_ACTIVATION = None\n",
    "\n",
    "\n",
    "input_layer_size = train_df.X.shape[1]\n",
    "try:\n",
    "    output_layer_size = len(np.unique(train_label))\n",
    "except NameError:\n",
    "     try: output_layer_size = len(np.unique(train_df.y))\n",
    "     except AttributeError: output_layer_size = train_df.y.shape[1]\n",
    "\n",
    "print(f\"Data Shapes: Input X={train_df.X.shape}, Train y={train_df.y.shape}\")\n",
    "print(f\"Deduced/Set Parameters: Input Size={input_layer_size}, Output Size={output_layer_size}\")\n",
    "\n",
    "BASELINE_HIDDEN_NEURONS = [BASELINE_NEURONS_PER_LAYER] * BASELINE_NUM_HIDDEN_LAYERS\n",
    "BASELINE_LAYER_NEURONS = [input_layer_size] + BASELINE_HIDDEN_NEURONS + [output_layer_size]\n",
    "BASELINE_ACTIVATION_FUNCS = [INPUT_ACTIVATION] + ([BASELINE_HIDDEN_ACTIVATION] * BASELINE_NUM_HIDDEN_LAYERS) + [OUTPUT_ACTIVATION]\n",
    "\n",
    "\n",
    "# --- Storage for Results ---\n",
    "neuron_study_results = {}\n",
    "activation_study_results = {}\n",
    "num_layers_study_results = {}\n",
    "sgd_optim_study_results = {}\n",
    "weight_decay_study_results = {}\n",
    "batch_size_study_results = {}\n",
    "dropout_prob_study_results = {}\n",
    "learning_rate_study_results = {}\n",
    "\n",
    "# --- Helper Function for Metric Calculation (to reduce repetition) ---\n",
    "def calculate_metrics(nn_model, trial_hist, test_dataframe, train_dataframe):\n",
    "    \"\"\"Calculates and returns key metrics after model training.\"\"\"\n",
    "    print(\"    Calculating performance metrics...\")\n",
    "    test_accuracy, F1_avg, CELoss = -1.0, -1.0, -1.0 # Defaults\n",
    "\n",
    "    try:\n",
    "        test_preds_raw = nn_model.predict(test_dataframe.X)\n",
    "        # Decode predictions (ensure decode methods exist and work)\n",
    "        test_preds_decoded = test_dataframe.decode(test_preds_raw)\n",
    "        try:\n",
    "            # Using the structure from your snippet:\n",
    "            test_accuracy = np.sum(test_preds_decoded == test_dataframe.y[:, 0]) / test_preds_decoded.shape[0]\n",
    "            original_predictions_attr = getattr(test_dataframe, 'predictions', None)\n",
    "            test_dataframe.predictions = test_preds_decoded\n",
    "            CM = Utils.create_confusion_mat(test_dataframe)\n",
    "            measures = Utils.confusion_mat_measures(CM)\n",
    "            F1_avg = measures['F1'].mean()\n",
    "            if original_predictions_attr is not None: # Restore original\n",
    "                 test_dataframe.predictions = original_predictions_attr\n",
    "            else: # Clean up\n",
    "                delattr(test_dataframe, 'predictions')\n",
    "\n",
    "\n",
    "        except Exception as metric_e:\n",
    "            print(f\"    ERROR calculating accuracy/F1: {metric_e}\")\n",
    "        try:\n",
    "            if isinstance(trial_hist, dict) and 'Training Loss' in trial_hist and len(trial_hist['Training Loss']) > 0:\n",
    "                 CELoss = trial_hist['Training Loss'][-1]\n",
    "            else:\n",
    "                 print(f\"    WARNING: Could not find 'Training Loss' in trial_hist or it's empty/not a dict. trial_hist type: {type(trial_hist)}\")\n",
    "        except Exception as loss_e:\n",
    "             print(f\"    ERROR accessing training loss: {loss_e}\")\n",
    "\n",
    "        print(f\"    Metrics: Test Accuracy={test_accuracy:.4f}, Avg F1={F1_avg:.4f}, Final Train Loss={CELoss:.4f}\")\n",
    "    except AttributeError as ae:\n",
    "         print(f\"    ERROR: Missing method/attribute like '.predict', '.decode', or '.y'? Error: {ae}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR during metrics calculation: {e}\")\n",
    "\n",
    "    return test_accuracy, F1_avg, CELoss\n",
    "\n",
    "# ================================================================\n",
    "# PHASE 1: Neurons Per Hidden Layer\n",
    "# ================================================================\n",
    "neurons_per_layer_to_test = [50, 100, 150]\n",
    "fixed_activation_funcs_p1 = [INPUT_ACTIVATION] + ([BASELINE_HIDDEN_ACTIVATION] * BASELINE_NUM_HIDDEN_LAYERS) + [OUTPUT_ACTIVATION]\n",
    "for i, neurons_per_layer in enumerate(neurons_per_layer_to_test):\n",
    "    current_hidden_neurons = [neurons_per_layer] * BASELINE_NUM_HIDDEN_LAYERS\n",
    "    current_layer_neurons = [input_layer_size] + current_hidden_neurons + [output_layer_size]\n",
    "    config_key = f\"neurons_{'_'.join(map(str, current_hidden_neurons))}\"\n",
    "    print(f\"\\n--- Phase 1 / Trial {i+1}: Testing Neurons = {current_layer_neurons} ---\")\n",
    "    try:\n",
    "        nn = MLP(current_layer_neurons, fixed_activation_funcs_p1, weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
    "        t0 = time.time()\n",
    "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=BASELINE_BATCH_SIZE)\n",
    "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
    "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
    "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
    "        active_dropout_prob = DROPOUT_PROB\n",
    "        neuron_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': current_layer_neurons, 'activation_config': fixed_activation_funcs_p1, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': BASELINE_WEIGHT_DECAY, 'batch_size_tested': BASELINE_BATCH_SIZE, 'dropout_prob_tested': active_dropout_prob, 'learning_rate_tested': BASELINE_LEARNING_RATE, 'history': trial_result } # Store history too if needed\n",
    "    except Exception as e: print(f\"    ERROR: {e}\"); neuron_study_results[config_key] = {'error': str(e)}\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nPhase 1 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ================================================================\n",
    "# PHASE 2: Activation Functions\n",
    "# ================================================================\n",
    "hidden_activations_to_test = ['relu', 'leakyrelu', 'gelu']\n",
    "fixed_layer_neurons_p2 = BASELINE_LAYER_NEURONS\n",
    "for i, activation_func in enumerate(hidden_activations_to_test):\n",
    "    current_activation_funcs = [INPUT_ACTIVATION] + ([activation_func] * BASELINE_NUM_HIDDEN_LAYERS) + [OUTPUT_ACTIVATION]\n",
    "    config_key = f\"activation_{activation_func}\"\n",
    "    print(f\"\\n--- Phase 2 / Trial {i+1}: Testing Activation = '{activation_func}' ---\")\n",
    "    try:\n",
    "        nn = MLP(fixed_layer_neurons_p2, current_activation_funcs, weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
    "        t0 = time.time()\n",
    "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=BASELINE_BATCH_SIZE)\n",
    "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
    "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
    "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
    "        active_dropout_prob = DROPOUT_PROB\n",
    "        activation_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': fixed_layer_neurons_p2, 'activation_config': current_activation_funcs, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': BASELINE_WEIGHT_DECAY, 'batch_size_tested': BASELINE_BATCH_SIZE, 'dropout_prob_tested': active_dropout_prob, 'learning_rate_tested': BASELINE_LEARNING_RATE, 'history': trial_result }\n",
    "    except Exception as e: print(f\"    ERROR: {e}\"); activation_study_results[config_key] = {'error': str(e)}\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nPhase 2 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ================================================================\n",
    "# PHASE 3: Number of Hidden Layers\n",
    "# ================================================================\n",
    "num_hidden_layers_to_test = [1, 3, 5]\n",
    "for i, num_hidden in enumerate(num_hidden_layers_to_test):\n",
    "    current_hidden_neurons = [BASELINE_NEURONS_PER_LAYER] * num_hidden\n",
    "    current_layer_neurons = [input_layer_size] + current_hidden_neurons + [output_layer_size]\n",
    "    current_activation_funcs = [INPUT_ACTIVATION] + ([BASELINE_HIDDEN_ACTIVATION] * num_hidden) + [OUTPUT_ACTIVATION]\n",
    "    config_key = f\"num_hidden_{num_hidden}\"\n",
    "    print(f\"\\n--- Phase 3 / Trial {i+1}: Testing Num Hidden Layers = {num_hidden} ---\")\n",
    "    try:\n",
    "        nn = MLP(current_layer_neurons, current_activation_funcs, weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
    "        t0 = time.time()\n",
    "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=BASELINE_BATCH_SIZE)\n",
    "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
    "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
    "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
    "        active_dropout_prob = DROPOUT_PROB\n",
    "        num_layers_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': current_layer_neurons, 'activation_config': current_activation_funcs, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': BASELINE_WEIGHT_DECAY, 'batch_size_tested': BASELINE_BATCH_SIZE, 'dropout_prob_tested': active_dropout_prob, 'learning_rate_tested': BASELINE_LEARNING_RATE, 'history': trial_result }\n",
    "    except Exception as e: print(f\"    ERROR: {e}\"); num_layers_study_results[config_key] = {'error': str(e)}\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nPhase 3 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ================================================================\n",
    "# PHASE 4: SGD Optimization Parameter (Using Dictionary Input, with Metrics)\n",
    "# ================================================================\n",
    "\n",
    "sgd_optim_values_to_test = [None, {\"Type\": \"Momentum\", \"Parameter\": 0.5}, {\"Type\": \"Momentum\", \"Parameter\": 0.0}]\n",
    "for i, sgd_optim_value in enumerate(sgd_optim_values_to_test):\n",
    "    if sgd_optim_value is None:\n",
    "        config_key = \"sgd_optim_None\"\n",
    "    elif isinstance(sgd_optim_value, dict):\n",
    "        optim_type = sgd_optim_value.get(\"Type\", \"Unknown\")\n",
    "        optim_param = sgd_optim_value.get(\"Parameter\", \"Unknown\")\n",
    "        config_key = f\"sgd_optim_{optim_type}_{optim_param}\"\n",
    "    else:\n",
    "        config_key = f\"sgd_optim_{str(sgd_optim_value)}\"\n",
    "\n",
    "    print(f\"\\n--- Phase 4 / Trial {i+1}: Testing SGD_optim = {sgd_optim_value} ---\")\n",
    "    try:\n",
    "        nn = MLP(BASELINE_LAYER_NEURONS, BASELINE_ACTIVATION_FUNCS,\n",
    "                 weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
    "        t0 = time.time()\n",
    "        trial_result = nn.fit(train_df.X, train_df.y,\n",
    "                            learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS,\n",
    "                            SGD_optim=sgd_optim_value, # Passing None or Dict\n",
    "                            batch_size=BASELINE_BATCH_SIZE)\n",
    "        t1 = time.time()\n",
    "        time_taken = round(t1 - t0, 4)\n",
    "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
    "        active_dropout_prob = DROPOUT_PROB\n",
    "        sgd_optim_study_results[config_key] = {\n",
    "            'time_taken': time_taken,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'F1_avg': F1_avg,\n",
    "            'final_train_loss': CELoss,\n",
    "            'layer_config': BASELINE_LAYER_NEURONS,\n",
    "            'activation_config': BASELINE_ACTIVATION_FUNCS,\n",
    "            'sgd_optim_tested': sgd_optim_value,\n",
    "            'weight_decay_tested': BASELINE_WEIGHT_DECAY,\n",
    "            'batch_size_tested': BASELINE_BATCH_SIZE,\n",
    "            'dropout_prob_tested': active_dropout_prob,\n",
    "            'learning_rate_tested': BASELINE_LEARNING_RATE,\n",
    "            'history': trial_result \n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR: {e}\")\n",
    "        sgd_optim_study_results[config_key] = {'error': str(e)}\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nPhase 4 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ================================================================\n",
    "# PHASE 5: Weight Decay Parameter\n",
    "# ================================================================\n",
    "\n",
    "weight_decay_values_to_test = [0.98, 0.9, 1]\n",
    "for i, weight_decay_value in enumerate(weight_decay_values_to_test):\n",
    "    weight_decay_key_str = 'None' if weight_decay_value is None else str(weight_decay_value)\n",
    "    config_key = f\"weight_decay_{weight_decay_key_str}\"\n",
    "    print(f\"\\n--- Phase 5 / Trial {i+1}: Testing Weight Decay = {weight_decay_value} ---\")\n",
    "    try:\n",
    "        nn = MLP(BASELINE_LAYER_NEURONS, BASELINE_ACTIVATION_FUNCS, weight_decay=weight_decay_value, initialisation=BASELINE_INITIALISATION)\n",
    "        t0 = time.time()\n",
    "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=BASELINE_BATCH_SIZE)\n",
    "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
    "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
    "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
    "        active_dropout_prob = DROPOUT_PROB\n",
    "        weight_decay_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': BASELINE_LAYER_NEURONS, 'activation_config': BASELINE_ACTIVATION_FUNCS, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': weight_decay_value, 'batch_size_tested': BASELINE_BATCH_SIZE, 'dropout_prob_tested': active_dropout_prob, 'learning_rate_tested': BASELINE_LEARNING_RATE, 'history': trial_result }\n",
    "    except Exception as e: print(f\"    ERROR: {e}\"); weight_decay_study_results[config_key] = {'error': str(e)}\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nPhase 5 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ================================================================\n",
    "# PHASE 6: Batch Size\n",
    "# ================================================================\n",
    "batch_size_values_to_test = [100, 50, 1000]\n",
    "for i, batch_size_value in enumerate(batch_size_values_to_test):\n",
    "    config_key = f\"batch_size_{batch_size_value}\"\n",
    "    print(f\"\\n--- Phase 6 / Trial {i+1}: Testing Batch Size = {batch_size_value} ---\")\n",
    "    try:\n",
    "        nn = MLP(BASELINE_LAYER_NEURONS, BASELINE_ACTIVATION_FUNCS, weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
    "        t0 = time.time()\n",
    "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=batch_size_value)\n",
    "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
    "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
    "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
    "        active_dropout_prob = DROPOUT_PROB\n",
    "        batch_size_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': BASELINE_LAYER_NEURONS, 'activation_config': BASELINE_ACTIVATION_FUNCS, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': BASELINE_WEIGHT_DECAY, 'batch_size_tested': batch_size_value, 'dropout_prob_tested': active_dropout_prob, 'learning_rate_tested': BASELINE_LEARNING_RATE, 'history': trial_result }\n",
    "    except Exception as e: print(f\"    ERROR: {e}\"); batch_size_study_results[config_key] = {'error': str(e)}\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nPhase 6 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ================================================================\n",
    "# PHASE 7: Dropout Probability (using global variable)\n",
    "# ================================================================\n",
    "\n",
    "dropout_prob_values_to_test = [0.5, 0.1, 0.9]\n",
    "for i, dropout_prob_value in enumerate(dropout_prob_values_to_test):\n",
    "    config_key = f\"dropout_prob_{dropout_prob_value}\"\n",
    "    print(f\"\\n--- Phase 7 / Trial {i+1}: Setting GLOBAL DROPOUT_PROB = {dropout_prob_value} ---\")\n",
    "    DROPOUT_PROB = dropout_prob_value # Set global variable\n",
    "    try:\n",
    "        nn = MLP(BASELINE_LAYER_NEURONS, BASELINE_ACTIVATION_FUNCS, weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
    "        t0 = time.time()\n",
    "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=BASELINE_LEARNING_RATE, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=BASELINE_BATCH_SIZE)\n",
    "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
    "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
    "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
    "        dropout_prob_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': BASELINE_LAYER_NEURONS, 'activation_config': BASELINE_ACTIVATION_FUNCS, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': BASELINE_WEIGHT_DECAY, 'batch_size_tested': BASELINE_BATCH_SIZE, 'dropout_prob_tested': dropout_prob_value, 'learning_rate_tested': BASELINE_LEARNING_RATE, 'history': trial_result }\n",
    "    except Exception as e: print(f\"    ERROR: {e}\"); dropout_prob_study_results[config_key] = {'error': str(e)}\n",
    "\n",
    "DROPOUT_PROB = BASELINE_DROPOUT_PROB\n",
    "print(f\"\\nGlobal DROPOUT_PROB reset to baseline: {DROPOUT_PROB} after Phase 7.\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nPhase 7 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ================================================================\n",
    "# PHASE 8: Learning Rate\n",
    "# ================================================================\n",
    "learning_rate_values_to_test = [0.005, 0.001, 0.01]\n",
    "for i, learning_rate_value in enumerate(learning_rate_values_to_test):\n",
    "    config_key = f\"learning_rate_{learning_rate_value}\"\n",
    "    print(f\"\\n--- Phase 8 / Trial {i+1}: Testing Learning Rate = {learning_rate_value} ---\")\n",
    "    try:\n",
    "        nn = MLP(BASELINE_LAYER_NEURONS, BASELINE_ACTIVATION_FUNCS, weight_decay=BASELINE_WEIGHT_DECAY, initialisation=BASELINE_INITIALISATION)\n",
    "        t0 = time.time()\n",
    "        trial_result = nn.fit(train_df.X, train_df.y, learning_rate=learning_rate_value, epochs=EPOCHS, SGD_optim=BASELINE_SGD_OPTIM, batch_size=BASELINE_BATCH_SIZE)\n",
    "        t1 = time.time(); time_taken = round(t1 - t0, 4)\n",
    "        print(f\"    Fit completed in {time_taken} seconds.\")\n",
    "        test_accuracy, F1_avg, CELoss = calculate_metrics(nn, trial_result, test_df, train_df)\n",
    "        active_dropout_prob = DROPOUT_PROB # Capture baseline dropout used\n",
    "        learning_rate_study_results[config_key] = { 'time_taken': time_taken, 'test_accuracy': test_accuracy, 'F1_avg': F1_avg, 'final_train_loss': CELoss, 'layer_config': BASELINE_LAYER_NEURONS, 'activation_config': BASELINE_ACTIVATION_FUNCS, 'sgd_optim_tested': BASELINE_SGD_OPTIM, 'weight_decay_tested': BASELINE_WEIGHT_DECAY, 'batch_size_tested': BASELINE_BATCH_SIZE, 'dropout_prob_tested': active_dropout_prob, 'learning_rate_tested': learning_rate_value, 'history': trial_result }\n",
    "    except Exception as e: print(f\"    ERROR: {e}\"); learning_rate_study_results[config_key] = {'error': str(e)}\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nPhase 8 Complete.\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ================================================================\n",
    "# WRITE ALL RESULTS TO FILE\n",
    "# ================================================================\n",
    "print(\"All Independent Ablation Studies Finished. Writing results to file...\")\n",
    "\n",
    "# Get current timestamp\n",
    "now = datetime.datetime.now()\n",
    "timestamp_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "results_filename = f\"results_{timestamp_str}.txt\"\n",
    "\n",
    "all_results = [\n",
    "    (\"Phase 1: Neurons Per Layer\", neuron_study_results),\n",
    "    (\"Phase 2: Activation Function\", activation_study_results),\n",
    "    (\"Phase 3: Number of Hidden Layers\", num_layers_study_results),\n",
    "    (\"Phase 4: SGD Optimization\", sgd_optim_study_results),\n",
    "    (\"Phase 5: Weight Decay\", weight_decay_study_results),\n",
    "    (\"Phase 6: Batch Size\", batch_size_study_results),\n",
    "    (\"Phase 7: Dropout Probability\", dropout_prob_study_results),\n",
    "    (\"Phase 8: Learning Rate\", learning_rate_study_results),\n",
    "]\n",
    "\n",
    "try:\n",
    "    with open(results_filename, 'w') as f:\n",
    "        f.write(f\"Ablation Study Results - Generated on: {now.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        # Write Baseline Configuration\n",
    "        f.write(\"Baseline Configuration:\\n\")\n",
    "        f.write(f\"  Num Hidden Layers: {BASELINE_NUM_HIDDEN_LAYERS}\\n\")\n",
    "        f.write(f\"  Neurons Per Layer: {BASELINE_NEURONS_PER_LAYER}\\n\")\n",
    "        f.write(f\"  Hidden Activation: {BASELINE_HIDDEN_ACTIVATION}\\n\")\n",
    "        f.write(f\"  SGD Optim: {BASELINE_SGD_OPTIM}\\n\")\n",
    "        f.write(f\"  Weight Decay: {BASELINE_WEIGHT_DECAY}\\n\")\n",
    "        f.write(f\"  Initialisation: {BASELINE_INITIALISATION}\\n\")\n",
    "        f.write(f\"  Batch Size: {BASELINE_BATCH_SIZE}\\n\")\n",
    "        f.write(f\"  Dropout Prob: {BASELINE_DROPOUT_PROB}\\n\")\n",
    "        f.write(f\"  Learning Rate: {BASELINE_LEARNING_RATE}\\n\")\n",
    "        f.write(f\"  Epochs: {EPOCHS}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "\n",
    "        for phase_name, results_dict in all_results:\n",
    "            f.write(f\"--- {phase_name} ---\\n\\n\")\n",
    "            if not results_dict:\n",
    "                f.write(\"  No results recorded for this phase.\\n\\n\")\n",
    "                continue\n",
    "\n",
    "            # Sort results by config key for consistent order (optional)\n",
    "            sorted_items = sorted(results_dict.items())\n",
    "\n",
    "            for config_key, data in sorted_items:\n",
    "                f.write(f\"Config Key: {config_key}\\n\")\n",
    "                if 'error' in data:\n",
    "                    f.write(f\"  Status: ERROR\\n\")\n",
    "                    f.write(f\"  Error Msg: {data['error']}\\n\")\n",
    "                else:\n",
    "                    # Safely get metrics with default -1 if missing\n",
    "                    time_val = data.get('time_taken', -1)\n",
    "                    acc = data.get('test_accuracy', -1)\n",
    "                    f1 = data.get('F1_avg', -1)\n",
    "                    loss = data.get('final_train_loss', -1)\n",
    "\n",
    "                    f.write(f\"  Status: Success\\n\")\n",
    "                    f.write(f\"  Time Taken (s): {time_val:.2f}\\n\")\n",
    "                    f.write(f\"  Test Accuracy: {acc:.4f}\\n\")\n",
    "                    f.write(f\"  Average F1 Score: {f1:.4f}\\n\")\n",
    "                    f.write(f\"  Final Train Loss: {loss:.4f}\\n\")\n",
    "\n",
    "                f.write(\"-\" * 40 + \"\\n\") # Separator between trials\n",
    "            f.write(\"\\n\") # Space after phase section\n",
    "\n",
    "    print(f\"Results successfully written to {results_filename}\")\n",
    "\n",
    "except IOError as e:\n",
    "    print(f\"ERROR: Could not write results to file {results_filename}. Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during file writing: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NedsEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
